<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[NLP之概率图模型]]></title>
    <url>%2Funcategorized%2FNLP%E4%B9%8B%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[NLP之词汇与分词]]></title>
    <url>%2Funcategorized%2FNLP%E4%B9%8B%E8%AF%8D%E6%B1%87%E4%B8%8E%E5%88%86%E8%AF%8D%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[NLP之中文语言机器处理]]></title>
    <url>%2Funcategorized%2FNLP%E4%B9%8B%E4%B8%AD%E6%96%87%E8%AF%AD%E8%A8%80%E6%9C%BA%E5%99%A8%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[系列第一篇：整体结构介绍。因为首先咱们得知道自然语言处理主要是干啥的，怎么干，所以请看本文。 自然语言系统 NLTK 中文分词 CRF(条件随机场) NShort结巴分词算法的核心 词性标注词性标注跟分词通常同时完成。算法相似 HMM算法或者最大熵模型 CRF算法 区别是词性标注需要一个词性标签 命名实体识别主要是对新词的识别。主要用户识别文本中三大类：实体类、时间类、数字类七小类：人名、机构名、地名、时间、日期、货币、百分比 句法解析根据语法体系自动推到出句子的语法结构，分析句子所包含的语法单元和这些语法单元之间的关系，将句子转化为一颗结构化的语法树。两种不同的理论：短语结构语法和依从语法 语义角色标注PropBank]]></content>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理之word2vec模型]]></title>
    <url>%2Fartificial-intelligence%2FNLP%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B9%8Bword2vec%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Word2Vec即Word to vector（词汇转向量）。我们希望词义相近的两个单词，在映射之后依然保持相近，词义很远的单词直接则保持很远的映射距离。 Word2Vec结构 目标函数极大化似然函数上面式子是一个标准的概率语言模型的形式，但是上面式子求解起来成本非常高，因为第二项中有个Vocab，也就是字典的大小。word2vec对上面图进行了改进，不在计算在整个词典中生成当前词的概率，而是从词典中『采样』k个. 关于Word2Vec实例总结为6步:1、下载数据；加载训练文本数据，属于无监督学习，所以文本数据没有标签 2、将原词汇数据转换为字典映射；原词汇数据分词，取出指定的词频排前n的词，低频的词用‘UNK’表示，转换为字典映射。按频率从高到低排序，现在我们的词汇文本变成了用数字编号替代的格式以及词汇表和逆词汇表。逆词汇只是编号为key,词汇为value。 3、为 skip-gram模型 建立一个扫描器；首先看一下扫描器函数:defgenerate_batch(batch_size, num_skips, skip_window):batch_size是指一次扫描多少块，skip_window为左右上下文取词的长短，num_skips输入数字的重用次数。假设我们的扫描器先扫这大段文字的前8个单词，左右各取1个单词，重用次数为2次。 4、建立并训练 skip-gram 模型；这里谈得都是嵌套，那么先来定义一个嵌套参数矩阵。我们用唯一的随机值来初始化这个大矩阵。12embeddings = tf.Variable( tf.random_uniform([self._options.vocabulary_size, self._options.emb_dim], -1.0, 1.0)) 对噪声-比对的损失计算就使用一个逻辑回归模型。对此，我们需要对语料库中的每个单词定义一个权重值和偏差值。(也可称之为输出权重 与之对应的 输入嵌套值)。定义如下。1234nce_weights = tf.Variable( tf.truncated_normal([self._options.vocabulary_size, self._options.emb_dim], stddev=1.0 / math.sqrt(self._options.emb_dim)))nce_biases = tf.Variable(tf.zeros([self._options.vocabulary_size])) 我们有了这些参数之后，就可以定义Skip-Gram模型了。简单起见，假设我们已经把语料库中的文字整型化了，这样每个整型代表一个单词。Skip-Gram模型有两个输入。一个是一组用整型表示的上下文单词，另一个是目标单词。给这些输入建立占位符节点，之后就可以填入数据了。然后我们需要对批数据中的单词建立嵌套向量，TensorFlow提供了方便的工具函数。1embed = tf.nn.embedding_lookup(embeddings, self.train_inputs) 好了，现在我们有了每个单词的嵌套向量，接下来就是使用噪声-比对的训练方式来预测目标单词。1234567self._loss = tf.reduce_mean( tf.nn.nce_loss(weights=nce_weights, biases=nce_biases, labels=self.train_labels, inputs=embed, num_sampled=self._options.num_sampled, num_classes=self._options.vocabulary_size)) 我们对损失函数建立了图形节点，然后我们需要计算相应梯度和更新参数的节点，比如说在这里我们会使用随机梯度下降法，TensorFlow也已经封装好了该过程。 5、开始训练模型；训练的过程很简单，只要在循环中使用feed_dict不断给占位符填充数据，同时调用 session.run即可。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>deep-learning</tag>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习之RNN]]></title>
    <url>%2Fartificial-intelligence%2Fdeep-learning%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8BRNN%2F</url>
    <content type="text"><![CDATA[通常的深度学习模型都只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。 RNN结构 x是一个向量，它表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；s是一个向量，它表示隐藏层的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；U是输入层到隐藏层的权重矩阵（读者可以回到第三篇文章零基础入门深度学习(3) - 神经网络和反向传播算法，看看我们是怎样用矩阵来表示全连接神经网络的计算的）；o也是一个向量，它表示输出层的值；V是隐藏层到输出层的权重矩阵。那么，现在我们来看看W是什么。循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。 这个网络在t时刻接收到输入之后，隐藏层的值是，输出值是。 N vs 1输入是一个序列，输出是一个单独的值而不是序列这种结构通常用来处理序列分类问题。如输入一段文字判别它所属的类别，输入一个句子判断其情感倾向，输入一段视频并判断它的类别等等。 1 VS N输入不是序列而输出为序列的情况怎么处理？我们可以只在序列开始进行输入计算： 这种1 VS N的结构可以处理的问题有：从图像生成文字（image caption），此时输入的X就是图像的特征，而输出的y序列就是一段句子从类别生成语音或音乐等 N VS M这种结构又叫Encoder-Decoder模型，也可以称之为Seq2Seq模型。原始的N vs N RNN要求序列等长，然而我们遇到的大部分问题序列都是不等长的，如机器翻译中，源语言和目标语言的句子往往并没有相同的长度。为此，Encoder-Decoder结构先将输入数据编码成一个上下文向量c：得到c有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给c，还可以对最后的隐状态做一个变换得到c，也可以对所有的隐状态做变换。 拿到c之后，就用另一个RNN网络对其进行解码，这部分RNN网络被称为Decoder。具体做法就是将c当做之前的初始状态h0输入到Decoder中：由于这种Encoder-Decoder结构不限制输入和输出的序列长度，因此应用的范围非常广泛，比如： 机器翻译。Encoder-Decoder的最经典应用，事实上这一结构就是在机器翻译领域最先提出的 文本摘要。输入是一段文本序列，输出是这段文本序列的摘要序列。 阅读理解。将输入的文章和问题分别编码，再对其进行解码得到问题的答案。 语音识别。输入是语音信号序列，输出是文字序列。 ………… Attention机制在Encoder-Decoder结构中，Encoder把所有的输入序列都编码成一个统一的语义特征c再解码，因此， c中必须包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈。如机器翻译问题，当要翻译的句子较长时，一个c可能存不下那么多信息，就会造成翻译精度的下降。Attention机制通过在每个时间输入不同的c来解决这个问题，下图是带有Attention机制的Decoder：每一个c会自动去选取与当前所要输出的y最合适的上下文信息。具体来说，我们用 a_{ij} 衡量Encoder中第j阶段的hj和解码时第i阶段的相关性，最终Decoder中第i阶段的输入的上下文信息 c_i 就来自于所有 h_j 对 a_{ij} 的加权和。事实上， a_{ij} 同样是从模型中学出的，它实际和Decoder的第i-1阶段的隐状态、Encoder第j个阶段的隐状态有关。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>deep-learning</category>
      </categories>
      <tags>
        <tag>deep-learning</tag>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法基础之矩阵奇异值分解及其应用]]></title>
    <url>%2Fartificial-intelligence%2Falgorithm%2F%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E4%B9%8B%E7%9F%A9%E9%98%B5%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[奇异值分解是一个有着很明显的物理意义的一种方法，它可以将一个比较复杂的矩阵用更小更简单的几个子矩阵的相乘来表示，这些小矩阵描述的是矩阵的重要的特性。在机器学习领域，有相当多的应用与奇异值都可以扯上关系，比如做feature reduction的PCA，做数据压缩（以图像压缩为代表）的算法，还有做搜索引擎语义层次检索的LSI（Latent Semantic Indexing） 奇异值分解在现实的世界中，我们看到的大部分矩阵都不是方阵，比如说有N个学生，每个学生有M科成绩，这样形成的一个N M的矩阵就不可能是方阵，我们怎样才能描述这样普通的矩阵呢的重要特征呢？奇异值分解可以用来干这个事情，奇异值分解是一个能适用于任意的矩阵的一种分解的方法。假设A是一个N M的矩阵，那么得到的U是一个N N的方阵（里面的向量是正交的，U里面的向量称为左奇异向量），Σ是一个N M的矩阵（除了对角线的元素都是0，对角线上的元素称为奇异值），V’(V的转置)是一个N * N的矩阵，里面的向量也是正交的，V里面的向量称为右奇异向量）。奇异值σ跟特征值类似，在矩阵Σ中也是从大到小排列，而且σ的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。 奇异值的计算奇异值的计算是一个难题，是一个O(N^3)的算法。在单机的情况下当然是没问题的，matlab在一秒钟内就可以算出1000 1000的矩阵的所有奇异值，但是当矩阵的规模增长的时候，计算的复杂度呈3次方增长，就需要并行计算参与了。其实SVD还是可以用并行的方式去实现的，在解大规模的矩阵的时候，一般使用迭代的方法，当矩阵的规模很大（比如说上亿）的时候，迭代的次数也可能会上亿次，如果使用Map-Reduce框架去解，则每次Map-Reduce完成的时候，都会涉及到写文件、读文件的操作。Lanczos迭代就是一种解对称方阵部分特征值的方法（之前谈到了，解A的转置 A得到的对称方阵的特征值就是解A的右奇异向量），是将一个对称的方程化为一个三对角矩阵再进行求解。 主要应用主成分分析PCA PCA的全部工作简单点说，就是对原始的空间中顺序地找一组相互正交的坐标轴，第一个轴是使得方差最大的，第二个轴是在与第一个轴正交的平面中使得方差最大的，第三个轴是在与第1、2个轴正交的平面中方差最大的，这样假设在N维空间中，我们可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间了，但是我们选择的r个坐标轴能够使得空间的压缩使得数据的损失最小。 还是假设我们矩阵每一行表示一个样本，每一列表示一个feature，用矩阵的语言来表示，将一个m n的矩阵A的进行坐标轴的变化，P就是一个变换的矩阵从一个N维的空间变换到另一个N维的空间，在空间中就会进行一些类似于旋转、拉伸的变化。 而将一个m n的矩阵A变换成一个m * r的矩阵，这样就会使得本来有n个feature的，变成了有r个feature了（r &lt; n)，这r个其实就是对n个feature的一种提炼，我们就把这个称为feature的压缩。 PCA对高阶相关性不适合，需要通过核函数变换。 潜在语义索引LSI潜在语义索引（Latent Semantic Indexing）与PCA不太一样，至少不是实现了SVD就可以直接用的，不过LSI也是一个严重依赖于SVD的算法。SVD之后会得到三个矩阵：三个矩阵有非常清楚的物理含义。第一个矩阵X中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关。最后一个矩阵Y中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。中间的矩阵则表示类词和文章雷之间的相关性。因此，我们只要对关联矩阵A进行一次奇异值分解，w 我们就可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark性能调优]]></title>
    <url>%2Fcloud-computing%2Fstorage%2Fspark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[Spark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。 ##开发调优 ###原则一：避免创建重复的RDD通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。 我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。 一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。 原则二：尽可能复用同一个RDD除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。 原则三：对多次使用的RDD进行持久化当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。 Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。 因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。 原则四：尽量避免使用shuffle类算子如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。 shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。 因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。 原则五：使用map-side预聚合的shuffle操作如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。 所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。 原则六：使用高性能的算子除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。 使用reduceByKey/aggregateByKey替代groupByKey详情见“原则五：使用map-side预聚合的shuffle操作”。 使用mapPartitions替代普通mapmapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！ 使用foreachPartitions替代foreach原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。 使用filter之后进行coalesce操作通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。 使用repartitionAndSortWithinPartitions替代repartition与sort类操作repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。 原则七：广播大变量有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。 在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。 因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。 原则八：使用Kryo优化序列化性能在Spark中，主要有三个地方涉及到了序列化： 在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。 ###原则九：优化数据结构Java中，有三种类型比较耗费内存： 对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。字符串，每个字符串内部都有一个字符数组以及长度等额外信息。集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。 因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。 因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。 直接看这里 Spark性能优化指南——基础篇Spark性能优化指南——高级篇]]></content>
      <categories>
        <category>cloud-computing</category>
        <category>storage</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统之EE问题与解决思路]]></title>
    <url>%2Fdata-mining%2Frecommender-system%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B9%8BEE%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[推荐系统不可避免会遇到冷启动问题，如何去尝试新用户的兴趣点，尝试到什么时候地步才算真正掌握了用户的兴趣，用户的兴趣发生改变如何灵活的调整推荐策略。这些，都与E&amp;E问题有关，而Bandit算法是解决E&amp;E问题的一种思路 E&amp;E问题条件：假设我们有K个准备推荐的item，每个item的回报的服从不同的概率分布p_item，且分布参数未知目标：如果有T次机会推荐，如何制定决策过程从而获取最大的累积回报 表现形式随机式(stochastic bandit): item的回报服从某个固定的概率分布对抗式(adversarial bandit): item的回报会动态调整，让你选的奖励变低 推荐应用计算机广告和推荐系统中，有很多问题可以抽象为E&amp;E问题：冷启动问题：假设一个用户对不同类别的内容感兴趣程度不同，那么我们的推荐系统初次见到这个用户时，怎么快速地知道他对每类内容的感兴趣程度？推荐item选择：假设资源池有若干item（或若干类item），怎么知道该给每个用户展示哪个（类），从而获得最大的点击？是每次都挑效果最好那个么？那么新广告如何才有出头之日？推荐策略选择：假设我们有了新的推荐策略，有没有比A/B test更快的方法知道它和旧模型相比谁更靠谱？ E&amp;E算法框架Exploitation &amp; Exploration（1）Exploitation：基于已知最好策略，开发利用已知具有较高回报的item（贪婪、短期回报）Advantage：充分利用已知高回报itemDisadvantage：陷于局部最优，错过潜在更高回报item的机会 （2）Exploration：不考虑曾经的经验，勘探潜在可能高回报的item（非贪婪、长期回报）Advantage：发现更好回报的itemDisadvantage：充分利用已有高回报item机会减少（如已经找到最好item） （3）目标：要找到Exploitation &amp; Exploration的trade-off，以达到累计回报最大化。极端情况下，Exploitation每次选择最高mean回报的item（太“confident”），Exploration每次随机选择一个item（太不“confident”），两个极端都不能达到最终目标，因此，在选择item时，我们不仅要考虑item的mean回报，同事也要兼顾confidence。 Bandit算法专治困难选择症 naive Algorithm最简单直接的，我们可以通过简单观察法（naive Algorithm）选取item：先对每个item进行一定次数（如100次）选择尝试，计算item的回报率，接下来选择回报率高的item。算法简单直接，但存在以下问题：item很多导致获取item回报率的成本太大尝试一定次数（如100次）得到的“高回报”item未必靠谱item的回报率有可能会随时间发生变化：好的变差、差的变好 ε-Greedy选一个(0,1)之间较小的数ε，每次决策以概率ε去勘探Exploration，1-ε的概率来开发Exploitation，基于选择的item及回报，更新item的回报期望，不断循环下去。这样做的好处在于：能够应对变化：如果item的回报发生变化，能及时改变策略，避免卡在次优状态可以控制对Exploration和Exploitation的偏好程度：ε大，模型具有更大的灵活性（能更快的探索潜在可能高回报item，适应变化，收敛速度更快），ε小，模型具有更好的稳定性（更多的机会用来开发利用当前最好回报的item），收敛速度变慢虽然ε-Greedy算法在Exploration和Exploitation之间做出了一定平衡，但设置最好的ε比较困难，大则适应变化较快，但长期累积回报低，小则适应变好的能力不够，但能获取更好的长期回报。策略运行一段时间后，我们对item的好坏了解的确定性增强，但仍然花费固定的精力去exploration，浪费本应该更多进行exploitation机会策略运行一段时间后，我们已经对各item有了一定程度了解，但没用利用这些信息，仍然不做任何区分地随机exploration（会选择到明显较差的item） Epsilon-Greedy算法的变体：1）ε-first strategy：首先进行小部分次数进行随机尝试来确定那些item能获得较高回报，然后接下来大部分次数选择前面确定较高回报的item2）ε-decreasing strategy: epsilon随着时间的次数逐步降低，开始的时候exploration更多次数，然后逐步降低（ε_t=1/log(itemSelectCnt+0.00001 )），增加exploitation比重(Annealing-Epsilon-Greedy) SoftMaxε-Greedy在探索时采用完全随机的策略，经常会选择一个看起来很差的item，此问题的解决方法之一是，基于我们目前已经知道部分item回报信息，不进行随机决策，而是使用softmax决策找出回报最大的item。SoftMax利用softmax函数来确定各item的回报的期望概率排序，进而在选择item时考虑该信息，减少exploration过程中低回报率item的选择机会，同时收敛速度也会较ε-Greedy更快。 但是，softmax算法的缺点是：没有考虑预估item回报率期望的置信度信息，因而选择的高回报率item未必靠谱。无法事先知道t的大小，因问题而异，同时也不容易与其他算法直接比较Softmax变体：Annealing-Softmax:随着策略的运行，我们期望降低T温度参数以减小exploration所占的比例：T=1/log(itemSelectCnt+0.0000001) UCB, Upper Confidence Bound统计学中，我们使用置信区间来度量估计的不确定性/置信性。如我们摇骰子一次得到的点数为2，那么得到均值的估计也是2（实际平均点数是3.5），但显然这个估计不太靠谱，可以用置信区间量化估计的变化性：骰子点数均值为2，其95%置信区间的上限、下限分别为1.4、5.2。 UCB思想是乐观地面对不确定性，以item回报的置信上限作为回报预估值的一类算法，其基本思想是：我们对某个item尝试的次数越多，对该item回报估计的置信区间越窄、估计的不确定性降低，那些均值更大的item倾向于被多次选择，这是算法保守的部分（exploitation）；对某个item的尝试次数越少，置信区间越宽，不确定性较高，置信区间较宽的item倾向于被多次选择，这是算法激进的部分（exploration）。 其中，x_j是item_j的平均回报，n_j是item_j截至当前被选择的次数，n为当前选择所有item的次数。上式反映了，均值越大，标准差越小，被选中的概率会越来越大，起到了exploit的作用；同时哪些被选次数较少的item也会得到试验机会，起到了explore的作用。 与ε-Greedy算法、softmax算法相比，这种策略的好处在于：考虑了回报均值的不确定性，让新的item更快得到尝试机会，将探索+开发融为一体基础的UCB算法不需要任何参数，因此不需要考虑如何验证参数（ε如何确定）的问题UCB1算法的缺点：UCB1算法需要首先尝试一遍所有item，因此当item数量很多时是一个问题一开始各item选择次数都比较少，导致得到的回报波动较大（经常选中实际比较差的item） Thompson samplingUCB算法部分使用概率分布（仅置信区间上界）来量化不确定性。而Thompson sampling基于贝叶斯思想，全部用概率分布来表达不确定性。 假设每个item有一个产生回报的概率p，我们通过不断试验来估计一个置信度较高的概率p的概率分布。如何估计概率p的概率分布呢？ 假设概率p的概率分布符合beta(wins, lose)分布，它有两个参数: wins, lose， 每个item都维护一个beta分布的参数。每次试验选中一个item，有回报则该item的wins增加1，否则lose增加1。每次选择item的方式是：用每个item现有的beta分布产生一个随机数b，选择所有item产生的随机数中最大的那个item。 相比于UCB算法，Thompson sampling：UCB采用确定的选择策略，可能导致每次返回结果相同（不是推荐想要的），Thompson Sampling则是随机化策略。Thompson sampling实现相对更简单，UCB计算量更大（可能需要离线/异步计算）在计算机广告、文章推荐领域，效果与UCB不相上下或更好competitive to or better对于数据延迟反馈、批量数据反馈更加稳健robust]]></content>
      <categories>
        <category>data-mining</category>
        <category>recommender-system</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Generative Adversarial Networks]]></title>
    <url>%2Fartificial-intelligence%2Fdeep-learning%2FGenerative-Adversarial-Networks%2F</url>
    <content type="text"><![CDATA[GAN 暨生成对抗网络（Generative Adversarial Networks）是由两个彼此竞争的深度神经网络——生成器和判别器组成的。本文主要介绍GAN的基本原理。 GAN原理GAN的基本原理其实非常简单，这里以生成图片为例进行说明。假设我们有两个网络，G（Generator）和D（Discriminator）。正如它的名字所暗示的那样，它们的功能分别是：G是一个生成图片的网络，它接收一个随机的噪声z，通过这个噪声生成图片，记做G(z)。D是一个判别网络，判别一张图片是不是“真实的”。它的输入参数是x，x代表一张图片，输出D（x）代表x为真实图片的概率，如果为1，就代表100%是真实的图片，而输出为0，就代表不可能是真实的图片。在训练过程中，生成网络G的目标就是尽量生成真实的图片去欺骗判别网络D。而D的目标就是尽量把G生成的图片和真实的图片分别开来。这样，G和D构成了一个动态的“博弈过程”。最后博弈的结果是什么？在最理想的状态下，G可以生成足以“以假乱真”的图片G(z)。对于D来说，它难以判定G生成的图片究竟是不是真实的，因此D(G(z)) = 0.5。这样我们的目的就达成了：我们得到了一个生成式的模型G，它可以用来生成图片。用论文里的公式表示就是：简单分析一下这个公式：整个式子由两项构成。x表示真实图片，z表示输入G网络的噪声，而G(z)表示G网络生成的图片。 D(x)表示D网络判断真实图片是否真实的概率（因为x就是真实的，所以对于D来说，这个值越接近1越好）。而D(G(z))是D网络判断G生成的图片的是否真实的概率。 G的目的：上面提到过，D(G(z))是D网络判断G生成的图片是否真实的概率，G应该希望自己生成的图片“越接近真实越好”。也就是说，G希望D(G(z))尽可能得大，这时V(D, G)会变小。因此我们看到式子的最前面的记号是min_G。 D的目的：D的能力越强，D(x)应该越大，D(G(x))应该越小。这时V(D,G)会变大。因此式子对于D来说是求最大(max_D) 训练算法那么如何用随机梯度下降法训练D和G？论文中也给出了算法：第一步我们训练D，D是希望V(G, D)越大越好，所以是加上梯度(ascending)。第二步训练G时，V(G, D)越小越好，所以是减去梯度(descending)。整个训练过程交替进行。 训练过程如图所示，我们手上有真实数据（黑色点，data）和模型生成的伪数据（绿色线，model distribution，是由我们的 z 映射过去的）（画成波峰的形式是因为它们都代表着各自的分布，其中纵轴是分布，横轴是我们的 x）。而我们要学习的 D 就是那条蓝色的点线，这条线的目的是把融在一起的 data 和 model 分布给区分开。写成公式就是 data 和 model 分布相加做分母，分子则是真实的 data 分布。我们最终要达到的效果是：D 无限接近于常数 1/2。换句话说就是要 Pmodel 和 Pdata 无限相似。这个时候，我们的 D 分布再也没法分辨出真伪数据的区别了。这时候，我们就可以说我们训练出了一个炉火纯青的造假者（生成模型）。 最终结果训练结束最终收敛到生成模型最优化：蓝色断点线是一条常数线（1/2），黑色与绿色完美重合了。 假设生成器生成了一张图片，辨别器认为这张图片有 0.4 的概率是真实图片。生成器如何调整它生成的图片来增加这个概率，比如说增加到 0.41？答案就是：为训练生成器，辨别器不得不告诉生成器如何调整从而使它生成的图片变得更加真实。生成器必须向辨别器寻求建议！直观来说，辨别器告诉生成器每个像素应调整多少来使整幅图像更真实一点点。技术上来说，通过反向传播辨别器输出的梯度来调整生成图片。以这种方式训练生成器，你将会得到与图片形状一样的梯度向量。如果你把这些梯度加到生成的图片上，在辨别器看来，图片就会变得更真实一点。但是我们不仅仅把梯度加到图片上。相反，我们进一步反向传播这些图片梯度成为组成生成器的权重，这样一来，生成器就学习到如何生成这幅新图片。我重复一遍，为生成好的图片，你必须向老师展示你的工作，得到反馈！如果辨别器不帮助生成器的话，那就太残酷了，因为生成器实际做的工作比辨别器更艰难，它生成图片！这就是生成器如何被训练的。就像这样，来回训练生成器和辨别器，直到达到一个平衡状态。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>deep-learning</category>
      </categories>
      <tags>
        <tag>deep-learning</tag>
        <tag>Neural-Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理之word2vec的损失函数]]></title>
    <url>%2Fartificial-intelligence%2FNLP%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B9%8Bword2vec%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[NCE的主要思想是，对于每一个样本，除了本身的label，同时采样出N个其他的label，从而我们只需要计算样本在这N+1个label上的概率，而不用计算样本在所有label上的概率。而样本在每个label上的概率最终用了Logistic的损失函数。 结构 如果在这里使用Softmax + Cross-Entropy作为损伤函数会有一个问题，Softmax当有几万+的分类时，速率会大大下降。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>deep-learning</tag>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习之全连接是必须的吗]]></title>
    <url>%2Fartificial-intelligence%2Fdeep-learning%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%85%A8%E8%BF%9E%E6%8E%A5%E6%98%AF%E5%BF%85%E9%A1%BB%E7%9A%84%E5%90%97%2F</url>
    <content type="text"><![CDATA[为什么好像所有的卷积层之后都要加上全连接层？全连接层的作用是什么？如果说神经网络是模仿人的大脑，可是人的大脑不是好像没有全连接层的吗？ 全连接层的作用： 全连接层（fully connected layers，FC）在整个卷积神经网络中起到“分类器”的作用。如果说卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为1x1的卷积；而前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽。 目前由于全连接层参数冗余（仅全连接层参数就可占整个网络参数80%左右），近期一些性能优异的网络模型如ResNet和GoogLeNet等均用全局平均池化（global average pooling，GAP）取代FC来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程。需要指出的是，用GAP替代FC的网络通常有较好的预测性能。 在FC越来越不被看好的当下，我们近期的研究（In Defense of Fully Connected Layers in Visual Representation Transfer）发现，FC可在模型表示能力迁移过程中充当“防火墙”的作用。具体来讲，假设在ImageNet上预训练得到的模型为 ，则ImageNet可视为源域（迁移学习中的source domain）。微调（fine tuning）是深度学习领域最常用的迁移学习技术。针对微调，若目标域（target domain）中的图像与源域中图像差异巨大（如相比ImageNet，目标域图像不是物体为中心的图像，而是风景照，见下图），不含FC的网络微调后的结果要差于含FC的网络。因此FC可视作模型表示能力的“防火墙”，特别是在源域与目标域差异较大的情况下，FC可保持较大的模型capacity从而保证模型表示能力的迁移。（冗余的参数并不一无是处。） 全连接层的缺点 参数太多 容易过拟合 不可解释 实验证明，将其替换为平均池化层（或者1x1卷积层）不仅不影响精度，还可以减少参数量。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>deep-learning</category>
      </categories>
      <tags>
        <tag>deep-learning</tag>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之GBDT]]></title>
    <url>%2Fartificial-intelligence%2Fmachine-learning%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BGBDT%2F</url>
    <content type="text"><![CDATA[GBDT的基本思想是“积跬步以至千里”！也就是说我每次都只学习一点，然后一步步的接近最终要预测的值（完全是gradient的思想~）。换句话来说，我们先用一个初始值来学习一棵决策树，叶子处可以得到预测的值，以及预测之后的残差，然后后面的决策树就要基于前面决策树的残差来学习，直到预测值和真实值的残差为零。最后对于测试样本的预测值，就是前面许多棵决策树预测值的累加。整个过程都是每次学习一点（真实值的一部分），最后累加，所以叫做“积跬步以至千里”！ 基于参差的版本网上流传了一个预测年龄的例子，例子举的很好，当时解释的过程没有结合GBDT的工作过程，这里我们改进一下：训练集：（A, 14岁）、（B，16岁）、（C, 24岁）、（D, 26岁）；训练数据的均值：20岁； （这个很重要，因为GBDT与i开始需要设置预测的均值，这样后面才会有残差！）决策树的个数：2棵；每个样本的特征有两个：购物金额是否小于1K；经常去百度提问还是回答； 开始GBDT学习了~首先，输入初值20岁，根据第一个特征（具体选择哪些特征可以根据信息增益来计算选择），可以把4个样本分成两类，一类是购物金额&lt;=1K，一类是&gt;=1K的。假如这个时候我们就停止了第一棵树的学习，这时我们就可以统计一下每个叶子中包含哪些样本，这些样本的均值是多少，因为这个时候的均值就要作为所有被分到这个叶子的样本的预测值了。比如AB被分到左叶子，CD被分到右叶子，那么预测的结果就是：AB都是15岁，CD都是25岁。和他们的实际值一看，结果发现出现的残差，ABCD的残差分别是-1, 1, -1, 1。这个残差，我们要作为后面第二棵决策树的学习样本。然后学习第二棵决策树，我们把第一棵的残差样本（A, -1岁）、（B，1岁）、（C, -1岁）、（D, 1岁）输入。此时我们选择的特征是经常去百度提问还是回答。这个时候我们又可以得到两部分，一部分是AC组成了左叶子，另一部分是BD组成的右叶子。那么，经过计算可知左叶子均值为-1，右叶子均值为1. 那么第二棵数的预测结果就是AC都是-1，BD都是1. 我们再来计算一下此时的残差，发现ABCD的残差都是0！停止学习~ 这样，我们的两棵决策树就都学习好了。进入测试环节：测试样本：请预测一个购物金额为3k，经常去百度问淘宝相关问题的女生的年龄~我们提取2个特征：购物金额3k，经常去百度上面问问题；第一棵树 —&gt; 购物金额大于1k —&gt; 右叶子，初步说明这个女生25岁第二棵树 —&gt; 经常去百度提问 —&gt; 左叶子，说明这个女生的残差为-1；叠加前面每棵树得到的结果：25-1=24岁，最终预测结果为24岁~ 这个版本的核心思路：每个回归树学习前面树的残差，并且用shrinkage把学习到的结果大步变小步，不断迭代学习。其中的代价函数是常见的均方差。 其基本做法就是：先学习一个回归树，然后“真实值-预测值*shrinkage”求此时的残差，把这个残差作为目标值，学习下一个回归树，继续求残差……直到建立的回归树的数目达到一定要求或者残差能够容忍，停止学习。 优缺点我们知道，残差是预测值和目标值的差值，这个版本是把残差作为全局最优的绝对方向来学习。这个版本更加适用于回归问题，线性和非线性的均可，而且在设定了阈值之后还可以有分类的功能。当时该版本使用残差，很难处理纯回归以外的问题。Shrinkage和梯度下降法中学习步长alpha的关系。shrinkage设小了只会让学习更慢，设大了就等于没设，它适用于所有增量迭代求解问题；而Gradient的步长设小了容易陷入局部最优点，设大了容易不收敛。它仅用于用梯度下降求解。这两者其实没太大关系。 基于梯度的版本只要建立的代价函数能够求导，那么就可以使用版本二的GBDT算法，例如LambdaMART学习排序算法。我们可以先明确一个思路，就是梯度版本的GBDT是用多类分类Multi-class classification 的思想来实现的，或者可以说GBDT的这个版本融入到多类分类中可以更好的掌握。 算法 基于残差的版本是把残差作为全局方向，偏向于回归的应用。而基于梯度的版本是把代价函数的梯度方向作为更新的方向，适用范围更广。如果使用Logistic函数作为代价函数，那么其梯度形式和残差的形式类似，这个就说明两个版本之间是紧密联系的，虽然实现的思路不同，但是总体的目的是一样的。或者说残差版本是梯度版本的一个特例，当代价函数换成其余的函数，梯度的版本仍是适用的。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘与数据化运营]]></title>
    <url>%2Fartificial-intelligence%2Fmachine-learning%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5%2F</url>
    <content type="text"><![CDATA[21世纪核心的竞争就是数据的竞争,谁拥有数据,谁就拥有未来。 预测响应(分类)模型的典型应用和技巧神经网络对模型结果产生影响的五大要素 层数没有具体规则 每层中输入变量的数量太多的自变量很可能会造成模型的过度拟合,使得模型搭建时看上去很稳定,可是一旦用到新数据中,模型的预测与实际结果却相差很大,这时模型就失去了预测的价值和意义。所以,在使用神经网络建模之前,输入变量的挑选、精简非常重要。 联系的种类。神经网络模型中,输入变量可以有不同方向的结合,可以向前,可以向后,还可以平行。采用不同的结合方式,可能就会对模型的结果产生不同的影响。 联系的程度。在每一层中,其元素可以与他层中的元素完全联系,也可以部分联系。部分联系可以减少模型过度拟合的风险,但是也可能减弱模型的预测能力。 转换函数。转换函数也称为挤压函数,因为它能把从正无穷大到负无穷大的所有输入变量挤压为很小范围内的一个输出结果。这种非线性的函数关系有助于模型的稳定和可靠性。 模型设计原理 隐藏层层数 每层内的输入变量：隐蔽层的数量为输入数与输出数的乘积开平方 联系的程度：全连接、部分连接 转换函数 样本数量：避免过拟合与欠拟合 优势 良好的自组织学习能力 有比较优秀的在数据中挑选非线性关系的能力,能有效发现非线性的内在规律。 由于神经网络具有复杂的结构,因此在很多实践场合中其应用效果都明显优于其他的建模算法;它对异常值不敏感,这是个很不错的“宽容”个性。 对噪声数据有比较高的承受能力。 缺陷１. 训练时间长２. 本身无法挑选变量３. 对缺失值敏感４. 容易过拟合５. 结构复杂，难以解释 过拟合现象以及解决方案产生原因 建模样本抽取错误。包括但不限于样本数量太少,抽样方法错误,抽样时没有足够正确地考虑业务场景或业务特点等,以致抽出的样本数据不能足够有效地代表业务逻辑或业务场景。 样本里的噪声数据干扰过大。样本噪声大到模型过分记住了噪声特征,反而忽略了真实的输入输出间的关系。 在决策树模型的搭建过程中,如果对于决策树的生长没有合理的限制和修剪 建模时的逻辑假设到了应用模型时已经不能成立了。 建模时使用了太多的输入变量。 防止过拟合两点：充分了解业务与数据，合理训练模型有以下几点需要注意： 合理、有效地抽样;包括分层抽样、过抽样等,从而用不同的样本去检验模型。 准备几个不同时间窗口、不同范围的测试数据集和验证数据集 如果数据太少,谨慎使用神经网络模型,只有拥有足够多的数据,神经网络模型才可以有效防止过拟合的产生。并且,使用神经网络时,一定要事先有效筛选输入变量,千万不能一股脑把所有的变量都放进去。 用户特征分析的典型应用和技术业务场景 寻找目标客户：虚拟客户（商品还没上线）、真实客户 用户群体细分数据化运营的精细化要求就是个性化运营的要求,虽然在企业的商业实践中不可能真的实现一对一的个性化服务,至少在目前是不可能的,主要原因在于资源配置和服务效率上。但是针对不同的细分群体进行个性化服务和运营却是必要的,否则它与传统的粗放经营又有什么区别呢? 新品开发的线索和依据 分析思路 直接通过业务分析 通过数据统计分析 混合分析 RFM模型 R(Recency),客户消费新鲜度,指客户最近一次购买公司产品的时间; F(Frequency),客户消费频度,指客户特定时间段里购买公司产品的次数、频度; M(Monetary),客户消费金额,指客户在特定时间段里消费公司产品的总金额 聚类技术针对聚类技术在用户特征分析中的具体应用,需要强调的是,如果参与聚类的变量数量较少,为了能够更好地支持用户特征分析的实践应用,非常有必要在聚类(分群)的基础上,增加更多的与业务目标和商业背景相关的非聚类变量来进行综合考虑。 决策树技术决策树技术最大的应用优势在于其结论非常直观易懂,生成的一系列“如果……那么……”的逻辑判断,很容易让人理解和应用。这个特点是决策树赢得广泛应用的最主要原因,真正体现了简单、直观、通俗、易懂。 评价体系 是否与当初的分析需求(商业目标)相一致 是否容易被业务方理解,是否容易特征化。 通过这些主要结论来圈定的客户基数是否足够大,是否可以满足特定运营活动(运营方案)的基本数量要求 是否方便业务方开发出有效的个性化的运营方案。 运营效果分析的典型应用与技巧]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>AUC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理之入门概念解析]]></title>
    <url>%2Fartificial-intelligence%2FNLP%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B9%8B%E5%85%A5%E9%97%A8%E6%A6%82%E5%BF%B5%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[自然语言处理（NLP）是指机器理解并解释人类写作、说话方式的能力。它的目标是让计算机／机器在理解语言上像人类一样智能。最终目标是弥补人类交流（自然语言）和计算机理解（机器语言）之间的差距。 系列步骤： 分词基于词典分词算法也称字符串匹配分词算法。该算法是按照一定的策略将待匹配的字符串和一个已建立好的“充分大的”词典中的词进行匹配，若找到某个词条，则说明匹配成功，识别了该词。常见的基于词典的分词算法分为以下几种：正向最大匹配法、逆向最大匹配法和双向匹配分词法等。基于词典的分词算法是应用最广泛、分词速度最快的。很长一段时间内研究者都在对基于字符串匹配方法进行优化，比如最大长度设定、字符串存储和查找方式以及对于词表的组织结构，比如采用TRIE索引树、哈希索引等。 基于统计的机器学习算法这类目前常用的是算法是HMM、CRF、SVM、深度学习等算法，比如stanford、Hanlp分词工具是基于CRF算法。以CRF为例，基本思路是对汉字进行标注训练，不仅考虑了词语出现的频率，还考虑上下文，具备较好的学习能力，因此其对歧义词和未登录词的识别都具有良好的效果。常见的分词器都是使用机器学习算法和词典相结合，一方面能够提高分词准确率，另一方面能够改善领域适应性。 词表示one-hotNLP 中最直观，也是到目前为止最常用的词表示方法是 One-hot Representation，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。 分布式表示基于分布假说的词表示方法，根据建模的不同，主要可以分为三类： 基于矩阵的分布表示 基于矩阵的分布表示通常又称为分布语义模型，在这种表示下，矩阵中的一行，就成为了对应词的表示，这种表示描述了该词的上下文的分布。由于分布假说认为上下文相似的词，其语义也相似，因此在这种表示下，两个词的语义相似度可以直接转化为两个向量的空间距离。常见到的Global Vector 模型（ GloVe模型）是一种对“词-词”矩阵进行分解从而得到词表示的方法，属于基于矩阵的分布表示。 基于聚类的分布表示 基于神经网络的分布表示 基于神经网络的分布表示一般称为词向量、词嵌入（ word embedding）或分布式表示（ distributed representation）。 尽管这些不同的分布表示方法使用了不同的技术手段获取词表示，但由于这些方法均基于分布假说，它们的核心思想也都由两部分组成：一、选择一种方式描述上下文；二、选择一种模型刻画某个词（下文称“目标词”）与其上下文之间的关系。 词嵌入（word embedding）由one-hot稀疏到【低维、实数】的空间表示传统自然语言处理的表达，一般以词为单位。自然语言字符表达成计算机能处理的信息，采取离散表达。假设有个词表，可以理解为字典，如何在这一万个字中表达这一个字。用一万维的向量表达这个字，某个字只可能在某一维地方为1，其他位置都为0。这种方法特点：根据词表大小决定向量长度。很占空间，即使稀疏表示。缺乏泛化性，即 lack of generalization。这种表示没法表达相似（cos函数）。所以希望能够有一种方式计算这种泛化性，稠密向量。那就是词嵌入（word embedding）具体是什么东西呢？ word2vec处理 word embedding 的常用技术。Word2Vec 是代码项目的名字，只是计算 word embedding 的一个工具，是 CBOW 和 Skip-Gram 这两个模型开源出来的工具。连续实值词表达也叫词嵌入 word embedding。CBOW 是利用词的上下文预测当前的单词；而 Skip-Gram 则是利用当前词来预测上下文。 CBOW skip-gram 语言模型基于统计：N-gramN-Gram是一种基于统计语言模型的算法。它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列。每一个字节片段称为gram，对所有gram的出现频度进行统计，并且按照事先设定好的阈值进行过滤，形成关键gram列表，也就是这个文本的向量特征空间，列表中的每一种gram就是一个特征向量维度。 该模型基于这样一种假设，第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计N个词同时出现的次数得到。常用的是二元的Bi-Gram和三元的Tri-Gram。 应用领域自然语言理解（NLU)NLU 是要理解给定文本的含义。文本内每个单词的特性与结构需要被理解。在理解结构上，NLU 要理解自然语言中的以下几个歧义性：词法歧义性：单词有多重含义句法歧义性：语句有多重解析树语义歧义性：句子有多重含义回指歧义性（Anaphoric Ambiguity）：之前提到的短语或单词在后面句子中有不同的含义。接下来，通过使用词汇和语法规则，理解每个单词的含义。然而，有些词有类似的含义（同义词），有些词有多重含义（多义词）。 自然语言生成(NLG)NLG 是从结构化数据中以可读地方式自动生成文本的过程。自然语言生成的问题是难以处理。自然语言生成可被分为三个阶段： 文本规划：完成结构化数据中基础内容的规划。 语句规划：从结构化数据中组合语句，来表达信息流。 实现：产生语法通顺的语句来表达文本。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>deep-learning</tag>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习之新型激活函数Swish]]></title>
    <url>%2Fartificial-intelligence%2Fdeep-learning%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%96%B0%E5%9E%8B%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0Swish%2F</url>
    <content type="text"><![CDATA[近日，谷歌大脑团队提出了新型激活函数 Swish，团队实验表明使用 Swish 直接替换 ReLU 激活函数总体上可令 DNN 的测试准确度提升。此外，该激活函数的形式十分简单，且提供了平滑、非单调等特性从而提升了整个神经网络的性能。 概念 激活函数 Swish:f(x) = x · sigmoid(x) 图 1 展示的是 Swish 函数的图像： 和 ReLU 一样，Swish 无上界有下界。与 ReLU 不同的是，Swish 是平滑且非单调的函数。事实上，Swish 的非单调特性把它与大多数常见的激活函数区别开来。 Swish 的导数: Swish 的一阶导和二阶导如图 2 所示。输入低于 1.25 时，导数小于 1。Swish 的成功说明 ReLU 的梯度不变性（即 x &gt; 0 时导数为 1）在现代架构中或许不再是独有的优势。事实上，实验证明在使用批量归一化（Ioffe &amp; Szegedy, 2015）的情况下，我们能够训练出比 ReLU 网络更深层的 Swish 网络。 原论文指出当全连接网络在 40 层以内时，Swish 只稍微优于 ReLU 激活函数，但当层级增加到 40 至 50 层时，使用 Swish 函数的测试准确度要远远高于使用 ReLU 的测试准确度。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>deep-learning</category>
      </categories>
      <tags>
        <tag>deep-learning</tag>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法基础之假设检验]]></title>
    <url>%2Fartificial-intelligence%2Falgorithm%2F%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E4%B9%8B%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[假设检验是一种规则，它根据数据样本所提供的证据，指定是肯定还是否定有关总体的声明。 概念 假设检验检查有关总体的两个相反假设：原假设和备择假设。原假设是要检验的声明。通常，原假设声明“无效应”或“无差异”。备择假设是希望根据样本数据所提供的证据得出真结论的声明。 检验基于样本数据，确定是否要否定原假设。可使用 p 值来做出判断。如果 p 值小于显著性水平（用 α 或 alpha 表示），则可以否定原假设。 通俗来讲，就是：研究时，我们当然不希望否证自己的研究假设，所以我们就搞个和研究假设相反的虚无假设。如果我们否证了虚无假设，就相当于我们证明了研究假设。所以假设检验就是要试图否证虚无假设，或者说拒绝虚无假设。 证明逻辑就是：我要证明命题为真-&gt;证明该命题的否命题为假-&gt;在否命题的假设下，观察到小概率事件发生了-&gt;否命题被推翻-&gt;原命题为真-&gt;搞定。 结合这个例子来看：证明A是合格的投手-》证明“A不是合格投手”的命题为假-》观察到一个事件（比如A连续10次投中10环），而这个事件在“A不是合格投手”的假设下，概率为p，小于0.05-&gt;小概率事件发生，否命题被推翻。可以看到p越小-》这个事件越是小概率事件-》否命题越可能被推翻-》原命题越可信 原理带概率性质的反证法原理 反证法证明命题的一般步骤如下： 1.假设结论的反面成立，即反设； 2.由这个假设出发，经过正确的推理导出矛盾，即归谬； 3.由矛盾判定假设不正确，从而肯定命题结论正确，即结论。 小概率事件原理 费歇尔老先生提出了p值这个概念，用来表示在原假设成立的条件下，抽样结果的不合理和更不合理的概率。他还给出了一个判决点，即0.05，费歇尔p值小于1/20就足以拒绝原假设了。 0.05是足够小的概率，一般认为，在一次抽样(试验)中，小概率事件几乎不可能发生，如果出现发生了，则说明事先的假设是错误的。但小概率事件并不是一定不会发生，当抽样次数足够多时，小概率事件是一定会发生的。这说明即使是一次抽样，小概率事件仍有可能发生，也就是说存在判断错误的可能性。在标准的假设检验中，对判错的概率进行了定义，这就是α和β风险. ##步骤 将实际问题提炼为统计问题加入：模型上线后效果是否好于上线前 建立对立假设原假设H_0 和备择假设H_a,通常我们将无区别的、不需证明的放在原假设，将有差别的、需要证明的放在备择假设. 确定显著性水平α显著性水平在这里也叫第一类风险，大多数情况下取0.05，当然也有取0.1或者0.01的。 验证前提条件在很多的方法应用中，都会有一些前提条件，如单样本z检验要求收集的数据要服从正态分布；方差分析中要求每个样本都要服从正态分布，且要满足方差齐性即方差相等的要求。这是因为每一种检验方法都是在这些前提假设中推导出来的，如果这些前提条件不能满足，则这个方法应用的效果就要大打折扣，这就是在每次检验是要验证前提条件的原因。 确定检验统计量 确定拒绝域所谓的拒绝域，就是根据原假设划定的一个区域，当所抽样本计算出的统计量落在这个区域时，就可以拒绝原假设，可想而知，它代表的是样本远离原假设的程度。 根据样本计算检验统计量的值并进行判断确定了拒绝域，下面就是根据样本计算检验统计量的值，如果这个值落在拒绝域中，则拒绝原假设，接受备择假设；如果没有落在拒绝域，就说明拒绝原假设的证据还不够充分，但尽量不要说接受原假设。 将统计判断结果转换成实际结果最后一步，要把统计上得出的结论转换为实际的结论，并据此作出相应的决策。 Z检验当样本容量n&gt;30的正态分布或非正态分布的样本的均值检验。当n&lt;30的正态分布样本的均值检验要用t检验。 一般用于大样本(即样本容量大于30)平均值差异性检验的方法。它是用标准正态分布的理论来推断差异发生的概率，从而比较两个平均数的差异是否显著。当已知总体标准差时，验证一组数的均值是否与某一期望值相等时（H0假设），用Z检验。当样本n&gt;30时，Z 检验和t检验结果是一致的！当n&lt;30时，若样本是服从正态分布的，则要用t检验 T 检验 T 检验是针对分布期望 u 的检验。 t检验不需要知道总体方差，它用样本方差替代总体方差，得到的统计量服从t分布。 卡方检验卡方检验用于检验观测到的数据是否服从特定多项分布。 F 检验F检验又叫方差齐性检验，用于检验两组服从正态分布的样本是否具有相同的总体方差，即方差齐性。 Z检验与T检验的区别【z检验】—适用的data types：1.随机，独立2.n&gt;30,preferable3.n&lt;=30,正态分布。n&gt;30, not necessary.4.样本方差应该相同5.sample size尽可能相等，但一些不同也允许 标准差已知，则z检验preferable 【t检验】—适用的data types：1.随机，独立(除了paired-sample t test)2.n&lt;303.正态分布for the equal and unequal variance t-test4.样本方差应该相同for the equal variance t-test5.sample size尽可能相等，但一些不同也允许 用n&lt;30(或其他标准60等）的小样本去估计总体的未知均值和未知方差，前提要求总体样本rough normal就是似正态分布，样本均值服从自由度为n-1的t分布，总体的方差为样本方差*root（n/n-1)，此时是t检验，同样的条件，n&lt;30的小样本，总体均值未知，但总体方差已知，总体样本是似正态分布，样本均值服从正态分布，使用z检验；而n&gt;30（或其他标准60等）的小样本去估计总体的未知均值和未知方差，不要求总体分布是否是似正态分布，因为中心极限定理，样本均值服从正态分布，总体方差可以用样本方差来估计，用z检验]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简述mapreduce]]></title>
    <url>%2Fartificial-intelligence%2Fmachine-learning%2F%E7%AE%80%E8%BF%B0mapreduce%2F</url>
    <content type="text"><![CDATA[Hadoop Map/Reduce是一个使用简易的软件框架，基于它写出来的应用程序能够运行在由上千个商用机器组成的大型集群上，并以一种可靠容错的方式并行处理上T级别的数据集。 在运行一个mapreduce计算任务时候，任务过程被分为两个阶段： 映射（Mapping）对集合里的每个目标应用同一个操作。即，如果你想把表单里每个单元格乘以二，那么把这个函数单独地应用在每个单元格上的操作就属于mapping。 化简（Reducing ）遍历集合中的元素来返回一个综合的结果。即，输出表单里一列数字的和这个任务属于reducing。 而程序员要做的就是定义好这两个阶段的函数：map函数和reduce函数。 下面我从逻辑实体的角度讲解mapreduce运行机制，这些按照时间顺序包括：输入分片（input split）、map阶段、combiner阶段、shuffle阶段和reduce阶段。 输入分片（input split）：在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务，输入分片（input split）存储的并非数据本身，而是一个分片长度和一个记录数据的位置的数组，输入分片（input split）往往和hdfs的block（块）关系很密切，假如我们设定hdfs的块的大小是64mb，如果我们输入有三个文件，大小分别是3mb、65mb和127mb，那么mapreduce会把3mb文件分为一个输入分片（input split），65mb则是两个输入分片（input split）而127mb也是两个输入分片（input split），换句话说我们如果在map计算前做输入分片调整，例如合并小文件，那么就会有5个map任务将执行，而且每个map执行的数据大小不均，这个也是mapreduce优化计算的一个关键点。 map阶段：就是程序员编写好的map函数了，因此map函数效率相对好控制，而且一般map操作都是本地化操作也就是在数据存储节点上进行； combiner阶段：combiner阶段是程序员可以选择的，combiner其实也是一种reduce操作，因此我们看见WordCount类里是用reduce进行加载的。Combiner是一个本地化的reduce操作，它是map运算的后续操作，主要是在map计算出中间文件前做一个简单的合并重复key值的操作，例如我们对文件里的单词频率做统计，map计算时候如果碰到一个hadoop的单词就会记录为1，但是这篇文章里hadoop可能会出现n多次，那么map输出文件冗余就会很多，因此在reduce计算前对相同的key做一个合并操作，那么文件会变小，这样就提高了宽带的传输效率，毕竟hadoop计算力宽带资源往往是计算的瓶颈也是最为宝贵的资源，但是combiner操作是有风险的，使用它的原则是combiner的输入不会影响到reduce计算的最终输入，例如：如果计算只是求总数，最大值，最小值可以使用combiner，但是做平均值计算使用combiner的话，最终的reduce计算结果就会出错。 shuffle阶段：将map的输出作为reduce的输入的过程就是shuffle了，这个是mapreduce优化的重点地方。这里我不讲怎么优化shuffle阶段，讲讲shuffle阶段的原理，因为大部分的书籍里都没讲清楚shuffle阶段。Shuffle一开始就是map阶段做输出操作，一般mapreduce计算的都是海量数据，map输出时候不可能把所有文件都放到内存操作，因此map写入磁盘的过程十分的复杂，更何况map输出时候要对结果进行排序，内存开销是很大的，map在做输出时候会在内存里开启一个环形内存缓冲区，这个缓冲区专门用来输出的，默认大小是100mb，并且在配置文件里为这个缓冲区设定了一个阀值，默认是0.80（这个大小和阀值都是可以在配置文件里进行配置的），同时map还会为输出操作启动一个守护线程，如果缓冲区的内存达到了阀值的80%时候，这个守护线程就会把内容写到磁盘上，这个过程叫spill，另外的20%内存可以继续写入要写进磁盘的数据，写入磁盘和写入内存操作是互不干扰的，如果缓存区被撑满了，那么map就会阻塞写入内存的操作，让写入磁盘操作完成后再继续执行写入内存操作，前面我讲到写入磁盘前会有个排序操作，这个是在写入磁盘操作时候进行，不是在写入内存时候进行的，如果我们定义了combiner函数，那么排序前还会执行combiner操作。 每次spill操作也就是写入磁盘操作时候就会写一个溢出文件，也就是说在做map输出有几次spill就会产生多少个溢出文件，等map输出全部做完后，map会合并这些输出文件。这个过程里还会有一个Partitioner操作，对于这个操作很多人都很迷糊，其实Partitioner操作和map阶段的输入分片（Input split）很像，一个Partitioner对应一个reduce作业，如果我们mapreduce操作只有一个reduce操作，那么Partitioner就只有一个，如果我们有多个reduce操作，那么Partitioner对应的就会有多个，Partitioner因此就是reduce的输入分片，这个程序员可以编程控制，主要是根据实际key和value的值，根据实际业务类型或者为了更好的reduce负载均衡要求进行，这是提高reduce效率的一个关键所在。到了reduce阶段就是合并map输出文件了，Partitioner会找到对应的map输出文件，然后进行复制操作，复制操作时reduce会开启几个复制线程，这些线程默认个数是5个，程序员也可以在配置文件更改复制线程的个数，这个复制过程和map写入磁盘过程类似，也有阀值和内存大小，阀值一样可以在配置文件里配置，而内存大小是直接使用reduce的tasktracker的内存大小，复制时候reduce还会进行排序操作和合并文件操作，这些操作完了就会进行reduce计算了。 reduce阶段：和map函数一样也是程序员编写的，最终结果是存储在hdfs上的。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>PCA</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习维度规约之PCA]]></title>
    <url>%2Fartificial-intelligence%2Fmachine-learning%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BB%B4%E5%BA%A6%E8%A7%84%E7%BA%A6%E4%B9%8BPCA%2F</url>
    <content type="text"><![CDATA[PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维. 算法步骤： 第一步，分别求每列的平均值，然后对于所有的样例，都减去对应的均值。 第二步，求特征协方差矩阵。 第三步，求协方差的特征值和特征向量。 第四步，将特征值按照从大到小的顺序排序，选择其中最大的k个，然后将其对应的k个特征向量分别作为列向量组成特征向量矩阵。 第五步，将样本点投影到选取的特征向量上。假设样例数为m，特征数为n，减去均值后的样本矩阵为DataAdjust(mn)，协方差矩阵是nn，选取的k个特征向量组成的矩阵为EigenVectors(n*k)。 有两篇文章写的不错：http://www.jianshu.com/p/673d4fe72362https://zhuanlan.zhihu.com/p/21580949 PCA与Autoencoder的关系当AutoEncoder只有一个隐含层的时候，其原理相当于主成分分析（PCA）。但是 AutoEncoder 明显比PCA的效果更好一点，尤其在图像上。 当AutoEncoder有多个隐含层的时候，每两层之间可以用RBM来pre-training，最后由BP来调整最终权值。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>PCA</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python2跟python3的主要区别]]></title>
    <url>%2Funcategorized%2Fpython2%E8%B7%9Fpython3%E7%9A%84%E4%B8%BB%E8%A6%81%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[Python的3​​.0版本，常被称为Python 3000，或简称Py3k。相对于Python的早期版本，这是一个较大的升级。为了不带入过多的累赘，Python 3.0在设计的时候没有考虑向下相容。许多针对早期Python版本设计的程式都无法在Python 3.0上正常执行。为了照顾现有程式，Python 2.6作为一个过渡版本，基本使用了Python 2.x的语法和库，同时考虑了向Python 3.0的迁移，允许使用部分Python 3.0的语法与函数。新的Python程式建议使用Python 3.0版本的语法。 future模块Python 3.x引入了一些与Python 2不兼容的关键字和特性，在Python 2中，可以通过内置的future模块导入这些新内容。如果你希望在Python 2环境下写的代码也可以在Python 3.x中运行，那么建议使用future模块。例如，如果希望在Python 2中拥有Python 3.x的整数除法行为，可以通过下面的语句导入相应的模块。 print函数虽然print语法是Python 3中一个很小的改动，且应该已经广为人知，但依然值得提一下：Python 2中的print语句被Python 3中的print()函数取代，这意味着在Python 3中必须用括号将需要输出的对象括起来。 整数除法由于人们常常会忽视Python 3在整数除法上的改动（写错了也不会触发Syntax Error），所以在移植代码或在Python 2中执行Python 3的代码时，需要特别注意这个改动。 所以，我还是会在Python 3的脚本中尝试用float(3)/2或 3/2.0代替3/2，以此来避免代码在Python 2环境下可能导致的错误（或与之相反，在Python 2脚本中用from future import division来使用Python 3的除法）。 UnicodePython 2有基于ASCII的str()类型，其可通过单独的unicode()函数转成unicode类型，但没有byte类型。而在Python 3中，终于有了Unicode（utf-8）字符串，以及两个字节类：bytes和bytearrays。 xrange在Python 2.x中，经常会用xrange()创建一个可迭代对象，通常出现在“for循环”或“列表/集合/字典推导式”中。这种行为与生成器非常相似（如”惰性求值“），但这里的xrange-iterable无尽的，意味着可能在这个xrange上无限迭代。由于xrange的“惰性求知“特性，如果只需迭代一次（如for循环中），range()通常比xrange()快一些。不过不建议在多次迭代中使用range()，因为range()每次都会在内存中重新生成一个列表。 在Python 3中，range()的实现方式与xrange()函数相同，所以就不存在专用的xrange()（在Python 3中使用xrange()会触发NameError）。Python 3中的range对象中的contains方法 另一个值得一提的是，在Python 3.x中，range有了一个新的contains方法。 触发异常Python 2支持新旧两种异常触发语法，而Python 3只接受带括号的的语法（不然会触发SyntaxError）： 异常处理Python 3中的异常处理也发生了一点变化。在Python 3中必须使用“as”关键字。 For循环变量与全局命名空间泄漏好消息是：在Python 3.x中，for循环中的变量不再会泄漏到全局命名空间中了！ 比较无序类型Python 3中另一个优秀的改动是，如果我们试图比较无序类型，会触发一个TypeError。 通过input()解析用户的输入幸运的是，Python 3改进了input()函数，这样该函数就会总是将用户的输入存储为str对象。在Python 2中，为了避免读取非字符串类型会发生的一些危险行为，不得不使用raw_input()代替input()。 返回可迭代对象，而不是列表在xrange一节中可以看到，某些函数和方法在Python中返回的是可迭代对象，而不像在Python 2中返回列表。由于通常对这些对象只遍历一次，所以这种方式会节省很多内存。然而，如果通过生成器来多次迭代这些对象，效率就不高了。此时我们的确需要列表对象，可以通过list()函数简单的将可迭代对象转成列表。下面列出了Python 3中其他不再返回列表的常用函数和方法：zip()map()filter()字典的.key()方法字典的.value()方法字典的.item()方法]]></content>
  </entry>
  <entry>
    <title><![CDATA[深度学习之反向传播]]></title>
    <url>%2Fartificial-intelligence%2Fdeep-learning%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%2F</url>
    <content type="text"><![CDATA[BP算法实际上是一种近似的最优解决方案，背后的原理仍然是梯度下降，但为了解决上述困难，其方案是将多层转变为一层接一层的优化：只优化一层的参数是可以得到显式梯度下降表达式的；而顺序呢必须反过来才能保证可工作——由输出层开始优化前一层的参数，然后优化再前一层……跑一遍下来，那所有的参数都优化过一次了。 https://www.zhihu.com/question/24827633/answer/29189075 maxpool 怎么做反向传播？对于max-pooling，在前向计算时，是选取的每个22区域中的最大值，这里需要记录下最大值在每个小区域中的位置。在反向传播时，只有那个最大值对下一层有贡献，所以将残差传递到该最大值的位置，区域内其他22-1=3个位置置零。对于mean-pooling，我们需要把残差平均分成2*2=4份，传递到前边小区域的4个单元即可。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>deep-learning</category>
      </categories>
      <tags>
        <tag>deep-learning</tag>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据处理之Box-Cox变换]]></title>
    <url>%2Fartificial-intelligence%2Fmachine-learning%2F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B9%8BBox-Cox%E5%8F%98%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[日常解决业务问题的时候，不管是线性回归分析，还是分析问题的时候，都需要把数据变换成类似正态分布的样子，然而统计分析中，基础数据的分布可能比较特别，不符合所谓的“正态分布”。那么数据不正态能怎么办呢？变换啊 原始数据是非正态分布的，或者原始数据右偏，或者左偏，需要对原数据做一定的变换。 正态分布的变换，比较经典的就是BOX-COX transformation 原则它得保持原来样本里头数据的大小次序关系。 Box-Cox 变换看着很复杂？我们来把它解剖一下，你会发现其实挺简单。这里出现的 λ，是一个有待确定的常数。这里λ是一个待定变换参数。对不同的λ，所做的变换自然就不同，所以是一个变换族。它包括了对数变换（λ＝0），平方根变换（λ = 1/2）和倒数变换λ＝-１）等常用变换。最关键的问题在于怎样选定一个最优的 λ，使得变换后的样本（及总体）正态性最好。要找到使变换后样本正态性最好的那个 λ，我们只需在所有的 λ 里找出使得正态假设下似然函数最大的那一个。所以，关键点还是最大似然估计。在关于λ的对数最大似然图像上找估计值的95%置信区间。尽管 Box-Cox 变换十分强大，但是它依旧不是全能的。它只能在幂函数和对数函数中「搜索」出最好的变换，但不能保证一定能达到正态性。 数据变换的局限性 数据变换并不能解决所有非正态性的问题 对数据进行变换后，重新进行原来计划的统计检验，其意义会发生变化。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>AUC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git 创建本地分支管理远程分支]]></title>
    <url>%2Ftechnology%2Ftools%2Fgit-%E5%88%9B%E5%BB%BA%E6%9C%AC%E5%9C%B0%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF%2F</url>
    <content type="text"><![CDATA[本地Git仓库和远程仓库的创建及关联 创建远程仓库界面操作，没说的切记:如果我们在创建远程仓库的时候添加了README和.ignore等文件,我们在后面关联仓库后,需要先执行pull操作 ### 创建本地分支git init注意:Git会自动为我们创建唯一一个master分支我们能够发现在当前目录下多了一个.git的目录，这个目录是Git来跟踪管理版本库的，千万不要手动修改这个目录里面的文件，不然改乱了，就把Git仓库给破坏了。 添加远程分支1git remote add origin git@github.com:muyunzhe/helloTest.git 备注:origin就是我们的远程库的名字，这是Git默认的叫法，也可以改成别的;git@github.com:YotrolZ/helloTest.git是我们远程仓库的路径(这里我们使用的github) 本地创建文件并提交123vim ***add .git commit -m 'xxx' 拉远程分支原因上面已经提到了用命令12git checkout -b initgit pull origin init 本地推送到远程1git push --set-upstream origin init 备注：这时候已经讲本地的init分支跟远程的init分支建立关联了，以后的push操作在init分支下直接用git push就可以默认push到远程的init分支了]]></content>
      <categories>
        <category>technology</category>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之特征选择]]></title>
    <url>%2Fartificial-intelligence%2Fmachine-learning%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[数据工程项目往往严格遵循着riro (rubbish in, rubbish out) 的原则，所以我们经常说数据预处理是数据工程师或者数据科学家80%的工作，它保证了数据原材料的质量。而特征工程又至少占据了数据预处理的半壁江山，在实际的数据工程工作中，无论是出于解释数据或是防止过拟合的目的，特征选择都是很常见的工作。如何从成百上千个特征中发现其中哪些对结果最具影响，进而利用它们构建可靠的机器学习算法是特征选择工作的中心内容。 模型训练的步骤： 盗图如下： 数据预处理无量纲化 无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和区间缩放法。标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。 标准化 标准化需要计算特征的均值和标准差 归一化```x` = （x-min）/(max - min)``` 标准化与归一化的区别 简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。 二值化 定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0 one-hot编码 定性特征转定量特征 缺失值处理数据变换常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的。 特征选择 通常来说，从特征意义方面考虑来选择特征： 特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。 根据特征选择的形式又可以将特征选择方法分为3种： Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。 Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。 Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。 filter 方差选择法使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。 相关系数法使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。 卡方检验经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量 互信息法经典的互信息也是评价定性自变量对定性因变量的相关性的 wrapper 递归特征消除法递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。 embedded使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。 降维当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法除了以上提到的基于L1惩罚项的模型以外，另外还有主成分分析法（PCA）和线性判别分析（LDA），线性判别分析本身也是一个分类模型。PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。 sklearn包包 类 参数列表 类别 fit方法有用 说明sklearn.preprocessing StandardScaler 特征 无监督 Y 标准化sklearn.preprocessing MinMaxScaler 特征 无监督 Y 区间缩放sklearn.preprocessing Normalizer 特征 无信息 N 归一化sklearn.preprocessing Binarizer 特征 无信息 N 定量特征二值化sklearn.preprocessing OneHotEncoder 特征 无监督 Y 定性特征编码sklearn.preprocessing Imputer 特征 无监督 Y 缺失值计算sklearn.preprocessing PolynomialFeatures 特征 无信息 N 多项式变换（fit方法仅仅生成了多项式的表达式）sklearn.preprocessing FunctionTransformer 特征 无信息 N 自定义函数变换（自定义函数在transform方法中调用）sklearn.feature_selection VarianceThreshold 特征 无监督 Y 方差选择法sklearn.feature_selection SelectKBest 特征/特征+目标值 无监督/有监督 Y 自定义特征评分选择法sklearn.feature_selection SelectKBest+chi2 特征+目标值 有监督 Y 卡方检验选择法sklearn.feature_selection RFE 特征+目标值 有监督 Y 递归特征消除法sklearn.feature_selection SelectFromModel 特征+目标值 有监督 Y 自定义模型训练选择法sklearn.decomposition PCA 特征 无监督 Y PCA降维sklearn.lda LDA 特征+目标值 有监督 Y LDA降维 作者：城东链接：https://www.zhihu.com/question/28641663/answer/110165221来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive 数据倾斜解决方案]]></title>
    <url>%2Fcloud-computing%2Fstorage%2Fhive-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[数据倾斜是hive工程中常见的情况。 对sum，count,max,min来说，不存在数据倾斜问题。 在开始之前，先把MR的流程图帖出来（摘自Hadoop权威指南），方便后面对照。 表现任务进度长时间维持在99%（或100%），查看任务监控页面，发现只有少量（1个或几个）reduce子任务未完成。 原因 1)、key分布不均匀 2)、业务数据本身的特性 3)、建表时考虑不周 4)、某些SQL语句本身就有数据倾斜 map阶段的优化 mapred.min.split.size指的是数据的最小分割单元大小。 mapred.max.split.size指的是数据的最大分割单元大小。 dfs.block.size指的是HDFS设置的数据块大小。一般来说dfs.block.size这个值是一个已经指定好的值，而且这个参数hive是识别不到的,所以实际上只有mapred.min.split.size和mapred.max.split.size这两个参数来决定map数量。在hive中min的默认值是1B，max的默认值是256MB.所以如果不做修改的话，就是1个map task处理256MB数据，我们就以调整max为主。通过调整max可以起到调整map数的作用，减小max可以增加map数，增大max可以减少map数。需要提醒的是，直接调整mapred.map.tasks这个参数是没有效果的。调整大小的时机根据查询的不同而不同，总的来讲可以通过观察map task的完成时间来确定是否需要增加map资源。如果map task的完成时间都是接近1分钟，甚至几分钟了，那么往往增加map数量，使得每个map task处理的数据量减少，能够让map task更快完成；而如果map task的运行时间已经很少了，比如10-20秒，这个时候增加map不太可能让map task更快完成，反而可能因为map需要的初始化时间反而让job总体速度变慢，这个时候反而需要考虑是否可以把map的数量减少，这样可以节省更多资源给其他Job。 reduce阶段的优化与map优化不同的是，reduce优化时，可以直接设置mapred.reduce.tasks参数从而直接指定reduce的个数。当然直接指定reduce个数虽然比较方便，但是不利于自动扩展。Reduce数的设置虽然相较map更灵活，但是也可以像map一样设定一个自动生成规则，这样运行定时job的时候就不用担心原来设置的固定reduce数会由于数据量的变化而不合适。 map与reduce之间的优化map phase和reduce phase之间主要有3道工序。首先要把map输出的结果进行排序后做成中间文件，其次这个中间文件就能分发到各个reduce，最后reduce端在执行reduce phase之前把收集到的排序子文件合并成一个排序文件。需要强调的是，虽然这个部分可以调的参数挺多，但是大部分在一般情况下都是不要调整的，除非能精准的定位到这个部分有问题。 spill在spill阶段，由于内存不够，数据可能没办法在内存中一次性排序完成，那么就只能把局部排序的文件先保存到磁盘上，这个动作叫spill，然后spill出来的多个文件可以在最后进行merge。如果发生spill，可以通过设置io.sort.mb来增大mapper输出buffer的大小，避免spill的发生。另外合并时可以通过设置io.sort.factor来使得一次性能够合并更多的数据，默认值为10，也就是一次归并10个文件。调试参数的时候，一个要看spill的时间成本，一个要看merge的时间成本，还需要注意不要撑爆内存（io.sort.mb是算在map的内存里面的）。Reduce端的merge也是一样可以用io.sort.factor。一般情况下这两个参数很少需要调整，除非很明确知道这个地方是瓶颈。比如如果map端的输出太大，考虑到map数不一定能很方便的调整，那么这个时候就要考虑调大io.sort.mb（不过即使调大也要注意不能超过jvm heap size）。而map端的输出很大，要么是每个map读入了很大的文件（比如不能split的大gz压缩文件），要么是计算逻辑导致输出膨胀了很多倍，都是比较少见的情况。 Copy这里说的copy，一般叫做shuffle更加常见。但是由于一开始的配图以及MR job的web监控页对这个环节都是叫copy phase，指代更加精确，所以这里称为copy。 copy阶段是把文件从map端copy到reduce端。默认情况下在5%的map完成的情况下reduce就开始启动copy，这个有时候是很浪费资源的，因为reduce一旦启动就被占用，一直等到map全部完成，收集到所有数据才可以进行后面的动作，所以我们可以等比较多的map完成之后再启动reduce流程，这个比例可以通过mapred.reduce.slowstart.completed.maps去调整，他的默认值就是5%。如果觉得这么做会减慢reduce端copy的进度，可以把copy过程的线程增大。tasktracker.http.threads可以决定作为server端的map用于提供数据传输服务的线程，mapred.reduce.parallel.copies可以决定作为client端的reduce同时从map端拉取数据的并行度（一次同时从多少个map拉数据），修改参数的时候这两个注意协调一下，server端能处理client端的请求即可。 另外，在shuffle阶段可能会出现的OOM问题，原因比较复杂，一般认为是内存分配不合理，GC无法及时释放内存导致。对于这个问题，可以尝试调低shuffle buffer的控制参数mapred.job.shuffle.input.buffer.percent这个比例值，默认值0.7，即shuffle buffer占到reduce task heap size的70%。另外也可以直接尝试增加reduce数量。 解决方案group by类hive.map.aggr=true; #Map 端部分聚合，相当于Combiner，也就是在mapper里面做聚合。这个方法不同于直接写mapreduce的时候可以实现的combiner，但是却实现了类似combiner的效果。hive.groupby.skewindata; 意思是做reduce操作的时候，拿到的key并不是所有相同值给同一个reduce，而是随机分发，然后reduce做聚合，做完之后再做一轮MR，拿前面聚合过的数据再算结果。所以这个参数其实跟hive.map.aggr做的是类似的事情，只是拿到reduce端来做，而且要额外启动一轮job，所以其实不怎么推荐用，效果不明显。 count distinct类使用disticnt函数，所有的数据只会shuffle到一个reducer上，导致reducer数据倾斜严重。count(*) 可以在多个reduce上执行。直接改写sql12345/*改写前*/select a, count(distinct b) as c from tbl group by a;/*改写后*/select a, count(*) as c from (select a, b from tbl group by a, b) group by a; Join类大表与大表 设置参数set hive.optimize.skewjoin = true;set hive.skewjoin.key=100000; 这个是join的键对应的记录条数超过这个值则会进行优化。有数据倾斜的时候进行负载均衡，当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果到hdfs，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。 加随机数就是在join的时候增加一个随机数，随机数的取值范围n相当于将item给分散到n个reducer。 12345select a.*, b.*from (select *, cast(rand() * 10 as int) as r_id from logs)ajoin (select *, r_id from itemslateral view explode(range_list(1,10)) rl as r_id)bon a.item_id = b.item_id and a.r_id = b.r_id 上面的写法里，对行为表的每条记录生成一个1-10的随机整数，对于item属性表，每个item生成10条记录，随机key分别也是1-10，这样就能保证行为表关联上属性表。其中range_list(1,10)代表用udf实现的一个返回1-10整数序列的方法。这个做法是一个解决join倾斜比较根本性的通用思路，就是如何用随机数将key进行分散。当然，可以根据具体的业务场景做实现上的简化或变化。 小表与大表如何Join：关于驱动表的选取，选用join key分布最均匀的表作为驱动表做好列裁剪和filter操作，以达到两表做join的时候，数据量相对变小的效果。大小表Join：使用map join让小的维度表（1000条以下的记录条数,小于25m） 先进内存。在map端完成reduce.1select /*+mapjoin(a)*/ count(1) from tb_a a left outer join tb_b b on a.uid=b.uid； 有空值的key：把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null值关联不上，处理后并不影响最终结果。count distinct大量相同特殊值count distinct时，将值为空的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果中加1。如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行union。group by维度过小：采用sum() group by的方式来替换count(distinct)完成计算。特殊情况特殊处理：在业务逻辑优化效果的不大情况下，有些时候是可以将倾斜的数据单独拿出来处理。最后union回去。如：1234select * from log a left outer join users b on case when a.user_id is null then concat(‘hive’,rand() ) else a.user_id end = b.user_id; 不同数据类型关联产生数据倾斜用户表中user_id字段为int，log表中user_id字段既有string类型也有int类型。当按照user_id进行两个表的Join操作时，默认的Hash操作会按int型的id来进行分配，这样会导致所有string类型id的记录都分配到一个Reducer中。123select * from users a left outer join logs b on a.usr_id = cast(b.user_id as string) 小表不小不大，怎么用 map join 解决倾斜问题使用 map join 解决小表(记录数少)关联大表的数据倾斜问题，这个方法使用的频率非常高，但如果小表很大，大到不能放入内存处理，那么把小表分发到不同的map也是个不小的开销。12345678select * from log a left outer join ( select d.* from ( select distinct user_id from log ) c join users d on c.user_id = d.user_id ) x on a.user_id = b.user_id; distribute by类distribute by是控制在map端如何拆分数据给reduce端的。hive会根据distribute by后面列，对应reduce的个数进行分发，默认是采用hash算法。sort by为每个reduce产生一个排序文件。在有些情况下，你需要控制某个特定行应该到哪个reducer，这通常是为了进行后续的聚集操作。distribute by刚好可以做这件事。因此，distribute by经常和sort by配合使用。 sql整体优化job间并行设置job间并行的参数是hive.exec.parallel，将其设为true即可。默认的并行度为8，也就是最多允许sql中8个job并行。如果想要更高的并行度，可以通过hive.exec.parallel. thread.number参数进行设置，但要避免设置过大而占用过多资源。 减少job数量通过代码优化 控制map数两种方式控制Map数减少map数可以通过合并小文件来实现，这点是对文件数据源来讲，下面介绍。增加map数的可以通过控制上一个job的reduer数来控制. 控制reduce数有多少个reduce,就会有多少个输出文件。 合并小文件是否合并Map输出文件：hive.merge.mapfiles=true（默认值为真）是否合并Reduce 端输出文件：hive.merge.mapredfiles=false（默认值为假）Map输入合并：set mapred.max.split.size=1000000000;set mapred.min.split.size.per.node=1000000000;set mapred.min.split.size.per.rack=1000000000;set hive.merge.size.per.task=25610001000（默认值为 256000000） 上面部分是针对从很多小文件的表里读取的时候用的，并不能合并小文件。以下配置在合并小文件时亲测有效hive结果合并：set hive.merge.mapfiles=true;set hive.merge.mapredfiles=true;set hive.merge.smallfiles.avgsize=256000000;set hive.merge.size.per.task=256000000; 以下是个小集合：123456789101112131415161718192021222324-- 动态分区SET hive.exec.dynamic.partition=true;SET hive.exec.dynamic.partition.mode=nonstrict;SET hive.exec.max.dynamic.partitions.pernode=10000;SET hive.exec.max.dynamic.partitions=100000;SET hive.exec.max.created.files=1000000;-- map读入合并set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;set mapred.max.split.size=4294967296;set mapred.min.split.size.per.node=4294967296;set mapred.min.split.size.per.rack=4294967296;-- hive输出合并set hive.merge.mapfiles=true;set hive.merge.mapredfiles=true;set hive.merge.smallfiles.avgsize=256000000;set hive.merge.size.per.task=256000000;-- 输出压缩SET mapred.output.compress=true;SET mapred.output.compression.type=BLOCK;SET mapred.output.compression.codec=org.apache.hadoop.io.compress.Lz4Codec;SET mapred.map.output.compression.codec=org.apache.hadoop.io.compress.Lz4Codec;]]></content>
      <categories>
        <category>cloud-computing</category>
        <category>storage</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GBDT+LR模型实现离线+实时特征层次化预测]]></title>
    <url>%2Fartificial-intelligence%2Fmachine-learning%2FGBDT-LR%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0%E7%A6%BB%E7%BA%BF-%E5%AE%9E%E6%97%B6%E7%89%B9%E5%BE%81%E5%B1%82%E6%AC%A1%E5%8C%96%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[特征决定了所有算法效果的上限，而不同的算法只是离这个上限的距离不同而已。 本文中我将介绍Facebook发表的利用GBDT模型构造新特征的方法。 论文的思想很简单，就是先用已有特征训练GBDT模型，然后利用GBDT模型学习到的树来构造新特征，最后把这些新特征加入原有特征一起训练模型。构造的新特征向量是取值0/1的，向量的每个元素对应于GBDT模型中树的叶子结点。当一个样本点通过某棵树最终落在这棵树的一个叶子结点上，那么在新特征向量中这个叶子结点对应的元素值为1，而这棵树的其他叶子结点对应的元素值为0。新特征向量的长度等于GBDT模型里所有树包含的叶子结点数之和。 举例说明。下面的图中的两棵树是GBDT学习到的，第一棵树有3个叶子结点，而第二棵树有2个叶子节点。对于一个输入样本点x，如果它在第一棵树最后落在其中的第二个叶子结点，而在第二棵树里最后落在其中的第一个叶子结点。那么通过GBDT获得的新特征向量为[0, 1, 0, 1, 0]，其中向量中的前三位对应第一棵树的3个叶子结点，后两位对应第二棵树的2个叶子结点。Hybrid model structure 那么，GBDT中需要多少棵树能达到效果最好呢？具体数字显然是依赖于你的应用以及你拥有的数据量。一般数据量较少时，树太多会导致过拟合。在作者的应用中，大概500棵左右效果就基本不改进了。另外，作者在建GBDT时也会对每棵树的叶子结点数做约束——不多于12个叶子结点。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据处理中的IV和WOE]]></title>
    <url>%2Fartificial-intelligence%2Fmachine-learning%2F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84IV%E5%92%8CWOE%2F</url>
    <content type="text"><![CDATA[我们在用逻辑回归、决策树等模型方法构建分类模型时，经常需要对自变量进行筛选。比如我们有200个候选自变量，通常情况下，不会直接把200个变量直接放到模型中去进行拟合训练，而是会用一些方法，从这200个自变量中挑选一些出来，放进模型，形成入模变量列表。那么我们怎么去挑选入模变量呢？ 挑选入模变量过程是个比较复杂的过程，需要考虑的因素很多，比如：变量的预测能力，变量之间的相关性，变量的简单性（容易生成和使用），变量的强壮性（不容易被绕过），变量在业务上的可解释性（被挑战时可以解释的通）等等。但是，其中最主要和最直接的衡量标准是变量的预测能力。 怎么去评价一个变量的预测能力呢？ IV对于一个待预测的个体A，要判断A属于Y1还是Y2，我们是需要一定的信息的，假设这个信息总量是I，而这些所需要的信息，就蕴含在所有的自变量C1，C2，C3，……，Cn中，那么，对于其中的一个变量Ci来说，其蕴含的信息越多，那么它对于判断A属于Y1还是Y2的贡献就越大，Ci的信息价值就越大，Ci的IV就越大。 对于一个待评估变量，他的IV值究竟如何计算呢？为了介绍IV的计算方法，我们首先需要认识和理解另一个概念——WOE，因为IV的计算是以WOE为基础的。 woe WOE的全称是“Weight of Evidence”，即证据权重。WOE是对原始自变量的一种编码形式。实际上是“当前分组中响应客户占所有响应客户的比例”和“当前分组中没有响应的客户占所有没有响应的客户的比例”的差异。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>AUC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征选择]]></title>
    <url>%2Fartificial-intelligence%2Fmachine-learning%2F%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[给定一些特征，如何选择其中一些重要特征以减少特征数量，同时尽量保留分类信息？这个过程叫做特征选择或者特征压缩。其中一种方法是分别处理每个特征，去除掉哪些几乎不具辨识能力的特征；另一种方法是讲特征综合考虑，可以将特征进行线性变换或者非线性变换，使其具有更好的辨识特性。 数据预处理剔除离群点 离群点可能会对训练产生很大误差，如果离群点比较少，可以将其剔除，如果离群点符合长尾分布，那只能选择对离群点不敏感的代价函数。最小平方准则对离群点比较敏感，因此就不能选择。 数据归一化 减均值除方差 所有归一化后的特征具有零均值和单位方差 丢失数据 0 填充 绝对均值，从各自特征的有效值计算 条件均值，如果能够估计丢失值的概率密度函数 丢弃目前比较流行的是条件概率分布填充，它的思想是根据丢失值表现的统计特性来填充。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>PCA</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gbdt算法原理]]></title>
    <url>%2Fartificial-intelligence%2Fmachine-learning%2Fgbdt%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[GBDT(Gradient Boosting Decision Tree) 又叫 MART（Multiple Additive Regression Tree)，是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。 DT：回归树 Regression Decision Tree切记：GBDT中的树都是回归树，不是分类树 GB：梯度迭代 Gradient BoostingGBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。 工作过程残差的意思就是： A的预测值 + A的残差 = A的实际值 问题总结为何需要GBDT防止过拟合。 Boosting的最大好处在于，每一步的残差计算其实变相地增大了分错instance的权重，而已经分对的instance则都趋向于0。这样后面的树就能越来越专注那些前面被分错的instance。 这不是boosting吧？Adaboost可不是这么定义的。这是boosting，但不是Adaboost。GBDT不是Adaboost Decistion Tree。就像提到决策树大家会想起C4.5，提到boost多数人也会想到Adaboost。Adaboost是另一种boost方法，它按分类对错，分配不同的weight，计算cost function时使用这些weight，从而让“错分的样本权重越来越大，使它们更被重视”。Bootstrap也有类似思想，它在每一步迭代时不改变模型本身，也不计算残差，而是从N个instance训练集中按一定概率重新抽取N个instance出来（单个instance可以被重复sample），对着这N个新的instance再训练一轮。由于数据集变了迭代模型训练结果也不一样，而一个instance被前面分错的越厉害，它的概率就被设的越高，这样就能同样达到逐步关注被分错的instance，逐步完善的效果。 GBDT的适用范围GBDT几乎可用于所有回归问题（线性/非线性），相对logistic regression仅能用于线性回归，GBDT的适用面非常广。亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xgboost算法原理]]></title>
    <url>%2Fartificial-intelligence%2Fmachine-learning%2Fxgboost%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[xgboost的全称是eXtreme Gradient Boosting。正如其名，它是Gradient Boosting Machine的一个c++实现，作者为正在华盛顿大学研究机器学习的大牛陈天奇。他在研究中深感自己受制于现有库的计算速度和精度，因此在一年前开始着手搭建xgboost项目，并在去年夏天逐渐成型。xgboost最大的特点在于，它能够自动利用CPU的多线程进行并行，同时在算法上加以改进提高了精度。 和传统的boosting tree模型一样，xgboost的提升模型也是采用的残差（或梯度负方向），不同的是分裂结点选取的时候不一定是最小平方损失。 Xgboost引入了二阶导来进行求解，并且引入了节点的数目、参数的L2正则来评估模型的复杂度。xgboost的特性： 显示的把树模型复杂度作为正则项加到优化目标中。 公式推导中用到了二阶导数，用了二阶泰勒展开。（GBDT用牛顿法貌似也是二阶信息） 实现了分裂点寻找近似算法。 利用了特征的稀疏性。 数据事先排序并且以block形式存储，有利于并行计算。 基于分布式通信框架rabit，可以运行在MPI和yarn上。（最新已经不基于rabit了） 实现做了面向体系结构的优化，针对cache和内存做了性能优化。 xgboost跟gdbt比较大的不同是目标函数的定义。注：红色箭头指向的l即为损失函数；红色方框为正则项，包括L1、L2；红色圆圈为常数项。xgboost利用泰勒展开三项，做一个近似，我们可以很清晰地看到，最终的目标函数只依赖于每个数据点的在误差函数上的一阶导数和二阶导数。 原理定义树的复杂度 对于f的定义做一下细化，把树拆分成结构部分q和叶子权重部分w。 定义这个复杂度包含了一棵树里面节点的个数，以及每个树叶子节点上面输出分数的L2模平方。 打分函数计算示例Obj代表了当我们指定一个树的结构的时候，我们在目标上面最多减少多少。我们可以把它叫做结构分数(structure score) 分裂节点1、暴力枚举 2、近似方法 ，近似方法通过特征的分布，按照百分比确定一组候选分裂点，通过遍历所有的候选分裂点来找到最佳分裂点。两种策略：全局策略和局部策略。在全局策略中，对每一个特征确定一个全局的候选分裂点集合，就不再改变；而在局部策略中，每一次分裂 都要重选一次分裂点。前者需要较大的分裂集合，后者可以小一点。对比补充候选集策略与分裂点数目对模型的影响。 全局策略需要更细的分裂点才能和局部策略差不多 3、Weighted Quantile Sketch 定义损失函数见上图 调参 参考文章： xgboost原理]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随机森林跟GDBT的区别]]></title>
    <url>%2Fartificial-intelligence%2Fmachine-learning%2F%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E8%B7%9FGDBT%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[决策树这种算法有着很多良好的特性，比如说训练时间复杂度较低，预测的过程比较快速，模型容易展示。单决策树又有一些不好的地方，比如说容易over-fitting。多个决策树协同决策，能够有效提升模型的预测精度。 bagging 基于bootstrap sampling 自助采样法，重复性有放回的随机采用部分样本进行训练最后再将结果 voting 或者 averaging 。 并行式算法 每一层大约花费时间相同 主要关注于降低方差，但是不能降低偏差 随机森林在bagging的基础上，加入随机属性选择机制。 对于回归模型，选择属性数量，建议选择全部属性的三分之一 对于分类模型，选择属性数量，建议选择全部属性的平方根 随机森林对于高维数据集的处理能力很好，它可以处理成千上万的输入变量，并确定最重要的变量，因此被认为是一个不错的降维方法。 高度并行化，易于分布式实现 在解决回归问题时并没有它在分类中变现那么好，主要因为它不能给出一个连续的输出 无法控制模型内部的运行，等同于黑盒子 调参方法：网格搜索 n_estimators越多越好，但计算量对大增 随机森林不进行剪枝。决策树剪枝是因为防止过拟合，而随机森林的“随机”已经防止了过拟合，因此不需要剪枝。GBDTGBDT中的树都是回归树，不是分类树 ，因为gradient boost 需要按照损失函数的梯度近似的拟合残差，这样拟合的是连续数值，因此只有回归树。在Gradient Boosting中，每个新的模型的建立是为了使得之前模型的残差往梯度方向减少，与传统Boosting对正确、错误样本进行加权有着很大的区别。 XGBoost单独再说吧 随机森林与GBDT区别相同点：（1）都是由多棵树组成的，都是集成学习算法（2）最终的结果都是由多颗树一起决定 不同点：(1)组成随机森林的树可以是分类树，也可以是回归树，但是GBDT只能由回归树组成。(2)组成随机森林的树可以并行生成，但是组成GBDT的树只能串行生成。(3)对于最终的输出结果，随机森林采用多数投票；而GBDT是将所有的结果累加起来，或者加权起来(4)随机森林对异常值不敏感，而GBDT对异常值非常敏感(5)随机森林通过减小方差来提高性能，GBDT通过减小偏差来提高性能]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络物理层面的解释]]></title>
    <url>%2Fartificial-intelligence%2Fdeep-learning%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%89%A9%E7%90%86%E5%B1%82%E9%9D%A2%E7%9A%84%E8%A7%A3%E9%87%8A%2F</url>
    <content type="text"><![CDATA[Deep Learning是全部深度学习算法的总称，CNN是深度学习算法在图像处理领域的一个应用。CNN已经成为众多科学领域的研究热点之一，特别是在模式分类领域，由于该网络避免了对图像的复杂前期预处理，可以直接输入原始图像，因而得到了更为广泛的应用。 一般地，CNN的基本结构包括两层: 其一为特征提取层，每个神经元的输入与前一层的局部接受域相连，并提取该局部的特征。一旦该局部特征被提取后，它与其它特征间的位置关系也随之确定下来； 其二是特征映射层，网络的每个计算层由多个特征映射组成，每个特征映射是一个平面，平面上所有神经元的权值相等。特征映射结构采用影响函数核小的sigmoid函数作为卷积网络的激活函数，使得特征映射具有位移不变性。此外，由于一个映射面上的神经元共享权值，因而减少了网络自由参数的个数。卷积神经网络中的每一个卷积层都紧跟着一个用来求局部平均与二次提取的计算层，这种特有的两次特征提取结构减小了特征分辨率。 步骤：卷积也叫滤波特征图的大小（卷积特征）由下面三个参数控制，我们需要在卷积前确定它们：深度（Depth）：深度对应的是卷积操作所需的滤波器个数。步长（Stride）：步长是我们在输入矩阵上滑动滤波矩阵的像素数。零填充（Zero-padding）：有时，在输入矩阵的边缘使用零值进行填充，这样我们就可以对输入图像矩阵的边缘进行滤波。 非线性处理ReLU 是一个元素级别的操作（应用到各个像素），并将特征图中的所有小于 0 的像素值设置为零。ReLU 的目的是在 ConvNet 中引入非线性，因为在大部分的我们希望 ConvNet 学习的实际数据是非线性的（卷积是一个线性操作——元素级别的矩阵相乘和相加，所以我们需要通过使用非线性函数 ReLU 来引入非线性。 池化空间池化（Spatial Pooling）（也叫做亚采用或者下采样）降低了各个特征图的维度，但可以保持大部分重要的信息。空间池化有下面几种方式：最大化、平均化、加和等等。池化函数可以逐渐降低输入表示的空间尺度。特别地，池化：使输入表示（特征维度）变得更小，并且网络中的参数和计算的数量更加可控的减小，因此，可以控制过拟合使网络对于输入图像中更小的变化、冗余和变换变得不变性（输入的微小冗余将不会改变池化的输出——因为我们在局部邻域中使用了最大化/平均值的操作。帮助我们获取图像最大程度上的尺度不变性（准确的词是“不变性”）。它非常的强大，因为我们可以检测图像中的物体，无论它们位置在哪里 全连接卷积和池化层的输出表示了输入图像的高级特征。全连接层的目的是为了使用这些特征把输入图像基于训练数据集进行分类。除了分类，添加一个全连接层也（一般）是学习这些特征的非线性组合的简单方法。从卷积和池化层得到的大多数特征可能对分类任务有效，但这些特征的组合可能会更好。从全连接层得到的输出概率和为 1。这个可以在输出层使用 softmax 作为激活函数进行保证。softmax 函数输入一个任意大于 0 值的矢量，并把它们转换为零一之间的数值矢量，其和为一。 训练过程反向传播完整的卷积网络的训练过程可以总结如下： 第一步：我们初始化所有的滤波器，使用随机值设置参数/权重 第二步：网络接收一张训练图像作为输入，通过前向传播过程（卷积、ReLU 和池化操作，以及全连接层的前向传播），找到各个类的输出概率 第三步：在输出层计算总误差（计算各类的和）Total Error = ∑ ½ (target probability – output probability) ² 第四步：使用反向传播算法，根据网络的权重计算误差的梯度，并使用梯度下降算法更新所有滤波器的值/权重以及参数的值，使输出误差最小化权重的更新与它们对总误差的占比有关当同样的图像再次作为输入，这时的输出概率可能会是 [0.1, 0.1, 0.7, 0.1]，这就与目标矢量 [0, 0, 1, 0] 更接近了这表明网络已经通过调节权重/滤波器，可以正确对这张特定图像的分类，这样输出的误差就减小了像滤波器数量、滤波器大小、网络结构等这样的参数，在第一步前都是固定的，在训练过程中保持不变——仅仅是滤波器矩阵的值和连接权重在更新 第五步：对训练数据中所有的图像重复步骤 1 ~ 4 上面的这些步骤可以训练 ConvNet —— 这本质上意味着对于训练数据集中的图像，ConvNet 在更新了所有权重和参数后，已经优化为可以对这些图像进行正确分类。当一张新的（未见过的）图像作为 ConvNet 的输入，网络将会再次进行前向传播过程，并输出各个类别的概率（对于新的图像，输出概率是使用已经在前面训练样本上优化分类的参数进行计算的）。如果我们的训练数据集非常的大，网络将会（有希望）对新的图像有很好的泛化，并把它们分到正确的类别中去。 注 1: 上面的步骤已经简化，也避免了数学详情，只为提供训练过程的直观内容。注 2:在上面的例子中我们使用了两组卷积和池化层。然而，这些操作可以在一个 ConvNet 中重复多次。实际上，现在有些表现最好的 ConvNet 拥有多达十几层的卷积和池化层！同时，每次卷积层后面不一定要有池化层。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>deep-learning</category>
      </categories>
      <tags>
        <tag>deep-learning</tag>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之评价指标]]></title>
    <url>%2Fartificial-intelligence%2Fmachine-learning%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%2F</url>
    <content type="text"><![CDATA[很多项目中都是用AUC来评价分类器的好坏，而不是使用精确率，召回率，F1值，请问这是什么原因呢？他们各自有什么优缺点和使用场景啊？ 精确度与召回率精确率是针对我们预测结果而言的，它表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP)，也就是P=TP/(TP+FP) 召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)。也就是P=TP/(TP+FN)。 其实就是分母不同，一个分母是预测为正的样本数，另一个是原来样本中所有的正样本数。 另外还有一个准确率，即预测对的/所有=（TP + TN）/（TP+FN+TN+FP） F1值 F值 = 正确率 召回率 2 / (正确率 + 召回率) （F 值即为正确率和召回率的调和平均值） ROCROC（receiver operating characteristic curve）是曲线。一般来说，如果ROC是光滑的，那么基本可以判断没有太大的overfitting（比如图中0.2到0.4可能就有问题，但是样本太少了），这个时候调模型可以只看AUC，面积越大一般认为模型越好。ROC 关注两个指标，true positive rate:TP/(TP+FN)和false positive rate:FP/(FP+TN)直观上，TPR 代表能将正例分对的概率，FPR 代表将负例错分为正例的概率。在 ROC 空间中，每个点的横坐标是 FPR，纵坐标是 TPR，这也就描绘了分类器在 TP（真正率）和 FP（假正率）间的 trade-off。 PRCPRC， precision recall curve。和ROC一样，先看平滑不平滑（蓝线明显好些），在看谁上谁下（同一测试集上），一般来说，上面的比下面的好（绿线比红线好）。F1（计算公式略）当P和R接近就也越大，一般会画连接(0,0)和(1,1)的线，线和PRC重合的地方的F1是这条线最大的F1（光滑的情况下），此时的F1对于PRC就好象AUC对于ROC一样。一个数字比一条线更方便调模型。 AUCAUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。AUC值越大的分类器，正确率越高.AUC=1AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。0.5&lt;AUC&lt;10.5&lt;AUC&lt;1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。AUC=0.5AUC=0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。AUC&lt;0.5AUC&lt;0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测，因此不存在 AUC&lt;0.5AUC&lt;0.5 的情况。 总结既然已经这么多评价标准，为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。而PR则会出现大变化。 ROC曲线和PR曲线的关系在ROC空间，ROC曲线越凸向左上方向效果越好。与ROC曲线左上凸不同的是，PR曲线是右上凸效果越好。 PR曲线会面临一个问题，当需要获得更高recall时，model需要输出更多的样本，precision可能会伴随出现下降/不变/升高，得到的曲线会出现浮动差异（出现锯齿），无法像ROC一样保证单调性。 PRC相对的优势当正负样本差距不大的情况下，ROC和PR的趋势是差不多的，但是当负样本很多的时候，两者就截然不同了，ROC效果依然看似很好，但是PR上反映效果一般。解释起来也简单，假设就1个正例，100个负例，那么基本上TPR可能一直维持在100左右，然后突然降到0.如图，(a)(b)分别为正负样本1:1时的ROC曲线和PR曲线，二者比较接近。而(c)(d)的正负样本比例为1:1，这时ROC曲线效果依然很好，但是PR曲线则表现的比较差。这就说明PR曲线在正负样本比例悬殊较大时更能反映分类的性能。在正负样本分布得极不均匀(highly skewed datasets)的情况下，PRC比ROC能更有效地反应分类器的好坏。 学术论文在假定正负样本均衡的时候多用ROC/AUC，实际工程更多存在数据标签倾斜问题一般使用F1。 参考文章： 机器学习性能评估指标 机器学习评价指标大汇总]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>AUC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural Networks Activation Functions]]></title>
    <url>%2Funcategorized%2FNeural-Networks-Activation-Functions%2F</url>
    <content type="text"><![CDATA[common 随笔 others 人工智能 artificial-intelligence 机器学习 machine-learning 深度学习 deep-learning 迁移学习 transfer-learning 模式识别 pattern-recognition 基础算法 algorithm 云计算 cloud-computing 分布式框架 architecture 分布式存储 storage 数据挖掘 data-mining 推荐系统 recommender-system 搜索引擎 search-engine 爬虫 广告 数据仓库 database technology 语言相关 programming 工具相关 tools]]></content>
  </entry>
  <entry>
    <title><![CDATA[hive 外表与分区]]></title>
    <url>%2Fcloud-computing%2Fstorage%2Fhive-external-table%2F</url>
    <content type="text"><![CDATA[在大数据分析中，我们经常会用到外部接入的数据源，因此使用外部表可直接将外部数据源挂载进我们的hive数据库中。 外部表的创建123456create external table if not exists test( username String, work string)PARTITIONED BY(year String, month String, day String)ROW FORMAT DELIMITED FIELDS TERMINATED BY ','LOCATION '/tmp/test/'; 关键字external告诉hive这个表是外部的。而后面的location…子句则用户告诉hive数据位于哪个路径下。如果创建的外部表不是分区表，则location…子句是必需的，而如果创建的外部表是分区表，则location…子句是可选的，可以在后面add分区的时候指定。 分区外部表是建议分区的，这样可以减小数据风险，并且可以缩短查询范围以及逻辑管理能力等。可以通过12alter table test add partition (year='2010', month='04', day='18')location '2010/04/18'; 子句添加分区。hive不关心一个分区对应的分区目录是否存在或者分区目录下是否有文件。如果分区下没有文件或者分区不存在，则针对该分区的查询语句返回的结果将是空，并不报错。 删除分区1ALTER TABLE test DROP IF EXISTS PARTITION (year='2010', month='04', day='18'); 修改分区12ALTER TABLE test PARTITION (dt='2008-08-08') SET LOCATION "new location";ALTER TABLE test PARTITION (dt='2008-08-08') RENAME TO PARTITION (dt='20080808'); 删除因为表是外部的，所以hive并非完全拥有这份数据，因此在删除的时候，删除表并不会删除这份数据，不过描述表的元数据信息会被删除掉。不过这里要注意，如果hive表当前操作用户对该表对应的目录没有写权限只有读权限，则创建表是没有问题的，但是创建完之后，要删除表就难了，会提示权限错误。]]></content>
      <categories>
        <category>cloud-computing</category>
        <category>storage</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式日志收集与数据流处理系统]]></title>
    <url>%2Fcloud-computing%2Farchitecture%2F%E5%88%86%E5%B8%83%E5%BC%8F%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B5%81%E5%A4%84%E7%90%86%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[ETL负责将分散的、异构数据源中的数据如关系数据、平面数据文件等抽取到临时中间层后，进行清洗、转换、集成，最后加载到数据仓库或数据集市中，成为联机分析处理、数据挖掘提供决策支持的数据。 数据源mysql日志 存储空间hdfshbaseredises 数据分发系统kafka：降低编程复杂度，各个子系统不在是相互协商接口，各个子系统类似插口插在插座上，Kafka承担高速数据总线的作用。 ETL工具flumecanalstormflinkspark-streaming]]></content>
      <categories>
        <category>cloud-computing</category>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>architecture</tag>
        <tag>ETL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习之卷积神经网络]]></title>
    <url>%2Fartificial-intelligence%2Fdeep-learning%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。 什么是卷积 什么是池化给一个最大池化层的例子：池化层的操作，是一种降采样操作。该操作是在一个小区域内，采取一个特定的值作为输出值。 在图像处理中，往往把图像表示为像素的向量，比如一个1000×1000的图像，可以表示为一个1000000的向量。在上一节中提到的神经网络中，如果隐含层数目与输入层一样，即也是1000000时，那么输入层到隐含层的参数数据为1000000×1000000=10^12，这样就太多了，基本没法训练。所以图像处理要想练成神经网络大法，必先减少参数加快速度。 卷积神经网络有两种神器可以降低参数数目，第一种神器叫做局部感知野。一般认为人对外界的认知是从局部到全局的，而图像的空间联系也是局部的像素联系较为紧密，而距离较远的像素相关性则较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。网络部分连通的思想，也是受启发于生物学里面的视觉系统结构。视觉皮层的神经元就是局部接受信息的（即这些神经元只响应某些特定区域的刺激）。 但其实这样的话参数仍然过多，那么就启动第二级神器，即权值共享。怎么理解权值共享呢？我们可以这100个参数（也就是卷积操作）看成是提取特征的方式，该方式与位置无关。这其中隐含的原理则是：图像的一部分的统计特性与其他部分是一样的。这也意味着我们在这一部分学习的特征也能用在另一部分上，所以对于这个图像上的所有位置，我们都能使用同样的学习特征。 一般地，CNN的基本结构包括两层，其一为特征提取层，每个神经元的输入与前一层的局部接受域相连，并提取该局部的特征。一旦该局部特征被提取后，它与其它特征间的位置关系也随之确定下来；其二是特征映射层，网络的每个计算层由多个特征映射组成，每个特征映射是一个平面，平面上所有神经元的权值相等。特征映射结构采用影响函数核小的sigmoid函数作为卷积网络的激活函数，使得特征映射具有位移不变性。此外，由于一个映射面上的神经元共享权值，因而减少了网络自由参数的个数。卷积神经网络中的每一个卷积层都紧跟着一个用来求局部平均与二次提取的计算层，这种特有的两次特征提取结构减小了特征分辨率。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>deep-learning</category>
      </categories>
      <tags>
        <tag>deep-learning</tag>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习之简要介绍]]></title>
    <url>%2Fartificial-intelligence%2Fdeep-learning%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[神经网络是深度学习的基础，感知器是最简单的神经网络结构，因此关于深度学习的篇章从感知器开始学起。 本文转载自机器之心，原文来自KDnugget，作者：Ujjwal Karn，链接：http://www.jiqizhixin.com/article/1886 神经元神经网络中计算的基本单元是神经元，一般称作「节点」（node）或者「单元」（unit）。节点从其他节点接收输入，或者从外部源接收输入，然后计算输出。每个输入都辅有「权重」（weight，即 w），权重取决于其他输入的相对重要性。节点将函数 f（定义如下）应用到加权后的输入总和，如图 1 所示：神经元可以看作一个计算与存储单元。计算是神经元对其的输入进行计算功能。存储是神经元会暂存计算结果，并传递到下一层。 单层神经网络(感知器)在“感知器”中，有两个层次。分别是输入层和输出层。输入层里的“输入单元”只负责传输数据，不做计算。输出层里的“输出单元”则需要对前面一层的输入进行计算。 我们把需要计算的层次称之为“计算层”，并把拥有一个计算层的网络称之为“单层神经网络”。 此网络接受 X1 和 X2 的数值输入，其权重分别为 w1 和 w2。另外，还有配有权重 b（称为「偏置（bias）」）的输入 1。 函数f是激活函数，在实践中常用的激活函数包括： 偏置bias的作用：then the output of the network becomes sig(w0x + w11.0),x 为输入，w0是输入的权重，1.0是偏置，w1是偏置的权重，则结果如下： 两层神经网络（多层感知器）多层感知器（Multi Layer Perceptron，即 MLP）包括至少一个隐藏层（除了一个输入层和一个输出层以外）。单层感知器只能学习线性函数，而多层感知器也可以学习非线性函数。 需要说明的是，在两层神经网络中，我们不再使用sgn函数作为函数g，而是使用平滑函数sigmoid作为函数g。我们把函数g也称作激活函数（active function）。 事实上，神经网络的本质就是通过参数与激活函数来拟合特征与目标之间的真实函数关系。初学者可能认为画神经网络的结构图是为了在程序中实现这些圆圈与线，但在一个神经网络的程序中，既没有“线”这个对象，也没有“单元”这个对象。实现一个神经网络最需要的是线性代数库。 多层神经网络（深度学习）在两层神经网络的输出层后面，继续添加层次。原来的输出层变成中间层，新加的层次成为新的输出层。所以可以得到下图。多层神经网络中，输出也是按照一层一层的方式来计算。从最外面的层开始，算出所有单元的值以后，再继续计算更深一层。只有当前层所有单元的值都计算完毕以后，才会算下一层。有点像计算向前不断推进的感觉。所以这个过程叫做“正向传播”。 增加更多的层次有什么好处？更深入的表示特征，以及更强的函数模拟能力。 总结 可以看出，随着层数增加，其非线性分界拟合能力不断增强。图中的分界线并不代表真实训练出的效果，更多的是示意效果。 神经网络的研究与应用之所以能够不断地火热发展下去，与其强大的函数拟合能力是分不开关系的。 前馈神经网络具体到前馈神经网络中，就有了本文中所分别描述的三个网络：单层神经网络，双层神经网络，以及多层神经网络。其结构都大致如下：一个前馈神经网络可以包含三种节点： 输入节点（Input Nodes）：输入节点从外部世界提供信息，总称为「输入层」。在输入节点中，不进行任何的计算——仅向隐藏节点传递信息。 隐藏节点（Hidden Nodes）：隐藏节点和外部世界没有直接联系（由此得名）。这些节点进行计算，并将信息从输入节点传递到输出节点。隐藏节点总称为「隐藏层」。尽管一个前馈神经网络只有一个输入层和一个输出层，但网络里可以没有也可以有多个隐藏层。 输出节点（Output Nodes）：输出节点总称为「输出层」，负责计算，并从网络向外部世界传递信息。 在前馈网络中，信息只单向移动——从输入层开始前向移动，然后通过隐藏层（如果有的话），再到输出层。在网络中没有循环或回路 [3]（前馈神经网络的这个属性和递归神经网络不同，后者的节点连接构成循环） 两个前馈神经网络的例子：单层感知器——这是最简单的前馈神经网络，不包含任何隐藏层。多层感知器——多层感知器有至少一个隐藏层。 反向传播(BP)最初，所有的边权重（edge weight）都是随机分配的。对于所有训练数据集中的输入，人工神经网络都被激活，并且观察其输出。这些输出会和我们已知的、期望的输出进行比较，误差会「传播」回上一层。该误差会被标注，权重也会被相应的「调整」。该流程重复，直到输出误差低于制定的标准。 为什么要反向传播神经网络模型的学习算法一般是SGD。SGD需要用到损失函数C关于各个权重参数的偏导数。一个模型的参数w,b是非常多的，故而需要反向传播算法快速计算。也就是说反向传播算法是一种计算偏导数的方法。有两种求导模式：前向传播和反向传播前向传播计算太复杂，因此通常采用后向传播 存在的问题 它是一个全连接的网络，因此在输入比较大的时候，权值会特别多 梯度发散]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>deep-learning</category>
      </categories>
      <tags>
        <tag>deep-learning</tag>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习数学基础]]></title>
    <url>%2Fartificial-intelligence%2Fmachine-learning%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[Linear Algebra (线性代数) 和 Statistics (统计学) 是最重要和不可缺少的。这代表了Machine Learning中最主流的两大类方法的基础。 Laplacian矩阵拉普拉斯矩阵（Laplacian matrix)），也称为基尔霍夫矩阵, 是表示图的一种矩阵。给定一个有n个顶点的图，其拉普拉斯矩阵被定义为:L = D - W,其中D为图的度矩阵，W为图的邻接矩阵。性质： L为对称半正定矩阵，保证所有特征值都大于等于0； L矩阵有最小的特征值0，其对应的特征向量为1。 特征值中0出现的次数就是图连通区域的个数。 最优化理论与KKT条件最优化理论是研究函数在给定一组约束条件下的最小值(或者最大值)的数学问题。在很多情况下, 不等式约束条件可以通过引入新的变量而转化为等式约束条件, 因此最优化问题的一般形式可以简化为仅仅包含等式约束条件的形式,也就是引入松弛变量。最优化问题可以根据目标函数和约束条件的类型进行分类:1). 如果目标函数和约束条件都为变量的线性函数, 称该最优化问题为线性规划;2). 如果目标函数为变量的二次函数, 约束条件为变量的线性函数, 称该最优化问题为二次规划;3). 如果目标函数或者约束条件为变量的非线性函数, 称该最优化问题为非线性规划. KKT条件是指在满足一些有规则的条件下, 一个非线性规划(Nonlinear Programming)问题能有最优化解法的一个必要和充分条件. KKT条件第一项是说最优点x∗必须满足所有等式及不等式限制条件, 也就是说最优点必须是一个可行解, 这一点自然是毋庸置疑的.第二项表明在最优点x∗, ∇f必须是∇gi和∇hj的线性組合, μi和λj都叫作拉格朗日乘子.所不同的是不等式限制条件有方向性, 所以每一个μi都必须大于或等于零, 而等式限制条件没有方向性，所以λj没有符号的限制, 其符号要视等式限制条件的写法而定.]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习维度规约]]></title>
    <url>%2Fartificial-intelligence%2Fmachine-learning%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BB%B4%E5%BA%A6%E8%A7%84%E7%BA%A6%2F</url>
    <content type="text"><![CDATA[任何分类或者回归方法的复杂度都依赖于样本的规模N和输入的维度d，而特征选择和特征提取是较少复杂性的有效手段。也就是所谓的维度规约 特征选择从d个维度中找出提供最多信息的k个维度 子集选择d个变量有2的d次方个可能的子集，在如此多可能情况下如何选择最佳子集呢？ 用F代表输入维的特征xi的集合，E（F）表示当只使用F中的输入时，在验证样本上出现的误差。 向前选择思想：从空集开始，逐渐添加变量，每次添加一个使得误差降低最多的变量，直到进一步的添加不会降低误差或降低很少算法：从F=空开始，在每一步，对于所有可能的输入xi，训练我们的模型并在验证集上计算E（F∪xi），然后我们选择导致最小误差的输入xi，并将xi加入到F。也可以设定一个阈值，提前结束算法。 向后选择思想：从所有变量集合中，逐个删除变量，每一步删除降低误差最多的变量，直到进一步的删除不会降低误差或者降低很少算法：同上相反 这两种情况下，误差检测都应该在不同于训练集的验证集上进行，因为我们需要检测泛化准确率，使用更多的特征一般会有更低的训练误差，但不一定有更低的验证误差。开销很大，且不能保证找到最佳子集（因为xi和xj本身可能效果不好，但是合起来可能误差降低很多），是个局部最优化算法。可以通过一次增加多个甚至还可以去掉之前增加的特征的方法提高找到最佳子集的可能性，但也增加可复杂度在无用特征大于有用特征时，选择向前搜索，反之选择向后搜索 人脸识别这样的应用中，特征选择不是降维的好方法，携带脸部识别信息的是许多像素值的组合，因此可以用特征提取方法来实现。 特征提取找出k个维的新集合，这些维是原来d个维的组合，最主要的方法包括主成分分析(PCA)和线性判别分析(LDA)，分别为非监督的和监督的。 主成分分析（非监督方法）Principal Component Analysis(PCA)是最常用的线性降维方法，它的目标是通过某种线性投影，将高维的数据映射到低维的空间中表示，并期望在所投影的维度上数据的方差最大，以此使用较少的数据维度，同时保留住较多的原数据点的特性。需要最大化的准则是方差主成分是这样的wi，样本投影到wi上之后最分散，使得样本点之间的差别变得最明显。为了得到唯一解且使该方向成为最重要因素，我们要求||wi||=1.也就是输入样本的协方差矩阵的最大特征值对应的特征向量对离群点很敏感适用于d&lt;N的情况主成分分析是通过变量变换把注意力集中在具有较大变差的那些主成分上，而舍弃那些变差小的主成分；用原始变量的线性组合表示新的综合变量，即主成分； PCA不能被用来作为防止过拟合的手段，要防止过拟合，最好还是采用规则化。因为PCA不考虑label信息。因此只有在内存不足、数据量太大导致硬盘不足等情况下才考虑PCA，否则，不建议滥用PCA。 特征嵌入适用于d&gt;N的情况 因子分析把注意力集中在少数不可观测的潜在变量（即公共因子）上，而舍弃特殊因子。因子分析就是认为高维样本点实际上是由低维样本点经过高斯分布、线性变换、误差扰动生成的。因子分析通过研究众多变量之间的内部依赖关系，探求观测数据的基本结构。并用少数几个假想变量来 表示其基本结构。这种假想变量能够反应原来众多变量的主要信息，属于不可观测的潜在变量，称为银子。潜在的假想变量和随机影响变量的线性组合表示原始变量。因子分析有两个核心问题：一是如何构造因子变量，二是如何对因子变量进行命名解释。因子分析有下面4个基本步骤： 确定原有若干变量是否适合于因子分析。因子分析的基本逻辑是从原始变量中构造出少数几个具有代表意义的因子变量，这就要求原有变量之间要具有比较强的相关性，否则，因子分析将无法提取变量间的“共性特征”（变量间没有共性还如何提取共性？）。实际应用时，可以使用相关性矩阵进行验证，如果相关系数小于0.3，那么变量间的共性较小，不适合使用因子分析。 构造因子变量。因子分析中有多种确定因子变量的方法，如基于主成分模型的主成分分析法和基于因子分析模型的主轴因子法、极大似然法、最小二乘法等。其中基于主成分模型的主成分分析法是使用最多的因子分析方法之一。 利用旋转使得因子变量更具有可解释性。在实际分析工作中，主要是因子分析得到因子和原变量的关系，从而对新的因子能够进行命名和解释，否则其不具有可解释性的前提下对比PCA就没有明显的可解释价值。 计算因子变量的得分。计算因子得分是因子分析的最后一步，因子变量确定以后，对每一样本数据，希望得到它们在不同因子上的具体数据值，这些数值就是因子得分，它和原变量的得分相对应。 线性判别分析(监督方法)线性判别分析(linear discriminant analysis, LDA)是一种用于分类问题的维度规约的监督方法。分类的目标是，使得类别内的点距离越近越好（集中），类别间的点越远越好。局限性： 当样本数量远小于样本的特征维数，样本与样本之间的距离变大使得距离度量失效，使LDA算法中的类内、类间离散度矩阵奇异，不能得到最优的投影方向，在人脸识别领域中表现得尤为突出 LDA不适合对非高斯分布的样本进行降维 LDA在样本分类信息依赖方差而不是均值时，效果不好 LDA可能过度拟合数据 PCA和LDA的区别上图左侧是PCA的降维思想，它所作的只是将整组数据整体映射到最方便表示这组数据的坐标轴上，映射时没有利用任何数据内部的分类信息。因此，虽然PCA后的数据在表示上更加方便（降低了维数并能最大限度的保持原有信息），但在分类上也许会变得更加困难；上图右侧是LDA的降维思想，可以看到LDA充分利用了数据的分类信息，将两组数据映射到了另外一个坐标轴上，使得数据更易区分了（在低维上就可以区分，减少了运算量）。 局部线性嵌入拉普拉斯特征映射]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>PCA</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统冷启动问题]]></title>
    <url>%2Fdata-mining%2Frecommender-system%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%86%B7%E5%90%AF%E5%8A%A8%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[协同过滤是一种常用的减少信息过载的技术，已成为个性化推荐系统的主要工具，但大多数协同过滤算法存在一个共性——新项目的冷启动问题。 冷启动问题（cold start)主要分三类： • 用户冷启动 • 物品冷启动 • 系统冷启动 针对用户冷启动：• 推热门• 利用用户的信息。（如：性别、年龄、地域等）• 利用登录帐号的社交网络信息• 要求新用户登录时做一些反馈 利用用户的注册信息+局部热门推荐： 先聚合，对所有用户分成不同的类 按照分成的类别创建分类树 对新用户通过分类树归到某一类 取当前类下用户的热门商品做推荐 对于每种特征f, 计算具有这种特征的用户对各个物品的喜欢程度p(f,i)p(f,i)可以简单定义为物品i在具有f的特征的用户中的热门程度：N(i)是喜欢物品i的用户集合，U(f)表示具有特征f的用户集合。可以看出具有比较高的N(i)就很可能有比较高的p(f,i), 所以推出的结果很可能就是热门结果。因此我们可以将p(f,i)定义为喜欢物品i的用户中具有特征f的比例：N(i)是喜欢物品i的用户集合，U(f)是具有特征f的用户集合，参数a的目的是解决数据系数的问题。 针对物品冷启动：user-cf对于user-cf 来说，针对推荐列表并不是给用户展示内容的唯一列表（大多网站都是这样的）的网站当新物品加入时，总会有用户通过某些途径看到，那么当一个用户对其产生反馈后，和他历史兴趣相似的用户的推荐列表中就有可能出现该物品，从而更多的人对该物品做出反馈，导致更多的人的推荐列表中出现该物品。因此，该物品就能不断扩散开来，从而逐步展示到对它感兴趣用户的推荐列表中针对推荐列表是用户获取信息的主要途径（例如豆瓣网络电台）的网站，userCF算法就需要解决第一推动力的问题，即第一个用户从哪儿发现新物品。最简单的方法是将新的物品随机战士给用户，但是太不个性化。因此可以考虑利用物品的内容信息，将新物品先投放给曾经喜欢过和它内容相似的其他物品的用户 item-cf对itemCF算法来说，物品冷启动就是很严重的问题了。因为该算法的基础是通过用户对物品产生的行为来计算物品之间的相似度，当新物品还未展示给用户时，用户就无法产生行为。为此，只能利用物品的内容信息计算物品的相关程度。基本思路就是将物品转换成关键词向量，通过计算向量之间的相似度（例如计算余弦相似度），得到物品的相关程度。]]></content>
      <categories>
        <category>data-mining</category>
        <category>recommender-system</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统]]></title>
    <url>%2Fdata-mining%2Frecommender-system%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[推荐系统、搜索引擎和社交媒体作为互联网信息的三大入口，推荐系统的核心是联系用户与信息。 概述随着互联网的发展，人们正处于一个信息爆炸的时代。搜索引擎的出现在一定程度上解决了信息筛选问题，但还远远不够。搜索引擎需要用户主动提供关键词来对海量信息进行筛选。当用户无法准确描述自己的需求时，搜索引擎的筛选效果将大打折扣，而用户将自己的需求和意图转化成关键词的过程本身就是一个并不轻松的过程。互联网企业都在追求用户体验，成为一个具有良好用户体验的平台，就应该将海量商品信息进行筛选过滤，大大降低用户海底捞针的困扰。推荐系统正是起到了这样的作用，一方面帮助用户发现对自己有价值的信息;另一方面讲信息更准确的展现到对它感兴趣的用户面前。从而实现用户与信息提供者两者的双赢。 架构推荐系统会根据平台性质、数据类型、用户特征（例如统计的、行为的等）、采用的推荐模型（实时推荐模型、离线推荐模型）等不同，表现为不同的体系架构，但是最基本最通用的架构仍然包含下面几个部分：不同的平台和需求可以结合自身情况灵活取用数据源、用户特征和推荐模型，然后通过线性加权、依场景切换等策略进行融合推荐结果，实现推荐系统的优化。 分类基于内容的推荐根据用户或者推荐物品的元数据，发现物品或者内容本身之间的相关性，或者发现用户之间的相关性。 1.基于人物画像根据系统用户的基本信息发现用户的相关程度，然后将相似用户喜爱的其他物品推荐给当前用户。系统首先会根据用户的属性建模，比如用户的年龄，性别，兴趣等信息。根据这些特征计算用户间的相似度。比如系统通过计算发现用户A和C比较相似。就会把A喜欢的物品推荐给C。优缺点： 不需要历史数据，没有冷启动问题。 不依赖于物品的属性，因此其他领域的问题都可无缝接入。 算法比较粗糙，效果很难令人满意，只适合简单的推荐。 2.基于商品属性系统首先对物品（图中举电影的例子）的属性进行建模，图中用类型作为属性。在实际应用中，只根据类型显然过于粗糙， 还需要考虑演员，导演等更多信息。通过相似度计算，发现电影A和C相似度较高，因为他们都属于爱情类。系统还会发现用户A喜欢电影A，由此得出结论，用户 A很可能对电影C也感兴趣。于是将电影C推荐给A。优缺点： 对用户兴趣可以很好的建模，并通过对物品属性维度的增加，获得更好的推荐精度 物品的属性有限，很难有效的得到更多数据 物品相似度的衡量标准只考虑到了物品本身，有一定的片面性 需要用户的物品的历史数据，有冷启动的问题 基于协同过滤的推荐根据用户对物品的偏好与行为，发现物品或者内容本身的相关性，或者用户之间的相关性 1.基于用户的协同过滤 找到与目标用户兴趣相似的用户群;假设用户u和v的正反馈的商品集合为N(u)，N(v)，那么两者兴趣相似度可以记为 找到这个集合中用户喜欢的，而目标用户没有听说过得商品推荐之;UserCF提供的一个参数K表示要考虑目标用户兴趣最相似的人的个数，在保证精度的同时，K不宜过大，否则推荐结果会趋向于热门商品，流行度指标和覆盖度指标都会降低。 基于用户的协同过滤推荐机制和基于人口统计学的推荐机制都是计算用户的相似度，并基于“邻居”用户群计算推荐，但它 们所不同的是如何计算用户的相似度，基 于人口统计学的机制只考虑用户本身的特征，而基于用户的协同过滤机制可是在用户的历史偏好的数据上计算用户的相似度，它的基本假设是，喜欢类似物品的用户 可能有相同或者相似的口味和偏好。 2.基于物品的协同过滤 计算商品之间的相似度。物品相似度可以表示为(其实跟前面的支持度比较像)第二个式子比第一个式子好在可以惩罚过热产品j。 根据商品的相似度和用户的历史行为，给用户生成推荐列表。 基于项目的协同过滤推荐和基于内容的推荐其实都是基于物品相似度预测推荐，只是相似度计算的方法不一样，前者是从用户历史的偏好推断，而后者是基于物品本身的属性特征信息。 3.协同过滤的优缺点： 它不需要对物品或者用户进行严格的建模，而且不要求物品的描述是机器可理解的，所以这种方法也是领域无关的。 这种方法计算出来的推荐是开放的，可以共用他人的经验，很好的支持用户发现潜在的兴趣偏好 方法的核心是基于历史数据，所以对新物品和新用户都有“冷启动”的问题 推荐的效果依赖于用户历史偏好数据的多少和准确性 在大部分的实现中，用户历史偏好是用稀疏矩阵进行存储的，而稀疏矩阵上的计算有些明显的问题，包括可能少部分人的错误偏好会对推荐的准确度有很大的影响等等 对于一些特殊品味的用户不能给予很好的推荐 由于以历史数据为基础，抓取和建模用户的偏好后，很难修改或者根据用户的使用演变，从而导致这个方法不够灵活 4.UserCF和ItemCF之间的比较在现实的情况中，往往物品的个数是远远小于用户的数量的，而且物品的个数和相似度相对比较稳定，可以离线完成工作量最大的相似性计算步骤，从而大大降低了在线计算量，基于用户的实时性更好一些。 但是具体使用的场景，还需要根据具体的业务类型来区分，User-CF偏重于反应用户小群体热点，更具社会 化，而Item-CF在于维持用户的历史兴趣，比如：对于新闻、阅读类的推荐，新闻阅读类的信息是实时更新的，所以ItemCF在这种情况下需要不断更新，而用户对新闻的个性化推荐不是特别的强烈情况，用户有新行为不会导致相似用户的剧烈运动。 对于电子商务类别的，由于用户消费代价比较高，所以对个性化的精确程度要求也比较高，而一段用户有新的行为，也会导致推荐内容的实时变化 协同过滤的算法缺点也很明显，除了上面的冷启动之外，往往商家的用户数量和产品数量都很多，所以矩阵的计算量会非常的大，但某个具体的用户往往买的东西又有限，所以数据同时也是高度稀疏的。 基于用户标签数据一种是根据用户打标签的行为为其推荐物品，还有一种是在用户给物品打标签的时候为其推荐合适的标签。 根据标签推荐物品的基本思想就是找到用户常用的一些标签，然后找到具有这些标签的热门物品，将其推荐给用户。这里要注意两个问题，一个是要保证新颖性和多样性，可以用TF-IDF方法来降低热门物品的权重；另一个则是需要清除某些同义重复标签和没有意义的标签。 基于标签的推荐有很多优点，一方面可以给用户提供比较准确的推荐理由；另一方面标签云的形式也提高了推荐的多样性，给了用户一定的自主选择。标签其实可以看做一种物品的内容数据，比如书的作者，出版社，类型；音乐的国别，风格，作者等等，基于这些信息的推荐可以弥补上述基于用户行为推荐的一些弱点。 基于上下文信息所谓的上下文，是指用户所处的时间，地点，心情等。这些因素对于推荐也是至关重要的，比如听歌的心情，商品的季节性等等。另外用户的社交关系对与用户的喜好也起到非常重要的作用。 基于社交网络基于社交网络兴趣图谱和社会图谱的精准广告投放也是推荐系统的关键应用，它决定着社交网站的变现能力。利用人与人之间的信任，广泛结合人口统计学、好友关系、社交图谱、兴趣图谱等信息，建立人与人之间的强关联。 混合推荐算法 加权混合：用线性公式将几种不同的推荐结果按照一定权重组合起来，具体权重需要在测试训练集上反复实验，从而达到最好效果 切换策略：根据不同的场景（数据量、用户与物品的数量等），选择最为适合的推荐机制，灵活切换。 分区混合：采用多种推荐策略，并将不同的推荐结果分不同的区展示给用户，比如用户购买某商品之后给展示购买过该商品的其他用户也购买过的其他商品。 分层混合：采用多种推荐策略，讲一种推荐结果作为下一种推荐结果的输入，从而综合各推荐机制的优缺点，得到更为准确的推荐。 问题时效性用户反馈显式用户反馈：用户在平台上自然浏览或者产生行为之外，显示的提供反馈信息，比如用户对物品的评分、顶(赞)等行为数据隐式用户反馈：用户在使用过程中产生的数据，隐式反应了用户对物品的喜好，例如浏览了但没购买、查看了物品的详细信息等 冷启动1.如何给新用户做个性化推荐对于新用户，首先可以根据其注册信息进行粗粒度的推荐，如年龄，性别，爱好等。另外也可以在新用户注册后为其提供一些内容，让他们反馈对这些内容的兴趣，再根据这些数据来进行推荐。这些内容需要同时满足热门和多样的要求。 利用用户的社交网络账号登录（需要用户授权），导入用户在社交网站上的好友信息，然后给用户推荐其好友喜欢的物品。 2.如何将新物品推荐给用户而对于新物品的推荐，可能就要从其内容数据上下功夫了。我们可以通过语义分析对物品抽取关键词并赋予权重，这种内容特征类似一个向量，通过向量之间的余弦相似度便可得出物品之间的相似度，从而进行推荐。这种内容过滤算法在物品（内容）更新较快的服务中得到大量应用，如新闻资讯类的个性化推荐。 3.新网站在数据稀少的情况下如何做个性化推荐通过人工的力量来建立早期的推荐系统了。简单一点的，人工编辑热门榜单，高级一点的，人工分类标注。国外的个性化音乐电台Pandora就雇了一批懂计算机的音乐人来给大量音乐进行多维度标注，称之为音乐基因。有了这些初始数据，就可以方便地进行推荐了。国内的Jing.fm初期也是通过对音乐的物理信息，情感信息，社会信息进行人工分类，而后再通过机器学习和推荐算法不断完善，打造出了不一样的个性化电台。 评判标注用户满意度 描述用户对推荐结果的满意程度，这是推荐系统最重要的指标。一般通过对用户进行问卷或者监测用户线上行为数据获得。预测准确度 描述推荐系统预测用户行为的能力。一般通过离线数据集上算法给出的推荐列表和用户行为的重合率来计算。重合率越大则准确率越高。覆盖率 描述推荐系统对物品长尾的发掘能力。一般通过所有推荐物品占总物品的比例和所有物品被推荐的概率分布来计算。比例越大，概率分布越均匀则覆盖率越大。因为推荐系统中马太效应频繁，所以好的推荐 算法应当是所有商品被推荐的几率差不多，都可以找到各自合适的用户，所以实际中会考虑信息熵、基尼系数等指标。多样性 描述推荐系统中推荐结果能否覆盖用户不同的兴趣领域。一般通过推荐列表中物品两两之间不相似性来计算，物品之间越不相似则多样性越好。新颖性 如果用户没有听说过推荐列表中的大部分物品，则说明该推荐系统的新颖性较好。可以通过推荐结果的平均流行度和对用户进行问卷来获得。惊喜度 如果推荐结果和用户的历史兴趣不相似，但让用户很满意，则可以说这是一个让用户惊喜的推荐。可以定性地通过推荐结果与用户历史兴趣的相似度和用户满意度来衡量。信仸度 用户是否信仸推荐系统的结果。增加透明度，利用社交网络好友进行解释。健壮性 反攻击，抗作弊的能力]]></content>
      <categories>
        <category>data-mining</category>
        <category>recommender-system</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git几种不同场景下的撤销方法总结]]></title>
    <url>%2Ftechnology%2Ftools%2Fgit%E5%87%A0%E7%A7%8D%E4%B8%8D%E5%90%8C%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%9A%84%E6%92%A4%E9%94%80%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本文主要介绍作为普通开发人员，在不需要特别深入了解git内部原理的情况下，如何能够对常用的基本操作灵活运用，操作熟练且不坑队友。 当然还是得简单了解下基本原理。 Git架构 目录结构 四个阶段 工作目录 index(又称为暂存区) 本地仓库 远程仓库。 存储原理Git 是简单的 key-value 数据存储。它允许插入任意类型的内容，并会返回一个键值，通过该键值可以在任何时候再取出该内容。可以通过底层命令hash-object 来示范这点，传一些数据给该命令，它会将数据保存在 .git 目录并返回表示这些数据的键值。如:12$ echo 'test content' | git hash-object w -stdind670460b4b4aece5915caf5c68d12f560a9fe3e4 参数 w 指示 hash-object 命令存储 (数据) 对象，若不指定这个参数该命令仅仅返回键值。-stdin 指定从标准输入设备 (stdin) 来读取内容，若不指定这个参数则需指定一个要存储的文件的路径。该命令输出长度为 40 个字符的校验和。这是个 SHA-1 哈希值──其值为要存储的数据加上你马上会了解到的一种头信息的校验和。现在可以查看到 Git 已经存储了数据： 12$ find .git/objects -type f.git/objects/d6/70460b4b4aece5915caf5c68d12f560a9fe3e4 可以在 objects 目录下看到一个文件。这便是 Git 存储数据内容的方式──为每份内容生成一个文件，取得该内容与头信息的 SHA-1 校验和，创建以该校验和前两个字符为名称的子目录，并以 (校验和) 剩下 38 个字符为文件命名 (保存至子目录下)。通过 cat-file 命令可以将数据内容取回。该命令是查看 Git 对象的瑞士军刀。传入 -p 参数可以让该命令输出数据内容的类型：12$ git cat-file -p d670460b4b4aece5915caf5c68d12f560a9fe3e4test content 基本命令git init or git clone略 git fetch命令基本命令git fetch &lt;远程主机名&gt; &lt;分支名&gt; Git fetch:拉取而不合并这一步其实是执行了两个关键操作: 创建并更新所有远程分支的本地远程分支. 设定当前分支的FETCH_HEAD为远程服务器的master分支 (上面说的第一种情况) 如果没有显式的指定远程分支, 则远程分支的master将作为默认的FETCH_HEAD.如果指定了远程分支, 就将这个远程分支作为FETCH_HEAD.需要注意的是: 和push不同, fetch会自动获取远程`新加入’的分支. 示例一：git fetch origin branch1设定当前分支的 FETCH_HEAD’ 为远程服务器的branch1分支`.注意: 在这种情况下, 不会在本地创建本地远程分支, 这是因为:这个操作是git pull origin branch1的第一步, 而对应的pull操作,并不会在本地创建新的branch. 示例二：git fetch origin branch1:branch2只要明白了上面的含义, 这个就很简单了,首先执行上面的fetch操作使用远程branch1分支在本地创建branch2(但不会切换到该分支),如果本地不存在branch2分支, 则会自动创建一个新的branch2分支,如果本地存在branch2分支, 并且是`fast forward’, 则自动合并两个分支, 否则, 会阻止以上操作.git fetch origin :branch2等价于: git fetch origin master:branch2 git 关联远程分支基本命令1234git checkout -b [local-branch-name] [remote-name]/[remote-branch-name]git checkout --track [remote-name]/[remote-branch-name]git checkout -b [local-branch-name] --track [remote-name]/[remote-branch-name]git branch --set-upstream master origin/next git pull基本命令git pull &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt; 如果合并需要采用rebase模式，可以使用–rebase选项。git pull --rebase &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt; 如果远程主机删除了某个分支，默认情况下，git pull 不会在拉取远程分支的时候，删除对应的本地分支。这是为了防止，由于其他人操作了远程主机，导致git pull不知不觉删除了本地分支。但是，你可以改变这个行为，加上参数 -p 就会在本地删除远程已经删除的分支。1234$ git pull -p# 等同于下面的命令$ git fetch --prune origin$ git fetch -p git push基本命令git push &lt;远程主机名&gt; &lt;本地分支名&gt;:&lt;远程分支名&gt; 如果省略远程分支名，则表示将本地分支推送与之存在”追踪关系”的远程分支（通常两者同名），如果该远程分支不存在，则会被新建。$ git push origin master上面命令表示，将本地的master分支推送到origin主机的master分支。如果后者不存在，则会被新建。 如果省略本地分支名，则表示删除指定的远程分支，因为这等同于推送一个空的本地分支到远程分支。123$ git push origin :master# 等同于$ git push origin --delete master 上面命令表示删除origin主机的master分支。 如果远程主机的版本比本地版本更新，推送时Git会报错，要求先在本地做git pull合并差异，然后再推送到远程主机。这时，如果你一定要推送，可以使用–force选项。$ git push --force origin上面命令使用–force选项，结果导致远程主机上更新的版本被覆盖。除非你很确定要这样做，否则应该尽量避免使用–force选项。 最后，git push不会推送标签（tag），除非使用–tags选项。$ git push origin --tags Git的几种撤销方法没add：只撤销某个文件的修改123$ git checkout filename或者$ git checkout -- filename 原理：git checkout将工作目录（working directory）里的文件修改成先前Git已知的状态。你可以提供一个期待回退分支的名字或者一个确切的SHA码，Git也会默认检出HEAD——即：当前分支的上一次提交。 注意：用这种方法“撤销”的修改都将真正的消失。它们永远不会被提交。因此Git不能恢复它们。此时，一定要明确自己在做什么！（或许可以用git diff来确定） add之后但没commit之前：撤回本次addgit reset HEAD filename Commit之后：情况一：只是在最后的提交信息中敲错了字或者忘了有个文件没加进去123git commit -amend或git commit -amend -m "Fixes bug #42" 原理：git commit –amend将使用一个包含了刚刚错误提交所有变更的新提交，来更新并替换这个错误提交。由于没有staged的提交，所以实际上这个提交只是重写了先前的提交信息。 例子：123$ git commit -m 'initial commit'$ git add forgotten_file$ git commit --amend 结果：最终只有一次提交，第二次提交修改了第一次提交 情况二、你已经在本地做了一些提交（还没push），但所有的东西都糟糕透了，你想撤销最近的三次提交123git reset或git reset --hard 原理：git reset将你的仓库纪录一直回退到指定的最后一个SHA代表的提交，那些提交就像从未发生过一样。 注意：默认情况下，git reset会保留工作目录（working directory）。这些提交虽然消失了，但是内容还在磁盘上。这是最安全的做法，但通常情况是：你想使用一个命令来“撤销”所有提交和本地修改—–那么请使用-hard参数吧。 Push之后：git revert 原理：git revert将根据给定SHA的相反值，创建一个新的提交。如果旧提交是“matter”，那么新的提交就是“anti-matter”—–旧提交中所有已移除的东西将会被添加进到新提交中，旧提交中增加的东西将在新提交中移除。这是Git最安全、也是最简单的“撤销”场景，因为这样不会修改历史记录—–你现在可以git push下刚刚revert之后的提交来纠正错误了。 几个特殊场景：场景一：你已经提交了一些内容，并使用git reset –hard撤销了这些更改（见上面），突然意识到：你想还原这些修改！ 使用撤销命令：git reflog和git reset, 或者git checkout 发生了什么：git reflog是一个用来恢复项目历史记录的好办法。你可以通过git reflog恢复几乎任何已提交的内容。 场景二：你提交了一些变更，然后你意识到你正在master分支上，但你期望的是在feature分支上执行这些提交。 使用撤销命令：git branch feature, git reset –hard origin/master, 和 git checkout feature 发生了什么：你可能用的是git checkout b来建立新的分支，这是创建和检出分支的便捷方法—–但实际你并不想立刻切换分支。git branch feature会建立一个叫feature的分支，这个分支指向你最近的提交，但是你还停留在master分支上。git reset –hard将master回退至origin/master，并忽略所有新提交。别担心，那些提交都还保留在feature上。最后，git checkout将分支切换到feature，这个分支原封不动的保留了你最近的所有工作。 git diff 合并merge rebase cherry-pick 总结]]></content>
      <categories>
        <category>technology</category>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习聚类算法总结]]></title>
    <url>%2Fartificial-intelligence%2Fmachine-learning%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[聚类算法是一种无监督学习，无训练样本，根据信息相似度原则进行聚类，通过聚类，人们能够识别密集的和稀疏的区域，因而发现全局的分布模式，以及数据属性之间的关系 聚类Kmeans原理：使各个样本与所在簇的质心的均值的误差平方和达到最小步骤： 随机在图中取K（这里K=2）个种子点。 然后对图中的所有点求到这K个种子点的距离，假如点Pi离种子点Si最近，那么Pi属于Si点群 接下来，我们要移动种子点到属于他的“点群”的中心。 然后重复第2）和第3）步，直到，种子点没有移动。 优点： 原理简单 容易解释 缺点： k值需要事先指定，但k值往往是难以估计的 最终状态跟初始值有关（用kmeans++算法解决） 对异常值比较敏感 收敛太慢 结果为局部最优 比层次聚类法运算量小, 适用于小到中大规模样本数据 适用于发现球状类 针对数据分布呈非圆形的情况下不适应 没有考虑数据的先验分布 通常采用欧式距离计算相似性，有很大局限性 层次聚类(Hierarchical Clustering)通过计算不同类别数据点间的相似度来创建一棵有层次的嵌套聚类树。在聚类树中，不同类别的原始数据点是树的最低层，树的顶层是一个聚类的根节点。 创建聚类树有自下而上合并和自上而下分裂两种方法其中合并法：通过计算两类数据点间的相似性，对所有数据点中最为相似的两个数据点进行组合，并反复迭代这一过程。简单的说层次聚类的合并算法是通过计算每一个类别的数据点与所有数据点之间的距离来确定它们之间的相似性，距离越小，相似度越高。并将距离最近的两个数据点或类别进行组合，生成聚类树。 距离计算：采用欧几里得距离 先计算数据点之间的距离 再计算数据点与组合点之间的距离 最后计算组合点与组合点之间的距离 层次聚类树状图 基于密度的聚类谱聚类(Spectral Clustering, SC)描述：一种基于图论的聚类方法——将带权无向图划分为两个或两个以上的最优子图，使子图内部尽量相似，而子图间距离尽量距离较远，以达到常见的聚类的目的。其中的最优是指最优目标函数不同，可以是割边最小分割——如图1的Smallest cut(如后文的Min cut)， 也可以是分割规模差不多且割边最小的分割——如图1的Best cut(如后文的Normalized cut)。这样，谱聚类能够识别任意形状的样本空间且收敛于全局最优解，其基本思想是利用样本数据的相似矩阵(拉普拉斯矩阵)进行特征分解后得到的特征向量进行聚类。 准则：紧密性，连通性。思想：通过讲数据映射到一个具有约化的维度的新空间，可使得相似性显而易见。谱聚类正是通过拉普拉斯特征映射，实现了这一目标。也就是，利用样本数据之间的相似矩阵（拉普拉斯矩阵）进行特征分解（ 通过Laplacian Eigenmap 的降维方式降维），然后将得到的特征向量进行 K-means聚类。 如何切割图则成为问题的关键。目标函数如下 最小切将图分成两个部分A,B,从而使连接A与B的边权值加和最小。 比例割(Ratio cut) 规范割(Normalized cut) 这是谱聚类的目标，式子虽然简单，但是要最小化它却是一个 NP 难问题，不方便求解，为了找到解决办法，让我们先来做做变形。令 V 表示 Graph 的所有节点的集合，首先定义一个 N 维向量 f：的矩阵 L=D-W ,这个 L 有一个性质就是：这个是对任意向量 f 都成立的，很好证明，只要按照定义展开就可以得到了。把我们刚才定义的那个 f 带进去，就可以得到因此最小化 RatioCut 就等价于最小化 f’Lf 。 步骤核心步骤：一、在原始空间中定义局部邻域。然后对相同领域内的实例，定义与实例之间的距离成反比的相似度度量，在这种拉普拉斯映射下，把实例安置在新空间二、在新空间运行k均值聚类 其中拉普拉斯矩阵定义为：L = D - W,其中D为图的度矩阵，W为图的邻接矩阵。 算法流程 根据数据构造一个 Graph ，Graph 的每一个节点对应一个数据点，将相似的点连接起来，并且边的权重用于表示数据之间的相似度。把这个 Graph 用邻接矩阵的形式表示出来，记为 W 。一个最偷懒的办法就是：直接用我们前面在 K-medoids 中用的相似度矩阵作为 W 。 把 W 的每一列元素加起来得到 N 个数，把它们放在对角线上（其他地方都是零），组成一个 N × N 的矩阵，记为 D 。并令 L = D-W 。 求出 L 的前 k 个特征值（在本文中，除非特殊说明，否则“前 k 个”指按照特征值的大小从小到大的顺序）以及对应的特征向量 。 把这 k 个特征（列）向量排列在一起组成一个 N × k 的矩阵，将其中每一行看作 k 维空间中的一个向量，并使用 K-means 算法进行聚类。聚类的结果中每一行所属的类别就是原来 Graph 中的节点亦即最初的 N 个数据点分别所属的类别 特性： 谱聚类和传统的聚类方法（例如 K-means）相比，谱聚类只需要数据之间的相似度矩阵就可以了，而不必像K-means那样要求数据必须是 N 维欧氏空间中的向量。 对于不规则的误差数据不是那么敏感，而且 performance 也要好一些。 计算复杂度比 K-means 要小，特别是在像文本数据或者平凡的图像数据这样维度非常高的数据上运行的时候。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>Clustering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习学习策略总结]]></title>
    <url>%2Fartificial-intelligence%2Fmachine-learning%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[未完待续。。。。。。 学习损失函数0-1损失函数L(Y,f(X)) = {1(Y=f(X)),0(Y!=f(X))} 平方损失函数L(Y,f(X)) = (Y-f(X))** 绝对损失函数L(Y,f(X)) = |(Y-f(X))| 合页损失函数逻辑斯蒂损失函数指数损失函数对数似然损失函数L(Y,f(X)) = -logP(Y|X) 学习策略极大似然估计正则化的极大似然估计估计最大后验概率估计极小化正则化合页损失软间隔最大化极小化加法模型的指数损失学习算法梯度下降算法迭代尺度算法###拟牛顿算法 序列最小最优化算法前向分步算法EM算法]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>strategy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习回归算法总结]]></title>
    <url>%2Fartificial-intelligence%2Fmachine-learning%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[主要通过模型、策略、算法、特点、优缺点等角度对回归算法进行学习总结 回归 回归其实就是对已知公式的未知参数进行估计。比如已知公式是y = a*x + b，未知参数是a和b。我们现在有很多真实的(x,y)数据（训练样本），回归就是利用这些数据对a和b的取值去自动估计。 回归的前提是公式已知，否则回归无法进行。 根据这些公式的不同，回归分为线性回归和非线性回归。线性回归中公式都是“一次”的（一元一次方程，二元一次方程…），而非线性则可以有各种形式（N元N次方程，log方程 等等）。 Kmeans/knn knn也可用于回归，在找到最近的k个实例之后，可以计算这k个实例的平均值作为预测值。或者还可以给这k个实例添加一个权重再求平均值，这个权重与度量距离成反比（越近权重越大）。 决策树 CART － Classification and Regression Trees 分类与回归树，是二叉树，可以用于分类，也可以用于回归问题，最先由 Breiman 等提出。 分类树的输出是样本的类别， 回归树的输出是一个实数。 线性回归逻辑回归 很多情况下，我们需要回归产生一个类似概率值的0~1之间的数值，于是引入了Logistic方程，来做归一化。 所以，Logistic Regression 就是一个被logistic方程归一化后的线性回归，仅此而已。 可用于概率预测，也可用于分类。 仅能用于线性问题 分类时计算量非常小，速度很快，存储资源低； 各feature之间不需要满足条件独立假设，但各个feature的贡献是独立计算的。逻辑回归不像朴素贝叶斯一样需要满足条件独立假设（因为它没有求后验概率）。但每个feature的贡献是独立计算的，即LR是不会自动帮你combine 不同的features产生新feature的 只能处理二分类问题 梯度下降会陷入局部最优，并且每次在对当前样本计算cost的时候都需要去遍历全部样本才能得到cost值，这样计算速度就会慢很多（虽然在计算的时候可以转为矩阵乘法去更新整个w值）所以现在好多框架（mahout）中一般使用随机梯度下降法，它在计算cost的时候只计算当前的代价，最终cost是在全部样本迭代一遍之求和得出，还有他在更新当前的参数w的时候并不是依次遍历样本，而是从所有的样本中随机选择一条进行计算，它方法收敛速度快（一般是使用最大迭代次数），并且还可以避免局部最优，并且还很容易并行（使用参数服务器的方式进行并行） 局部加权线性回归岭回归前向逐步回归]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习分类算法总结]]></title>
    <url>%2Fartificial-intelligence%2Fmachine-learning%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[主要通过模型、策略、算法、特点、优缺点等角度对分类算法进行学习总结 分类knn（多类分类，判别模型）KNN(K-Nearest Neighbor)描述：给一个训练数据集和一个新的实例，在训练数据集中找出与这个新实例最近的k个训练实例，然后统计最近的k个训练实例中所属类别计数最多的那个类，就是新实例的类特点：特种空间、样本点 三要素： k值的选择k值越小表明模型越复杂，更加容易过拟合但是k值越大，模型越简单，如果k=N的时候就表明无论什么点都是训练集中类别最多的那个类 距离的度量（常见的距离度量有欧式距离，马氏距离等） 分类决策规则 （多数表决规则） 优缺点：优点：思想简单，理论成熟，既可以用来做分类也可以用来做回归；可用于非线性分类；训练时间复杂度为O(n)；准确度高，对数据没有假设，对outlier不敏感；缺点：计算量大；样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；需要大量的内存； 朴素贝叶斯（多类分类，生成模型）描述：P(A|B)=P(A∩B)P(B) 对于给出的待分类项，求解在此项出现的条件下各个目标类别出现的概率，哪个最大，就认为此待分类项属于哪个类别特点：独立假设学习策略：极大似然估计、极大后验概率估计损失函数：对数似然函数学习方法：概率计算公式、EM算法优点：特征为离散值时直接统计即可（表示统计概率）对小规模的数据表现很好，适合多分类任务，适合增量式训练。缺点：对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）。 贝叶斯网络决策树（多类分类，判别模型）描述：根据信息熵的大小选择一个属性进行分枝特点：分类树、回归树学习策略：正则化的极大似然估计损失函数：对数似然损失学习算法：特征选择、生成、剪枝 id3信息增益 C4.5信息增益比 CART基尼指数 连续与缺失值多变量决策树容易过拟合（随机森林可以减小过拟合）解决：在目标函数中加入正则化项，增大数据量，减少模型复杂程度，决策树剪枝，神经网络Dropout策略，观察训练和交叉验证的loss图像 优点计算量简单，可解释性强，比较适合处理有缺失属性值的样本，能够处理不相关的特征； 缺点容易过拟合（后续出现了随机森林，减小了过拟合现象）,使用剪枝来避免过拟合； 逻辑斯蒂回归（多类分类，判别模型）学习策略：极大似然估计、正则化的极大似然估计损失函数：逻辑斯蒂损失学习算法：迭代尺度算法、梯度下降、拟牛顿算法 SVM（二类分类，判别模型）描述：svm的基本想法就是求解能正确划分训练样本并且其几何间隔最大化的超平面。特点：分离超平面，核技巧学习策略：极小化正则化合页损失、软间隔最大化损失函数：合页损失学习算法：序列最小最优化算法（SMO） 为什么要引入对偶算法对偶问题往往更加容易求解(结合拉格朗日和kkt条件)可以很自然的引用核函数（拉格朗日表达式里面有内积，而核函数也是通过内积进行映射的） 核函数将输入特征x（线性不可分）映射到高维特征R空间，可以在R空间上让SVM进行线性可以变，这就是核函数的作用多项式核函数:K(x,z)=(x∗z+1)pK(x,z)=(x∗z+1)p高斯核函数:K(x,z)=exp(−(x−z)2σ2)K(x,z)=exp(−(x−z)2σ2)字符串核函数：貌似用于字符串处理等 SVM优缺点优点：使用核函数可以向高维空间进行映射使用核函数可以解决非线性的分类分类思想很简单，就是将样本与决策面的间隔最大化分类效果较好缺点：对大规模数据训练比较困难无法直接支持多分类，但是可以使用间接的方法来做 对于多分类 直接法直接在目标函数上进行修改，将多个分类面的参数求解合并到一个最优化问题中，通过求解该优化就可以实现多分类（计算复杂度很高，实现起来较为困难） 间接法一对多其中某个类为一类，其余n-1个类为另一个类，比如A,B,C,D四个类，第一次A为一个类，{B,C,D}为一个类训练一个分类器，第二次B为一个类,{A,C,D}为另一个类,按这方式共需要训练4个分类器，最后在测试的时候将测试样本经过这4个分类器f1(x)f1(x),f2(x)f2(x),f3(x)f3(x)和f4(x)f4(x),取其最大值为分类器(这种方式由于是1对M分类，会存在偏置，很不实用)一对一(libsvm实现的方式)任意两个类都训练一个分类器，那么n个类就需要n(n-1)/2个svm分类器。还是以A,B,C,D为例,那么需要{A,B},{A,C},{A,D},{B,C},{B,D},{C,D}为目标共6个分类器，然后在预测的将测试样本通过这6个分类器之后进行投票选择最终结果。（这种方法虽好，但是需要n(n-1)/2个分类器代价太大，不过有好像使用循环图来进行改进） 多分类不平衡问题支持向量回归SVM、LR、决策树的对比？提升方法（二类分类，判别模型）特点：弱分类器的线性组合学习策略：极小化加法模型的指数损失损失函数：指数损失学习算法：前向分布加法算法 Bootstrap(自助法)一种抽样方法 Bagging（代表为随机森林）描述：bagging的思想是从训练集中进行子抽样，对抽取的各子训练集建立多个基模型，对所有基模型预测的结果进行综合产生最终的预测结果随机森林的随机包含两部分内容： 训练样本选择方面的Random：Bootstrap方法随机选择子样本 特征选择方面的Random：属性集中随机选择k个属性，每个树节点分裂时，从这随机的k个属性，选择最优的(如何选择最优又有各种最大增益的方法，不在本文讨论范围内)。 从样本集中用Bootstrap采样选出n个训练样本(放回，因为别的分类器抽训练样本的时候也要用)在所有属性上，用这n个样本训练分类器（CART or SVM or …）重复以上两步m次，就可以得到m个分类器（CART or SVM or …）将数据放在这m个分类器上跑，最后投票机制(多数服从少数)看到底分到哪一类(分类问题) 优点：1.不容易出现过拟合，因为选择训练样本的时候就不是全部样本。2.可以既可以处理属性为离散值的量，比如ID3算法来构造树，也可以处理属性为连续值的量，比如C4.5算法来构造树。3.对于高维数据集的处理能力令人兴奋，它可以处理成千上万的输入变量，并确定最重要的变量，因此被认为是一个不错的降维方法。此外，该模型能够输出变量的重要性程度，这是一个非常便利的功能。4.分类不平衡的情况时，随机森林能够提供平衡数据集误差的有效方法5.两个随机性的引入，使得随机森林具有很好的抗噪声能力6.训练速度快，可以得到变量重要性排序。7.在训练过程中，能够检测到feature间的互相影响。缺点1.随机森林在解决回归问题时并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续型的输出。当进行回归时，随机森林不能够作出超越训练集数据范围的预测，这可能导致在对某些还有特定噪声的数据进行建模时出现过度拟合。2.对于许多统计建模者来说，随机森林给人的感觉像是一个黑盒子——你几乎无法控制模型内部的运行，只能在不同的参数和随机种子之间进行尝试。 Boosting（代表为GBDT）GBDT，全称(Gradient Boosting Decision Tree)描述：一种迭代算法，针对同一个训练集训练不同的分类器(弱分类器)，然后进行分类，对于分类正确的样本权值低，分类错误的样本权值高（通常是边界附近的样本），最后的分类器是很多弱分类器的线性叠加（加权组合），分类器相当简单。实际上就是一个简单的弱分类算法提升(boost)的过程。 优点1.可以使用各种方法构造子分类器，Adaboost算法提供的是框架2.简单，不用做特征筛选3.相比较于RF，更不用担心过拟合问题缺点1.对于噪音数据和异常数据是十分敏感的，因此在每次迭代时候会给噪声点较大的权重。2.运行速度慢，凡是涉及迭代的基本上都无法采用并行计算，Adaboost是一种”串行”算法.所以GBDT(Gradient Boosting Decision Tree)也非常慢。 多样性GBDT 和 RF 的区别？1.RF并行 GBDT串行2.GBDT的基模型为弱模型，而RF中的基树是强模型3.GBDT重采样的不是样本，而是样本的分布，每次迭代之后，样本的分布会发生变化，也就是被分错的样本会更多的出现在下一次训练集中4.bagging是减少variance，而boosting是减少bias5.对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来6.随机森林对异常值不敏感，GBDT对异常值非常敏感；7.随机森林对训练数据一视同仁，GBDT是基于权值的弱分类器的集成 神经网络感知机与多层网络。。。。。。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql数据同步]]></title>
    <url>%2Fcloud-computing%2Fstorage%2Fmysql%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[阿里巴巴开发的canal，通过模拟slave向master发送请求，获取mysql改动记录，将同步消息发送给redis或者kafka，实现mysql数据同步 canal的工作原理mysql主备复制实现 master将改变记录到二进制日志(binary log)中（这些记录叫做二进制日志事件，binary log events，可以通过show binlog events进行查看）； slave将master的binary log events拷贝到它的中继日志(relay log)； slave重做中继日志中的事件，将改变反映它自己的数据。 canal的工作原理 canal模拟mysql slave的交互协议，伪装自己为mysql slave，向mysql master发送dump协议 mysql master收到dump请求，开始推送binary log给slave(也就是canal) canal解析binary log对象(原始为byte流) 所以，Canal的原理是模拟Slave向Master发送请求，Canal解析binlog，但不将解析结果持久化，而是保存在内存中，每次有客户端读取一次消息，就删除该消息。这里所说的客户端，就需要我们写一个连接Canal的程序，持续从Canal获取数据。 架构server代表一个canal运行实例，对应于一个jvminstance对应于一个数据队列 （1个server对应1..n个instance)instance模块： eventParser (数据源接入，模拟slave协议和master进行交互，协议解析) eventSink (Parser和Store链接器，进行数据过滤，加工，分发的工作) eventStore (数据存储) metaManager (增量订阅&amp;消费信息管理器) mysql的binlog是多文件存储，定位一个LogEvent需要通过binlog filename + binlog position，进行定位mysql的binlog数据格式，按照生成的方式，主要分为：statement-based、row-based、mixed。目前canal只能支持row模式的增量订阅(statement只有sql，没有数据，所以无法获取原始的变更日志) EventParser EventSink 说明：数据过滤：支持通配符的过滤模式，表名，字段内容等数据路由/分发：解决1:n (1个parser对应多个store的模式)数据归并：解决n:1 (多个parser对应1个store)数据加工：在进入store之前进行额外的处理，比如join EventStore 定义了3个cursorPut : Sink模块进行数据存储的最后一次写入位置Get : 数据订阅获取的最后一次提取位置Ack : 数据消费成功的最后一次消费位置 增量订阅/消费设计 可靠性与可用性1，虽然canal服务端解析binlog后不会把数据持久化，但canal服务端会记录每次客户端消费的位置(客户端每次ack时服务端会记录pos点)。如果数据正在更新时，canal服务端挂掉，客户端也会跟着挂掉，mysql依然在插入数据，而redis则因为客户端的关闭而停止更新，造成mysql和redis的数据不一致。解决办法是，只要重启canal服务端和客户端就可以了，虽然canal服务端因为重启之前解析数据清空，但因为canal服务端记录的是客户端最后一次获取的pos点，canal服务端再从这个pos点开始解析，客户端更新至redis，以达到数据的一致。2，如果只有一个canal服务端和一个客户端，肯定存在可用性低的问题，一种做法是用程序来监控canal服务端和客户端，如果挂掉，再重启；一种做法是多个canal服务端+zk，将canal服务端的配置文件放在zk，任何一个canal服务端挂掉后，切换到其他canal服务端，读到的配置文件的内容就是一致的（还有记录的消费pos点），保证业务的高可用，客户端可使用相同的做法。]]></content>
      <categories>
        <category>cloud-computing</category>
        <category>storage</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql binlog原理]]></title>
    <url>%2Fcloud-computing%2Fstorage%2Fmysql-binlog%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Mysql 的binlog，是mysql执行改动产生的二进制日志文件，记录对数据发生或潜在发生更改的SQL语句，并以二进制的形式保存在磁盘中。其主要作用有两个： 数据恢复和主从数据库。用于slave端执行增删改，保持与master同步。 binlog概述文件位置：默认存放位置为数据库文件所在目录下 文件的命名方式： 名称为hostname-bin.xxxxx （重启mysql一次将会自动生成一个新的binlog） 状态的查看：mysql&gt; show variables like ‘%log_bin%’;1234567891011121314151617mysql&gt; show variables like '%log_bin%';+---------------------------------+-------+| Variable_name | Value |+---------------------------------+-------+| log_bin | ON | //表示当前已开启二进制日志//| log_bin_trust_function_creators | OFF || sql_log_bin | ON |+---------------------------------+-------+3 rows in set (0.00 sec) binlog查看主要保存日志文件名、开始偏移、事件类型、结束偏移、时间戳，info，是否自动提交事务等信息 在客户端中使用 show binlog events in ‘mysql_bin.000001’ 语句进行查看 用mysql自带的工具mysqlbinlog 利用binlog进行数据恢复见 mysql数据同步]]></content>
      <categories>
        <category>cloud-computing</category>
        <category>storage</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive 文件存储格式]]></title>
    <url>%2Fcloud-computing%2Fstorage%2Fhive-%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[创建一个表时可以指定文件存储格式，默认格式是textfile,还有其他sequencefile，rcfile格式,总的来说就是文本、二进制、压缩三种格式 textfile textfile为默认格式 存储方式：行存储 磁盘开销大 数据解析开销大 可结合Gzip、Bzip2使用(系统自动检查，执行查询时自动解压)，但是使用这种方式，hive不会对数据进行切分，从无法对数据进行并行操作。 示例：12345678910111213//建表 create table if not exists textfile_table( sourceSystem_en string, sourceSystem_cn string) row format delimited fields terminated by '\t' stored as textfile;//插入数据前操作 hive&gt; set hive.exec.compress.output=true; hive&gt; set mapred.output.compress=true; hive&gt; set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec; hive&gt; set io.compression.codecs=org.apache.hadoop.io.compress.GzipCodec; //插入数据 hive&gt; insert overwrite table textfile_table select * from testfile_table; sequencefile 二进制文件,以&lt;key,value&gt;的形式序列化到文件中 存储方式：行存储 可分割 压缩 一般选择block压缩 这种二进制文件内部使用Hadoop 的标准的Writable 接口实现序列化和反序列化。优势是文件和hadoop api中的mapfile是相互兼容的。 SEQUENCEFILE支持三种压缩选择：NONE、RECORD、BLOCK。RECORD压缩率低，一般建议使用BLOCK压缩。示例:1234567891011121314//建表 create table if not exists seqfile_table( site string, url string, pv bigint, label string ) row format delimited fields terminated by '\t' stored as sequencefile; //插入数据前设置相关属性 hive&gt; set hive.exec.compress.output=true; hive&gt; set mapred.output.compression.type=BLOCK; //插入数据 insert overwrite table seqfile_table select * from textfile_table; rcfile 存储方式：数据按行分块 每块按照列存储 压缩快 快速列存取 读记录尽量涉及到的block最少 读取需要的列只需要读取每个row group 的头部定义。 读取全量数据的操作 性能可能比sequencefile没有明显的优势示例：12345678910111213//建表 create table if not exists rcfile_table( site string, url string, pv bigint, label string ) row format delimited fields terminated by '\t' stored as rcfile; 插入数据操作： set hive.exec.compress.output=true; set mapred.output.compress=true; //插入数据 insert overwrite table rcfile_table select * from testfile_table; 总结 SEQUENCEFILE、RCFILE、RCFILE格式的表不能直接从本地文件导入数据，数据要先导入到TEXTFILE格式的表中，然后再从表中用insert导入到SEQUENCEFILE、RCFILE、ORCFILE等。]]></content>
      <categories>
        <category>cloud-computing</category>
        <category>storage</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive orc 存储]]></title>
    <url>%2Fcloud-computing%2Fstorage%2Fhive-orc-%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[ORC的全称是(Optimized Record Columnar)，使用ORC文件格式可以提高hive读、写和处理数据的能力。 RCFile的设计与实现RCFile（Record Columnar File）存储结构遵循的是“先水平划分，再垂直划分”的设计理念，这个想法来源于PAX。它结合了行存储和列存储的优点：首先，RCFile保证同一行的数据位于同一节点，因此元组重构的开销很低；其次，像列存储一样，RCFile能够利用列维度的数据压缩，并且能跳过不必要的列读取。下图是一个HDFS块内RCFile方式存储的例子。 ORC在RCFile的基础上进行了一定的改进，所以与RCFile相比，具有以下一些优势： 1、ORC中的特定的序列化与反序列化操作可以使ORC file writer根据数据类型进行写出。 2、提供了多种RCFile中没有的indexes，这些indexes可以使ORC的reader很快的读到需要的数据，并且跳过无用数据，这使得ORC文件中的数据可以很快的得到访问。 3、由于ORC file writer可以根据数据类型进行写出，所以ORC可以支持复杂的数据结构（比如Map等）。 4、除了上面三个理论上就具有的优势之外，ORC的具体实现上还有一些其他的优势，比如ORC的stripe默认大小更大，为ORC writer提供了一个memory manager来管理内存使用情况。 ORC文件格式每个stripe包含index data、row data和stripe footer。stripe footer contains a directory of stream locations.Row data is used in table scans.Index data includes min and max values for each column and the row positions within each column. 语法123CREATE TABLE ... STORED AS ORCALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORCSET hive.default.fileformat=Orc TBLPROPERTIES的参数主要有： Key Default Notes orc.compress ZLIB high level compression (one of NONE, ZLIB, SNAPPY) orc.compress.size 262,144 number of bytes in each compression chunk orc.stripe.size 67,108,864 number of bytes in each stripe orc.row.index.stride 10,000 number of rows between index entries (must be &gt;= 1000) orc.create.index true whether to create row indexes orc.bloom.filter.columns “” comma separated list of column names for which bloom filter should be created orc.bloom.filter.fpp 0.05 false positive probability for bloom filter (must &gt;0.0 and &lt;1.0) ORC数据存储方法在ORC格式的hive表中，记录首先会被横向的切分为多个stripes，然后在每一个stripe内数据以列为单位进行存储，所有列的内容都保存在同一个文件中。每个stripe的默认大小为256MB，相对于RCFile每个4MB的stripe而言，更大的stripe使ORC的数据读取更加高效。 对于复杂数据类型，比如Map，ORC文件会将一个复杂数据类型字段解析成多个子字段。如： Data type child columns Arrya 一个包含所有数组元素的子字段 Map 两个子字段，一个key字段，一个value字段 Struct 每一个属性对应一个子字段 Union 每一个属性对应一个子字段 当字段类型都被解析后，会由这些字段类型组成一个字段树，只有树的叶子节点才会保存表数据，这些叶子节点中的数据形成一个数据流，如上图中的Data Stream。为了使ORC文件的reader更加高效的读取数据，字段的metadata会保存在Meta Stream中。在字段树中，每一个非叶子节点记录的就是字段的metadata，比如对一个array来说，会记录它的长度。下图根据表的字段类型生成了一个对应的字段树。12345CREATE TABLE `orcStructTable`( `name` string, `course` struct&lt;course:string,score:int&gt;, `score` map&lt;string,int&gt;, `work_locations` array&lt;string&gt;) 使用ORC文件格式时，用户可以使用HDFS的每一个block存储ORC文件的一个stripe。对于一个ORC文件来说，stripe的大小一般需要设置得比HDFS的block小，如果不这样的话，一个stripe就会分别在HDFS的多个block上，当读取这种数据时就会发生远程读数据的行为。如果设置stripe的只保存在一个block上的话，如果当前block上的剩余空间不足以存储下一个strpie，ORC的writer接下来会将数据打散保存在block剩余的空间上，直到这个block存满为止。这样，下一个stripe又会从下一个block开始存储。 索引在ORC文件中添加索引是为了更加高效的从HDFS读取数据。在ORC文件中使用的是稀疏索引(sparse indexes)。在ORC文件中主要有两种用途的索引，一个是数据统计(Data Statistics)索引，一个是位置指针(Position Pointers)索引。 Data StatisticsORC reader用这个索引来跳过读取不必要的数据，在ORC writer生成ORC文件时会创建这个索引文件。这个索引中统计的信息主要有记录的条数，记录的max, min, sum值，以及对text类型和binary类型字段还会记录其长度。对于复杂数据类型，比如Array, Map, Struct, Union，它们的子字段中也会记录这些统计信息。如图: Position Pointers当读取一个ORC文件时，ORC reader需要有两个位置信息才能准确的进行数据读取操作。（1）metadata streams和data streams中每个group的开始位置由于每个stripe中有多个group，ORC reader需要知道每个group的metadata streams和data streams的开始位置。ORC文件结构图中右边的虚线代表的就是这种pointer。（2）stripes的开始位置由于一个ORC文件可以包含多个stripes，并且一个HDFS block也能包含多个stripes。为了快速定位指定stripe的位置，需要知道每个stripe的开始位置。这些信息会保存在ORC file的File Footer中。如ORC文件结构图中间位置的虚线所示。 文件压缩ORC文件使用两级压缩机制，首先将一个数据流使用流式编码器进行编码，然后使用一个可选的压缩器对数据流进行进一步压缩。一个column可能保存在一个或多个数据流中，可以将数据流划分为以下四种类型： Byte Stream字节流保存一系列的字节数据，不对数据进行编码。 Run Length Byte Stream字节长度字节流保存一系列的字节数据，对于相同的字节，保存这个重复值以及该值在字节流中出现的位置。 Integer Stream整形数据流保存一系列整形数据。可以对数据量进行字节长度编码以及delta编码。具体使用哪种编码方式需要根据整形流中的子序列模式来确定。 Bit Field Stream比特流主要用来保存boolean值组成的序列，一个字节代表一个boolean值，在比特流的底层是用Run Length Byte Stream来实现的。 接下来会以Integer和String类型的字段举例来说明。 Integer对于一个整形字段，会同时使用一个比特流和整形流。比特流用于标识某个值是否为null，整形流用于保存该整形字段非空记录的整数值。 String对于一个String类型字段，ORC writer在开始时会检查该字段值中不同的内容数占非空记录总数的百分比不超过0.8的话，就使用字典编码，字段值会保存在一个比特流，一个字节流及两个整形流中。比特流也是用于标识null值的，字节流用于存储字典值，一个整形流用于存储字典中每个词条的长度，另一个整形流用于记录字段值。如果不能用字典编码，ORC writer会知道这个字段的重复值太少，用字典编码效率不高，ORC writer会使用一个字节流保存String字段的值，然后用一个整形流来保存每个字段的字节长度。 在ORC文件中，在各种数据流的底层，用户可以自选ZLIB, Snappy和LZO压缩方式对数据流进行压缩。编码器一般会将一个数据流压缩成一个个小的压缩单元，在目前的实现中，压缩单元的默认大小是256KB。 内存管理当ORC writer写数据时，会将整个stripe保存在内存中。由于stripe的默认值一般比较大，当有多个ORC writer同时写数据时，可能会导致内存不足。为了现在这种并发写时的内存消耗，ORC文件中引入了一个内存管理器。在一个Map或者Reduce任务中内存管理器会设置一个阈值，这个阈值会限制writer使用的总内存大小。当有新的writer需要写出数据时，会向内存管理器注册其大小（一般也就是stripe的大小），当内存管理器接收到的总注册大小超过阈值时，内存管理器会将stripe的实际大小按该writer注册的内存大小与总注册内存大小的比例进行缩小。当有writer关闭时，内存管理器会将其注册的内存从总注册内存中注销。 查看orc格式数据表12345678910111213desc formatted fileformat.test_orc;// Hive version 0.11 through 0.14:hive --orcfiledump &lt;location-of-orc-file&gt;// Hive version 0.15 and later:hive --orcfiledump [-d] [--rowindex &lt;col_ids&gt;] &lt;location-of-orc-file&gt;// Hive version 1.2.0 and later:hive --orcfiledump [-d] [-t] [--rowindex &lt;col_ids&gt;] &lt;location-of-orc-file&gt;// Hive version 1.3.0 and later:hive --orcfiledump [-j] [-p] [-d] [-t] [--rowindex &lt;col_ids&gt;] [--recover] [--skip-dump] [--backup-path &lt;new-path&gt;] &lt;location-of-orc-file-or-directory&gt; Specifying -d in the command will cause it to dump the ORC file data rather than the metadata (Hive 1.1.0 and later).Specifying –rowindex with a comma separated list of column ids will cause it to print row indexes for the specified columns, where 0 is the top level struct containing all of the columns and 1 is the first column id (Hive 1.1.0 and later).Specifying -t in the command will print the timezone id of the writer.Specifying -j in the command will print the ORC file metadata in JSON format. To pretty print the JSON metadata, add -p to the command.Specifying –recover in the command will recover a corrupted ORC file generated by Hive streaming.Specifying –skip-dump along with –recover will perform recovery without dumping metadata.Specifying –backup-path with a new-path will let the recovery tool move corrupted files to the specified backup path (default: /tmp).]]></content>
      <categories>
        <category>cloud-computing</category>
        <category>storage</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法基础]]></title>
    <url>%2Fartificial-intelligence%2Falgorithm%2F%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[机器学习之路初级：统计学基础知识 线性回归线性回归(Linear Regression)是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。 梯度下降算法梯度下降法(Gradient Descent, GD)是一个最优化算法，通常也称为最速下降法。最速下降法是用负梯度方向为搜索方向的，最速下降法越接近目标值，步长越小，前进越慢。梯度下降法的计算过程就是沿梯度下降的方向求解极小值（也可以沿梯度上升方向求解极大值）。思想：从当前点出发，找最陡的坡走一步，到达新的点之后，再找最陡的坡，这样一步一步，快速走到最低点（最小损失函数的收敛点） 优点只需求解损失函数的一阶导数，计算的代价比较小 梯度下降算法与随机梯度下降算法梯度下降算法：需要把m个样本全部带入计算，迭代一次计算量为m*n^2可以得到全局最优解，易于并行实现样本数目很多时，训练过程很慢 随机梯度下降算法：每次只使用一个样本，迭代一次计算量为n^2，当m很大的时候，随机梯度下降迭代一次的速度要远高于梯度下降训练速度快得到的是局部最优解，不易于并行实现 最小二乘法普通最小二乘法（ Ordinary Least Square，OLS）：所选择的回归模型应该使所有观察值的残差平方和达到最小。从Cost/Loss function角度去想，这是统计（机器）学习里面一个重要概念，一般建立模型就是让loss function最小，而最小二乘法可以认为是 loss function = （y_hat -y )^2的一个特例 最小二乘法与梯度下降算法如果把最小二乘看做是优化问题的话，那么梯度下降是求解方法的一种（线性和非线性都可以），x=(A^T A)^{-1}A^Tb是求解线性最小二乘的一种，高斯-牛顿法和Levenberg-Marquardt则能用于求解非线性最小二乘。知乎里有篇帖子分析的不错，最小二乘法和梯度下降法有哪些区别？ 牛顿法使用迭代的方法 寻找使f( θ )=0 的 θ 值先随机选一个点，然后求出f在该点的切线，即f在该点的导数。该切线等于0的点，即该切线与x轴相交的点为下一次迭代的值。直至逼近f等于0的点。过程如下图： 牛顿方法通常比梯度下降收敛速度快，迭代次数也少。但因为要计算Hessian矩阵的逆，所以每次迭代计算量比较大。当Hessian矩阵不是很大时牛顿方法要优于梯度下降。 拟牛顿法牛顿法需要计算Hessian矩阵的逆矩阵，运算复杂度太高。因此，很多牛顿算法的变形出现了，这类变形统称拟牛顿算法。拟牛顿算法的核心思想用一个近似矩阵(B)替代逆Hessian矩阵({H^{ - 1}})。不同算法的矩阵(B)的计算有差异，但大多算法都是采用迭代更新的思想在tranning的每一轮更新矩阵(B)。]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式消息系统：kafka]]></title>
    <url>%2Fcloud-computing%2Farchitecture%2F%E5%88%86%E5%B8%83%E5%BC%8F%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F%EF%BC%9Akafka%2F</url>
    <content type="text"><![CDATA[Kafka是分布式发布-订阅消息系统。 特性 高吞吐量、低延迟：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒 可扩展性：kafka集群支持热扩展 持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失 容错性：允许集群中节点失败（若副本数量为n,则允许n-1个节点失败） 高并发：支持数千个客户端同时读写 架构kafka的集群有多个Broker服务器组成，每个类型的消息被定义为topic，同一topic内部的消息按照一定的key和算法被分区(partition)存储在不同的Broker上，消息生产者producer和消费者consumer可以在多个Broker上生产/消费topic Topic 每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处） Partition Parition是物理上的概念,为了使得Kafka的吞吐率可以线性提高，物理上把Topic分成一个或多个Partition，每个Partition在物理上对应一个文件夹，该文件夹下存储这个Partition的所有消息和索引文件。因为每条消息都被append到该Partition中，属于顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证）。 Producer 负责发布消息到Kafka broker Consumer 消息消费者，向Kafka broker读取消息的客户端。 Consumer Group 每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。 因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高Kafka性能无关。选择怎样的删除策略只与磁盘以及具体的需求有关。另外，Kafka会为每一个Consumer Group保留一些metadata信息——当前消费的消息的position，也即offset。这个offset由Consumer控制。正常情况下Consumer会在消费完一条消息后递增该offset。当然，Consumer也可将offset设成一个较小的值，重新消费一些消息。因为offet由Consumer控制，所以Kafka broker是无状态的，它不需要标记哪些消息被哪些消费过，也不需要通过broker去保证同一个Consumer Group只有一个Consumer能消费某一条消息，因此也就不需要锁机制，这也为Kafka的高吞吐率提供了有力保障。Producer发送消息到broker时，会根据Paritition机制选择将其存储到哪一个Partition。如果Partition机制设置合理，所有消息可以均匀分布到不同的Partition里，这样就实现了负载均衡。 作为一个消息系统，Kafka遵循了传统的方式，选择由Producer向broker push消息并由Consumer从broker pull消息。 优势 吞吐量数据磁盘持久化：消息不在内存中cache，直接写入到磁盘，充分利用磁盘的顺序读写性能zero-copy：减少IO操作步骤数据批量发送数据压缩Topic划分为多个partition，提高parallelism 负载均衡producer根据用户指定的算法，将消息发送到指定的partition存在多个partiiton，每个partition有自己的replica，每个replica分布在不同的Broker节点上多个partition需要选取出lead partition，lead partition负责读写，并由zookeeper负责fail over通过zookeeper管理broker与consumer的动态加入与离开 push vs pull系统由于kafka broker会持久化数据，broker没有内存压力，因此，consumer非常适合采取pull的方式消费数据，具有以下几点好处：(1)简化kafka设计(2)consumer根据消费能力自主控制消息拉取速度(3)consumer根据自身情况自主选择消费模式，例如批量，重复消费，从尾端开始消费等 可扩展性当需要增加broker结点时，新增的broker会向zookeeper注册，而producer及consumer会根据注册在zookeeper上的watcher感知这些变化，并及时作出调整。 设计要点 直接使用linux 文件系统的cache，来高效缓存数据。 采用linux Zero-Copy提高发送性能。传统的数据发送需要发送4次上下文切换，采用sendfile系统调用之后，数据直接在内核态交换，系统上下文切换减少为2次。根据测试结果，可以提高60%的数据发送性能。 数据在磁盘上存取代价为O(1)。kafka以topic来进行消息管理，每个topic包含多个part（ition），每个part对应一个逻辑log，有多个segment组成。每个segment中存储多条消息，消息id由其逻辑位置决定，即从消息id可直接定位到消息的存储位置，避免id到位置的额外映射。每个part在内存中对应一个index，记录每个segment中的第一条消息偏移。发布者发到某个topic的消息会被均匀的分布到多个part上（随机或根据用户指定的回调函数进行分布），broker收到发布消息往对应part的最后一个segment上添加该消息，当某个segment上的消息条数达到配置值或消息发布时间超过阈值时，segment上的消息会被flush到磁盘，只有flush到磁盘上的消息订阅者才能订阅到，segment达到一定的大小后将不会再往该segment写数据，broker会创建新的segment。 显式分布式，即所有的producer、broker和consumer都会有多个，均为分布式的。Producer和broker之间没有负载均衡机制。broker和consumer之间利用zookeeper进行负载均衡。所有broker和consumer都会在zookeeper中进行注册，且zookeeper会保存他们的一些元数据信息。如果某个broker和consumer发生了变化，所有其他的broker和consumer都会得到通知。 关键点 zookeeper在kafka的作用是什么？管理broker和consumer，实现负载均衡 kafka中几乎不允许对消息进行“随机读写”的原因是什么？顺序读写效率更高 kafka集群consumer和producer状态信息是如何保存的？因为offet由Consumer控制，所以Kafka broker是无状态的，它不需要标记哪些消息被哪些消费过，也不需要通过broker去保证同一个Consumer Group只有一个Consumer能消费某一条消息，因此也就不需要锁机制，这也为Kafka的高吞吐率提供了有力保障。 partitions设计的目的的根本原因是什么？并行]]></content>
      <categories>
        <category>cloud-computing</category>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>architecture</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EM算法(最大期望算法)]]></title>
    <url>%2Fartificial-intelligence%2Falgorithm%2FEM-algorithm%2F</url>
    <content type="text"><![CDATA[EM算法是一种迭代算法，用于含有隐含变量（hidden variable）的概率模型参数的极大似然估计，或极大后验概率估计。EM算法的每次迭代由两步组成：E步，求期望（expectation）; M步，求极大（maximization） 相关统计知识极大似然估计极大似然估计，只是一种概率论在统计学的应用，它是参数估计的方法之一。说的是已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察其结果，利用结果推出参数的大概值。最大似然估计是建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。求最大似然函数估计值的一般步骤：（1）写出似然函数；（2）对似然函数取对数，并整理；（3）求导数，令导数为0，得到似然方程；（4）解似然方程，得到的参数即为所求； Jensen不等式如果f是上凸函数，X是随机变量，那么f(E[X]) ≥ E[f(X)] 上凸函数：函数f(x)满足对定义域上任意两个数a,b都有f[(a+b)/2] ≥ [f(a)+f(b)]/2 特别地，如果f是严格上凸函数，那么E[f(X)] = f(E[X])当且仅当p(X=E[X])，也就是说X是常量。 EM算法的引入也就是为了解决什么问题：如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或贝叶斯估计法估计模型参数。但是，当模型含有隐含变量时，就不能简单的使用这些估计方法。EM算法就是含有隐变量的概率模型参数的极大似然估计法。 EM算法首先选取参数的初值，只有通过迭代计算参数的估计值，直到收敛为知。那么问题来了： 初值怎么选 怎么个迭代法 怎么算收敛 初值怎么选EM算法与初值的选择有关，选择不同的初值可能得到不同的参数估计值通常情况下，选取几个不同的初值进行迭代，然后对得到的各个估计值加以比较，从中选择最好的 迭代法E步： 收敛性参数估计序列只能收敛到对数似然函数序列的稳定点，不能保证收敛到极大值点 EM算法的收敛性EM算法的应用EM算法的推广]]></content>
      <categories>
        <category>artificial-intelligence</category>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>EM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python HTTP库：httplib]]></title>
    <url>%2Ftechnology%2Fprogramming%2Fpython-HTTP%E5%BA%93%EF%BC%9Ahttplib%2F</url>
    <content type="text"></content>
      <categories>
        <category>technology</category>
        <category>programming</category>
      </categories>
      <tags>
        <tag>programming</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python HTTP库：Request（初级）]]></title>
    <url>%2Ftechnology%2Fprogramming%2Fpython-HTTP%E5%BA%93%EF%BC%9ARequest%EF%BC%88%E5%88%9D%E7%BA%A7%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Requests 使用的是 urllib3，继承了urllib2的所有特性。Requests支持HTTP连接保持和连接池，支持使用cookie保持会话，支持文件上传，支持自动确定响应内容的编码，支持国际化的 URL 和 POST 数据自动编码。 发送请求get请求r = requests.get(&#39;https://github.com/timeline.json&#39;) post请求r = requests.post(&quot;http://httpbin.org/post&quot;)现在，我们有一个名为 r 的 Response 对象。我们可以从这个对象中获取所有我们想要的信息。那么其他 HTTP 请求类型：PUT，DELETE，HEAD 以及 OPTIONS 又是如何的呢？都是一样的简单：1234r = requests.put("http://httpbin.org/put")r = requests.delete("http://httpbin.org/delete")r = requests.head("http://httpbin.org/get")r = requests.options("http://httpbin.org/get") 传递 URL 参数举例来说，如果你想传递 key1=value1 和 key2=value2 到 httpbin.org/get ，那么你可以使用如下代码：12payload = &#123;'key1': 'value1', 'key2': 'value2'&#125;r = requests.get("http://httpbin.org/get", params=payload) 通过打印输出该 URL，你能看到 URL 已被正确编码：12&gt;&gt;&gt; print(r.url)http://httpbin.org/get?key2=value2&amp;key1=value1 你还可以将一个列表作为值传入：1234&gt;&gt;&gt; payload = &#123;'key1': 'value1', 'key2': ['value2', 'value3']&#125;&gt;&gt;&gt; r = requests.get('http://httpbin.org/get', params=payload)&gt;&gt;&gt; print(r.url)http://httpbin.org/get?key1=value1&amp;key2=value2&amp;key2=value3 响应内容文本编码1234&gt;&gt;&gt; import requests&gt;&gt;&gt; r = requests.get('https://github.com/timeline.json')&gt;&gt;&gt; r.textu'[&#123;"repository":&#123;"open_issues":0,"url":"https://github.com/... Requests 会自动解码来自服务器的内容。大多数 unicode 字符集都能被无缝地解码。 二进制响应内容你也能以字节的方式访问请求响应体，对于非文本请求：12&gt;&gt;&gt; r.contentb'[&#123;"repository":&#123;"open_issues":0,"url":"https://github.com/... Requests 会自动为你解码 gzip 和 deflate 传输编码的响应数据。例如，以请求返回的二进制数据创建一张图片，你可以使用如下代码：1234&gt;&gt;&gt; from PIL import Image&gt;&gt;&gt; from io import StringIO&gt;&gt;&gt; i = Image.open(StringIO(r.content)) JSON 响应内容123r = requests.get('https://github.com/timeline.json')&gt;&gt;&gt; r.json()[&#123;u'repository': &#123;u'open_issues': 0, u'url': 'https://github.com/... 如果 JSON 解码失败， r.json 就会抛出一个异常。例如，相应内容是 401 (Unauthorized)，尝试访问 r.json 将会抛出 ValueError: No JSON object could be decoded 异常。 原始响应内容在罕见的情况下，你可能想获取来自服务器的原始套接字响应，那么你可以访问 r.raw。 如果你确实想这么干，那请你确保在初始请求中设置了 stream=True。具体你可以这么做：12345&gt;&gt;&gt; r = requests.get('https://github.com/timeline.json', stream=True)&gt;&gt;&gt; r.raw&lt;requests.packages.urllib3.response.HTTPResponse object at 0x101194810&gt;&gt;&gt;&gt; r.raw.read(10)'\x1f\x8b\x08\x00\x00\x00\x00\x00\x00\x03' 但一般情况下，你应该以下面的模式将文本流保存到文件：123with open(filename, 'wb') as fd: for chunk in r.iter_content(chunk_size): fd.write(chunk) 使用 Response.iter_content 将会处理大量你直接使用 Response.raw 不得不处理的。 当流下载时，上面是优先推荐的获取内容方式。 定制请求头如果你想为请求添加 HTTP 头部，只要简单地传递一个 dict 给 headers 参数就可以了。例如，在前一个示例中我们没有指定 content-type:1234&gt;&gt;&gt; url = 'https://api.github.com/some/endpoint'&gt;&gt;&gt; headers = &#123;'user-agent': 'my-app/0.0.1'&#125;&gt;&gt;&gt; r = requests.get(url, headers=headers) 注意: 定制 header 的优先级低于某些特定的信息源，例如： 如果在 .netrc 中设置了用户认证信息，使用 headers= 设置的授权就不会生效。而如果设置了 auth= 参数，.netrc 的设置就无效了。如果被重定向到别的主机，授权 header 就会被删除。代理授权 header 会被 URL 中提供的代理身份覆盖掉。在我们能判断内容长度的情况下，header 的 Content-Length 会被改写。更进一步讲，Requests 不会基于定制 header 的具体情况改变自己的行为。只不过在最后的请求中，所有的 header 信息都会被传递进去。 注意: 所有的 header 值必须是 string、bytestring 或者 unicode。尽管传递 unicode header 也是允许的，但不建议这样做。 更加复杂的 POST 请求通常，你想要发送一些编码为表单形式的数据——非常像一个 HTML 表单。要实现这个，只需简单地传递一个字典给 data 参数。你的数据字典在发出请求时会自动编码为表单形式：123456789101112&gt;&gt;&gt; payload = &#123;'key1': 'value1', 'key2': 'value2'&#125;&gt;&gt;&gt; r = requests.post("http://httpbin.org/post", data=payload)&gt;&gt;&gt; print(r.text)&#123; ... "form": &#123; "key2": "value2", "key1": "value1" &#125;, ...&#125; 很多时候你想要发送的数据并非编码为表单形式的。如果你传递一个 string 而不是一个 dict，那么数据会被直接发布出去。 例如，Github API v3 接受编码为 JSON 的 POST/PATCH 数据：123456&gt;&gt;&gt; import json&gt;&gt;&gt; url = 'https://api.github.com/some/endpoint'&gt;&gt;&gt; payload = &#123;'some': 'data'&#125;&gt;&gt;&gt; r = requests.post(url, data=json.dumps(payload)) 此处除了可以自行对 dict 进行编码，你还可以使用 json 参数直接传递，然后它就会被自动编码。这是 2.4.2 版的新加功能：1234&gt;&gt;&gt; url = 'https://api.github.com/some/endpoint'&gt;&gt;&gt; payload = &#123;'some': 'data'&#125;&gt;&gt;&gt; r = requests.post(url, json=payload) 响应状态码我们可以检测响应状态码：123&gt;&gt;&gt; r = requests.get('http://httpbin.org/get')&gt;&gt;&gt; r.status_code200 为方便引用，Requests还附带了一个内置的状态码查询对象：12&gt;&gt;&gt; r.status_code == requests.codes.okTrue 如果发送了一个错误请求(一个 4XX 客户端错误，或者 5XX 服务器错误响应)，我们可以通过 Response.raise_for_status() 来抛出异常：123456789&gt;&gt;&gt; bad_r = requests.get('http://httpbin.org/status/404')&gt;&gt;&gt; bad_r.status_code404&gt;&gt;&gt; bad_r.raise_for_status()Traceback (most recent call last): File "requests/models.py", line 832, in raise_for_status raise http_errorrequests.exceptions.HTTPError: 404 Client Error 但是，由于我们的例子中 r 的 status_code 是 200 ，当我们调用 raise_for_status() 时，得到的是：12&gt;&gt;&gt; r.raise_for_status()None 响应头我们可以查看以一个 Python 字典形式展示的服务器响应头：12345678910&gt;&gt;&gt; r.headers&#123; 'content-encoding': 'gzip', 'transfer-encoding': 'chunked', 'connection': 'close', 'server': 'nginx/1.0.4', 'x-runtime': '148ms', 'etag': '"e1ca502697e5c9317743dc078f67693f"', 'content-type': 'application/json'&#125; 但是这个字典比较特殊：它是仅为 HTTP 头部而生的。根据 RFC 2616， HTTP 头部是大小写不敏感的。因此，我们可以使用任意大写形式来访问这些响应头字段：12345&gt;&gt;&gt; r.headers['Content-Type']'application/json'&gt;&gt;&gt; r.headers.get('content-type')'application/json' 它还有一个特殊点，那就是服务器可以多次接受同一 header，每次都使用不同的值。但 Requests 会将它们合并，这样它们就可以用一个映射来表示出来，参见 RFC 7230: A recipient MAY combine multiple header fields with the same field name into one “field-name: field-value” pair, without changing the semantics of the message, by appending each subsequent field value to the combined field value in order, separated by a comma. 接收者可以合并多个相同名称的 header 栏位，把它们合为一个 “field-name: field-value” 配对，将每个后续的栏位值依次追加到合并的栏位值中，用逗号隔开即可，这样做不会改变信息的语义。 Cookie如果某个响应中包含一些 cookie，你可以快速访问它们：12345&gt;&gt;&gt; url = 'http://example.com/some/cookie/setting/url'&gt;&gt;&gt; r = requests.get(url)&gt;&gt;&gt; r.cookies['example_cookie_name']'example_cookie_value' 要想发送你的cookies到服务器，可以使用 cookies 参数：123456&gt;&gt;&gt; url = 'http://httpbin.org/cookies'&gt;&gt;&gt; cookies = dict(cookies_are='working')&gt;&gt;&gt; r = requests.get(url, cookies=cookies)&gt;&gt;&gt; r.text'&#123;"cookies": &#123;"cookies_are": "working"&#125;&#125;' 重定向与请求历史默认情况下，除了 HEAD, Requests 会自动处理所有重定向。可以使用响应对象的 history 方法来追踪重定向。Response.history 是一个 Response 对象的列表，为了完成请求而创建了这些对象。这个对象列表按照从最老到最近的请求进行排序。例如，Github 将所有的 HTTP 请求重定向到 HTTPS：123456789&gt;&gt;&gt; r = requests.get('http://github.com')&gt;&gt;&gt; r.url'https://github.com/'&gt;&gt;&gt; r.status_code200&gt;&gt;&gt; r.history[&lt;Response [301]&gt;] 如果你使用的是GET、OPTIONS、POST、PUT、PATCH 或者 DELETE，那么你可以通过 allow_redirects 参数禁用重定向处理：12345&gt;&gt;&gt; r = requests.get('http://github.com', allow_redirects=False)&gt;&gt;&gt; r.status_code301&gt;&gt;&gt; r.history[] 如果你使用了 HEAD，你也可以启用重定向：12345&gt;&gt;&gt; r = requests.head('http://github.com', allow_redirects=True)&gt;&gt;&gt; r.url'https://github.com/'&gt;&gt;&gt; r.history[&lt;Response [301]&gt;] 超时你可以告诉 requests 在经过以 timeout 参数设定的秒数时间之后停止等待响应：1234&gt;&gt;&gt; requests.get('http://github.com', timeout=0.001)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;requests.exceptions.Timeout: HTTPConnectionPool(host='github.com', port=80): Request timed out. (timeout=0.001) 注意timeout 仅对连接过程有效，与响应体的下载无关。 timeout 并不是整个下载响应的时间限制，而是如果服务器在 timeout 秒内没有应答，将会引发一个异常（更精确地说，是在 timeout 秒内没有从基础套接字上接收到任何字节的数据时） 错误与异常遇到网络问题（如：DNS 查询失败、拒绝连接等）时，Requests 会抛出一个 ConnectionError 异常。如果 HTTP 请求返回了不成功的状态码， Response.raise_for_status() 会抛出一个 HTTPError 异常。若请求超时，则抛出一个 Timeout 异常。若请求超过了设定的最大重定向次数，则会抛出一个 TooManyRedirects 异常。所有Requests显式抛出的异常都继承自 requests.exceptions.RequestException 。 缺陷requests不是python自带的库，需要另外安装 easy_install or pip installrequests缺陷:直接使用不能异步调用，速度慢（from others）。官方的urllib可以替代它。不建议使用requests模块]]></content>
      <categories>
        <category>technology</category>
        <category>programming</category>
      </categories>
      <tags>
        <tag>programming</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python HTTP库 urllib和urllib2]]></title>
    <url>%2Ftechnology%2Fprogramming%2Fpython-HTTP%E5%BA%93-urllib%E5%92%8Curllib2%2F</url>
    <content type="text"><![CDATA[urllib和urllib2模块都做与请求URL相关的操作，但他们提供不同的功能。urllib2.urlopen accepts an instance of the Request class or a url, （whereas urllib.urlopen only accepts a url 中文意思就是：urllib2.urlopen可以接受一个Request对象或者url，（在接受Request对象时候，并以此可以来设置一个URL的headers），urllib.urlopen只接收一个urlurllib 有urlencode,urllib2没有，这也是为什么总是urllib，urllib2常会一起使用的原因 Proxy 的设置urllib2 默认会使用环境变量 http_proxy 来设置 HTTP Proxy。如果想在程序中明确控制 Proxy 而不受环境变量的影响，可以使用下面的方式123456789101112import urllib2enable_proxy = Trueproxy_handler = urllib2.ProxyHandler(&#123;"http" : 'http://some-proxy.com:8080'&#125;)null_proxy_handler = urllib2.ProxyHandler(&#123;&#125;)if enable_proxy: opener = urllib2.build_opener(proxy_handler)else: opener = urllib2.build_opener(null_proxy_handler)urllib2.install_opener(opener) 这里要注意的一个细节，使用 urllib2.install_opener() 会设置 urllib2 的全局 opener 。这样后面的使用会很方便，但不能做更细粒度的控制，比如想在程序中使用两个不同的 Proxy 设置等。比较好的做法是不使用 install_opener 去更改全局的设置，而只是直接调用 opener 的 open 方法代替全局的 urlopen 方法。 Timeout 设置12import urllib2response = urllib2.urlopen('http://www.google.com', timeout=10) 在 HTTP Request 中加入特定的 Header在 HTTP Request 中加入特定的 Header要加入 header，需要使用 Request 对象:123request = urllib2.Request(uri)request.add_header('User-Agent', 'fake-client')response = urllib2.urlopen(request) User-Agent : 有些服务器或 Proxy 会通过该值来判断是否是浏览器发出的请求Content-Type : 在使用 REST 接口时，服务器会检查该值，用来确定 HTTP Body 中的内容该怎样解析。常见的取值有： application/xml ： 在 XML RPC，如 RESTful/SOAP 调用时使用 application/json ： 在 JSON RPC 调用时使用 application/x-www-form-urlencoded ： 浏览器提交 Web 表单时使用在使用服务器提供的 RESTful 或 SOAP 服务时， Content-Type 设置错误会导致服务器拒绝服务 Redirecturllib2 默认情况下会针对 HTTP 3XX 返回码自动进行 redirect 动作，无需人工配置。要检测是否发生了 redirect 动作，只要检查一下 Response 的 URL 和 Request 的 URL 是否一致就可以了。12response = urllib2.urlopen('http://www.google.cn')redirected = response.geturl() == 'http://www.google.cn' 如果不想自动 redirect，除了使用更低层次的 httplib 库之外，还可以自定义 HTTPRedirectHandler 类。 Cookie123456cookie = cookielib.CookieJar()opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie))response = opener.open('http://www.google.com')for item in cookie: if item.name == 'some_cookie_item_name': print item.value 使用 HTTP 的 PUT 和 DELETE 方法urllib2 只支持 HTTP 的 GET 和 POST 方法，如果要使用 HTTP PUT 和 DELETE ，只能使用比较低层的 httplib 库。虽然如此，我们还是能通过下面的方式，使 urllib2 能够发出 PUT 或 DELETE 的请求：123request = urllib2.Request(uri, data=data)request.get_method = lambda: 'PUT' # or 'DELETE'response = urllib2.urlopen(request) 得到 HTTP 的返回码1234try: response = urllib2.urlopen('http://restrict.web.com')except urllib2.HTTPError, e: print e.code Debug Log异常处理URLError异常通常引起URLError的原因是：无网络连接（没有到目标服务器的路由）、访问的目标服务器不存在。在这种情况下，异常对象会有reason属性（是一个（错误码、错误原因）的元组）。12345url="http://www.baidu.com/"try: response=urllib2.urlopen(url)except urllib2.URLError,e: print e.reason HTTPError每一个从服务器返回的HTTP响应都有一个状态码。其中，有的状态码表示服务器不能完成相应的请求，默认的处理程序可以为我们处理一些这样的状态码（如返回的响应是重定向，urllib2会自动为我们从重定向后的页面中获取信息）。有些状态码，urllib2模块不能帮我们处理，那么urlopen函数就会引起HTTPError异常,其中典型的有404/401。HTTPError异常的实例有整数类型的code属性，表示服务器返回的错误状态码。urllib2模块默认的处理程序可以处理重定向（状态码是300范围），而且状态码在100-299范围内表示成功。因此，能够引起HTTPError异常的状态码范围是：400-599.当引起错误时，服务器会返回HTTP错误码和错误页面。你可以将HTPError实例作为返回页面，这意味着，HTTPError实例不仅有code属性，还有read、geturl、info等方法。123456url="http://cs.scu.edu.cn/~duanlei"try: response=urllib2.urlopen(url)except urllib2.HTTPError,e: print e.code print e.read() 总结如果想在代码中处理URLError和HTTPError有两种方法，代码如下：方法一12345678910111213url="xxxxxx" #需要访问的URLtry: response=urllib2.urlopen(url)except urllib2.HTTPError,e: #HTTPError必须排在URLError的前面 print "The server couldn't fulfill the request" print "Error code:",e.code print "Return content:",e.read()except urllib2.URLError,e: print "Failed to reach the server" print "The reason:",e.reasonelse: #something you should do pass #其他异常的处理 方法二12345678910111213url="http://xxx" #需要访问的URLtry: response=urllib2.urlopen(url)except urllib2.URLError,e: if hasattr(e,"reason"): print "Failed to reach the server" print "The reason:",e.reason elif hasattr(e,"code"): print "The server couldn't fulfill the request" print "Error code:",e.code print "Return content:",e.read()else: pass #其他异常的处理 urllib2各方法实例12345678910111213141516171819202122232425262728293031323334import urllib2response = urllib2.urlopen('http://python.org/')print "Response:", response# Get the URL. This gets the real URL.print "The URL is: ", response.geturl()# Getting the codeprint "This gets the code: ", response.code# Get the Headers.# This returns a dictionary-like object that describes the page fetched,# particularly the headers sent by the serverprint "The Headers are: ", response.info()# Get the date part of the headerprint "The Date is: ", response.info()['date']# Get the server part of the headerprint "The Server is: ", response.info()['server']# Get all datahtml = response.read()print "Get all data: ", html# Get only the lengthprint "Get the length :", len(html)# Showing that the file object is iterablefor line in response: print line.rstrip()# Note that the rstrip strips the trailing newlines and carriage returns before# printing the output. 发送get请求demo例子：1234567891011121314151617181920212223242526272829303132333435363738394041424344#!/usr/bin/python# -*- coding: utf-8 -*-# hello_http_get.pyimport httplib#请求头headers = &#123; "cityId":"1337", "userId":"1234", "token":"abcd", "device-id":"778E5992-6DD7-4AB9-B2CC-741F31134D6C", "caller-id":"python-test", "request-id":"2298760f18a846969725a7752331e3b8", "platform":"5", "platformVersion":"21", "apiVersion":"21", "Content-type":"application/json"&#125;#请求参数params=''#请求资源resource_uri='/helloworld'#请求方法method_type='GET'httpClient=Nonetry: print headers print params httpClient=httplib.HTTPConnection('localhost',9020,timeout=30) httpClient.request(method_type,resource_uri,params,headers) #获取返回数据对象 response=httpClient.getresponse() print response.status #HTTP返回状态 print response.reason #HTTP返回状态描述 print response.read() #输出返回数据except Exception,e: print efinally: if httpClient: httpClient.close() 发送post请求demo例子：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#!/usr/bin/python# -*- coding: utf-8 -*-# hello_http_post.pyimport httplibimport urllibimport json#请求头headers = &#123; "cityId":"1337", "userId":"1234", "token":"abcd", "device-id":"778E5992-6DD7-4AB9-B2CC-741F31134D6C", "caller-id":"python-test", "request-id":"2298760f18a846969725a7752331e3b8", "platform":"5", "platformVersion":"21", "apiVersion":"21", "Content-type":"application/json"&#125;#请求参数params=&#123; "houseId" : "24230", "houseTypeId" : "0", "userPhone" : "13866554433", "userName" : "张三", "houseName" : "helloworld测试楼盘",&#125;#服务端约定为json格式params=json.dumps(params)#服务端约定为body:k1=v1&amp;k2=v2格式#params=urllib.urlencode(params)#请求资源resource_uri='/helloworld'#请求方法method_type='POST'httpClient=Nonetry: print headers print params httpClient=httplib.HTTPConnection('localhost',9020,timeout=30) httpClient.request(method_type,resource_uri,params,headers) #获取返回数据对象 response=httpClient.getresponse() print response.status #HTTP返回状态 print response.reason #HTTP返回状态描述 print response.read() #输出返回数据except Exception,e: print efinally: if httpClient: httpClient.close()]]></content>
      <categories>
        <category>technology</category>
        <category>programming</category>
      </categories>
      <tags>
        <tag>programming</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark源码编译安装与IntelliJ源码阅读环境配置]]></title>
    <url>%2Fcloud-computing%2Farchitecture%2Fspark%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85%E4%B8%8EIntelliJ%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Spark是UC Berkeley AMP lab所开源的类Hadoop MapReduce的通用的并行计算框架，Spark基于map reduce算法实现的分布式计算，拥有Hadoop MapReduce所具有的优点；但不同于MapReduce的是Job中间输出和结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的map reduce的算法。 安装scala1、下载scala压缩包scala官网地址http://www.scala-lang.org/download/2、建立目录，解压文件到所建立目录3、添加环境变量1234567/*编辑配置文件bashrc (该配置文件只对当前用户有效)*/$ vim ~/.bashrc/*在文件的结尾添加如下：×/#add scala configureexport SCALA_PATH=/home/zhongjuliu/servers/scala-2.11.8export PATH=$PATH:$SCALA_PATH/bin/*按esc 输入 :wq 保存并退出×/ 或者编辑/etc/profile配置文件，使得配置对所有用户有效4、测试，观察结果版本号是否一致12$ scala -versionScala code runner version 2.11.2 -- Copyright 2002-2013, LAMP/EPFL 安装git此处省略几个字 安装IntelliJ IDEA（包含scala插件）下载IDEA安装文，可以到Jetbrains官网http://www.jetbrains.com/idea/download/ 选择最新的安装文件。由于以后的练习需要在Linux开发Scala应用程序，选择安装scala插件。 spark源码的下载与编译源码下载spark的github地址https://github.com/apache/sparkgie clone命令：1git clone git://github.com/apache/spark.git 源码编译编译的目的是生成指定环境下运行Spark本身或开发Spark Application的JAR包，本次编译的目的生成运行在hadoop2.2.0上的Spark JAR包。虽然Spark源代码本质上是使用Maven或SBT进行编译，但重点推荐使用SBT进行编译，原因如下：spark本身是使用scala语言编写的，SBT是scala的编译工具spark的官方文档从0.9.0开始没提供maven的编译命令，所以以后版本会不会提供pom还难说spark的部署工具make-distribution.sh内嵌SBT编译命令 maven编译先设置参数，后执行命令需事先安装好maven3.04或maven3.05，并设置要环境变量MAVEN_HOME，将$MAVEN_HOME/bin加入PATH变量。然后将源代码复制到指定目录，然后进入该目录，由于MAVEN工具默认的内存比较小，需要先调大其占用的内存上限：1export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m" 然后打包：123mvn -Pyarn -Dhadoop.version=2.2.0 -Dyarn.version=2.2.0 -DskipTests clean package或者mvn clean assembly:assembly sbt编译 sbt的安装Run the following from the terminal to install sbt (You’ll need superuser privileges to do so, hence the sudo). 1234echo "deb https://dl.bintray.com/sbt/debian /" | sudo tee -a /etc/apt/sources.list.d/sbt.listsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 642AC823sudo apt-get updatesudo apt-get install sbt sbt编译由于其构建信息集成自maven，maven中的profiles也是有效的，所以可以使用下面的命令进行编译、打包： 12345sbt -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver assemly或者build/sbt clean assembly或者构建**自定义hadoop版本**sbt clean assembly -Phive -Phadoop-2.4 -Pyarn -Dhadoop.version=2.4.0.2.1.4.0-632 这里使用的是spark自带的build目录下的sbt如果要运行测试，最好先进行打包：12sbt -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver assemlysbt -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver test 编译spark项目所需内存较大，如果用下载的sbt默认的内存是-Xmx1024m，会发生堆内存溢出。最好使用源代码中自带的build/sbt，它把内存设置为了-Xmx2048m，这样编译不会内存溢出。 用spark自带的dev/make-distribution.sh生成spark部署包编译完源代码后，虽然直接用编译后的目录再加以配置就可以运行spark，但是这时目录很庞大，部署起来不方便，所以需要生成部署包。spark源码根目录下带有一个脚本文件make-distribution.sh可以生成部署包，该脚本会使用MAVEN进行编译，然后打成一个tgz包。其参数有：–tgz：在根目录下生成 spark-$VERSION-bin.tar.gz，不加参数是不生成tgz文件，只生成/dist目录。–hadoop VERSION：打包时所用的Hadoop版本号，不加参数时为1.0.4。–with-yarn：是否支持Hadoop YARN，不加参数时为不支持yarn。–with-tachyon：是否支持内存文件系统Tachyon，不加参数时为不支持，此参数spark1.0.0-SNAPSHOT之后提供。如果要生成spark支持yarn、hadoop2.2.0的部署包，只需要将源代码复制到指定目录，进入该目录后运行：1./make-distribution.sh --hadoop 2.2.0 --with-yarn --tgz 如果要生成spark支持yarn、hadoop2.2.0、techyon的部署包，只需要将源代码复制到指定目录，进入该目录后运行：1./make-distribution.sh --hadoop 2.2.0 --with-yarn --with-tachyon --tgz 或者部署自定义版本1./make-distribution.sh --tgz --with-tachyon -Phadoop-2.4 -Pyarn -Phive -Dhadoop.version=2.4.0.2.1.4.0-632 生成在部署包位于根目录下，文件名类似于spark-1.0.0-SNAPSHOT-hadoop_2.2.0-bin.tar.gz。值得注意的是：make-distribution.sh已经带有SBT编译过程，所以不需要先编译再打包。 spark的IDEA开发环境配置创建工程1.在IDEA菜单栏选择File-&gt;New Project，选择创建Scala项目；2.在项目的基本信息填写项目名称、项目所在位置、Project SDK和Scala SDK，在这里设置项目名称为class3，关于Scala SDK的安装可直接点击create按钮，进入之后download。3.设置Modules，创建该项目后，可以看到现在还没有源文件，只有一个存放源文件的目录src以及存放工程其他信息的杂项。通过双击src目录或者点击菜单上的项目结构图标打开项目配置界面，在Modules设置界面中，分别设置main-&gt;scala目录为Sources类型4.配置Library选择Library目录，添加Scala SDK Library，这里选择scala-2.10.4版本添加Java Library，这里选择的是在$SPARK_HOME/lib/spark-assembly-1.1.0-hadoop2.2.0.jar文件 项目运行直接运行第一步 编写代码在src-&gt;main-&gt;scala下创建class3包，在该包中添加SogouResult对象文件 第二步 编译代码代码在运行之前需要进行编译，可以点击菜单Build-&gt;Make Project或者Ctrl+F9对代码进行编译，编译结果会在Event Log进行提示，如果出现异常可以根据提示进行修改. 第三步 运行环境配置SogouResult首次运行或点击菜单Run-&gt;Edit Configurations打开”运行/调试 配置界面”运行SogouResult时需要输入搜狗日志文件路径和输出结果路径两个参数，需要注意的是HDFS的路径参数路径需要全路径，否则运行会报错：l 搜狗日志文件路径：使用上节上传的搜狗查询日志文件hdfs://hadoop1:9000/sogou/SogouQ1.txtl 输出结果路径：hdfs://hadoop1:9000/class3/output2 第四步 运行结果查看启动Spark集群，点击菜单Run-&gt;Run或者Shift+F10运行SogouResult，在运行结果窗口可以运行情况。当然了如果需要观察程序运行的详细过程，可以加入断点，使用调试模式根据程序运行过程。使用如下命令查看运行结果，该结果和上节运行的结果一致12hadoop fs -ls /class3/output2hadoop fs -cat /class3/output2/part-00000 | less 打包运行上个例子使用了IDEA直接运行结果，在该例子中将使用IDEA打包程序进行执行编写代码在class3包中添加Join对象文件第一步 配置打包信息在项目结构界面中选择”Artifacts”，在右边操作界面选择绿色”+”号，选择添加JAR包的”From modules with dependencies”方式，出现如下界面，在该界面中选择主函数入口为Join 第二步 填写该JAR包名称和调整输出内容【注意】的是默认情况下”Output Layout”会附带Scala相关的类包，由于运行环境已经有Scala相关类包，所以在这里去除这些包只保留项目的输出内容 第三步 输出打包文件点击菜单Build-&gt;Build Artifacts，弹出选择动作，选择Build或者Rebuild动作 第四步 复制打包文件到Spark根目录下123cd /home/hadoop/IdeaProjects/out/artifacts/class3cp LearnSpark.jar /app/hadoop/spark-1.1.0/ls /app/hadoop/spark-1.1.0/ 第五步 运行查看结果通过如下命令调用打包中的Join方法，运行结果如下：12cd /app/hadoop/spark-1.1.0bin/spark-submit --master spark://hadoop1:7077 --class class3.Join --executor-memory 1g LearnSpark.jar hdfs://hadoop1:9000/class3/join/reg.tsv hdfs://hadoop1:9000/class3/join/clk.tsv]]></content>
      <categories>
        <category>cloud-computing</category>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>architecture</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive Row_number Function]]></title>
    <url>%2Fcloud-computing%2Fstorage%2FHive-Row-number-Function%2F</url>
    <content type="text"><![CDATA[Returns an ascending sequence of integers, starting with 1. Starts the sequence over for each group produced by the PARTITIONED BY clause. The output sequence includes different values for duplicate input values. Therefore, the sequence never contains any duplicates or gaps, regardless of duplicate input values.语法ROW_NUMBER() OVER([partition_by_clause] order_by_clause) The ORDER BY clause is required. The PARTITION BY clause is optional. The window clause is not allowed. 应用场景Often used for top-N and bottom-N queries where the input values are known to be unique, or precisely N rows are needed regardless of duplicate values. Because its result value is different for each row in the result set (when used without a PARTITION BY clause), ROW_NUMBER() can be used to synthesize unique numeric ID values, for example for result sets involving unique values or tuples. Similar to RANK and DENSE_RANK. These functions differ in how they treat duplicate combinations of values. 使用举例The following example demonstrates how ROW_NUMBER() produces a continuous numeric sequence, even though some values of X are repeated.123456789101112131415161718192021222324252627select x, row_number() over(order by x, property) as row_number, property from int_t;+----+------------+----------+| x | row_number | property |+----+------------+----------+| 1 | 1 | odd || 1 | 2 | square || 2 | 3 | even || 2 | 4 | prime || 3 | 5 | odd || 3 | 6 | prime || 4 | 7 | even || 4 | 8 | square || 5 | 9 | odd || 5 | 10 | prime || 6 | 11 | even || 6 | 12 | perfect || 7 | 13 | lucky || 7 | 14 | lucky || 7 | 15 | lucky || 7 | 16 | odd || 7 | 17 | prime || 8 | 18 | even || 9 | 19 | odd || 9 | 20 | square || 10 | 21 | even || 10 | 22 | round |+----+------------+----------+ The following example shows how a financial institution might assign customer IDs to some of history’s wealthiest figures. Although two of the people have identical net worth figures, unique IDs are required for this purpose. ROW_NUMBER() produces a sequence of five different values for the five input rows.1234567891011select row_number() over (order by net_worth desc) as account_id, name, net_worth from wealth order by account_id, name;+------------+---------+---------------+| account_id | name | net_worth |+------------+---------+---------------+| 1 | Solomon | 2000000000.00 || 2 | Croesus | 1000000000.00 || 3 | Midas | 1000000000.00 || 4 | Crassus | 500000000.00 || 5 | Scrooge | 80000000.00 |+------------+---------+---------------+]]></content>
      <categories>
        <category>cloud-computing</category>
        <category>storage</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive Date Type:TIMESTAMP]]></title>
    <url>%2Fcloud-computing%2Fstorage%2FHive-Date-Type-TIMESTAMP%2F</url>
    <content type="text"><![CDATA[The underlying Impala data type for date and time data is TIMESTAMP, which has both a date and a time portion.A data type used in CREATE TABLE and ALTER TABLE statements, representing a point in time. 时间范围hive里面TIMESTAMP Data Type，它的范围从1400-01-01 to 9999-12-31。 INTERVAL expressionsYou can perform date arithmetic by adding or subtracting a specified number of time units, using the INTERVAL keyword and the + and - operators or date_add() and date_sub() functions. You can specify units as YEAR[S], MONTH[S], WEEK[S], DAY[S], HOUR[S], MINUTE[S], SECOND[S], MILLISECOND[S], MICROSECOND[S], and NANOSECOND[S]. You can only specify one time unit in each interval expression, for example INTERVAL 3 DAYS or INTERVAL 25 HOURS, but you can produce any granularity by adding together successive INTERVAL values, such as timestamp_value + INTERVAL 3 WEEKS - INTERVAL 1 DAY + INTERVAL 10 MICROSECONDS.For example:12345select now() + interval 1 day;select date_sub(now(), interval 5 minutes);insert into auction_details select auction_id, auction_start_time, auction_start_time + interval 2 days + interval 12 hours from new_auctions; Conversions Impala automatically converts STRING literals of the correct format into TIMESTAMP values. Timestamp values are accepted in the format YYYY-MM-DD HH:MM:SS.sssssssss, and can consist of just the date, or just the time, with or without the fractional second portion. For example, you can specify TIMESTAMP values such as ‘1966-07-30’, ‘08:30:00’, or ‘1985-09-25 17:45:30.005’. You can cast an integer or floating-point value N to TIMESTAMP, producing a value that is N seconds past the start of the epoch date Partitioning Although you cannot use a TIMESTAMP column as a partition key, you can extract the individual years, months, days, hours, and so on and partition based on those columns. Because the partition key column values are represented in HDFS directory names, rather than as fields in the data files themselves, you can also keep the original TIMESTAMP values if desired, without duplicating data or wasting storage space. See Partition Key Columns for more details on partitioning with date and time values. Examples: 12345678910111213141516 select cast('1966-07-30' as timestamp);select cast('1985-09-25 17:45:30.005' as timestamp);select cast('08:30:00' as timestamp);select hour('1970-01-01 15:30:00'); -- Succeeds, returns 15.select hour('1970-01-01 15:30'); -- Returns NULL because seconds field required.select hour('1970-01-01 27:30:00'); -- Returns NULL because hour value out of range.select dayofweek('2004-06-13'); -- Returns 1, representing Sunday.select dayname('2004-06-13'); -- Returns 'Sunday'.select date_add('2004-06-13', 365); -- Returns 2005-06-13 with zeros for hh:mm:ss fields.select day('2004-06-13'); -- Returns 13.select datediff('1989-12-31','1984-09-01'); -- How many days between these 2 dates?select now(); -- Returns current date and time in UTC timezone.create table dates_and_times (t timestamp);insert into dates_and_times values ('1966-07-30'), ('1985-09-25 17:45:30.005'), ('08:30:00'), (now());]]></content>
      <categories>
        <category>cloud-computing</category>
        <category>storage</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive编程-Hive中正则表达式的使用]]></title>
    <url>%2Fcloud-computing%2Fstorage%2FHive%E7%BC%96%E7%A8%8B-Hive%E4%B8%AD%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[hive中的正则解析函数：regexp_extract的使用方法与注意事项 函数描述:regexp_extract(str, regexp[, idx]) - extracts a group that matches regexp返回值: string说明：将字符串subject按照pattern正则表达式的规则拆分，返回index指定的字符。注意，在有些情况下要使用转义字符 参数解释:其中：str是被解析的字符串regexp 是正则表达式idx是返回结果 取表达式的哪一部分 默认值为1。0表示把整个正则表达式对应的结果全部返回1表示返回正则表达式中第一个() 对应的结果 以此类推 举例：12345678hive&gt; select regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’, 1) from dual;thehive&gt; select regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’, 2) from dual;barhive&gt; select regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’, 0) from dual;foothebar]]></content>
      <categories>
        <category>cloud-computing</category>
        <category>storage</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown语法说明]]></title>
    <url>%2Fcommon%2Fothers%2Fmarkdown%E8%AF%AD%E6%B3%95%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[Markdown 的目标是实现「易读易写」。 可读性，无论如何，都是最重要的。一份使用 Markdown 格式撰写的文件应该可以直接以纯文本发布，并且看起来不会像是由许多标签或是格式指令所构成。 欢迎光临我的博客@(标签帮助文档)[牧云者|帮助|Markdown] 牧云者是我的网名，代表着一种自由与不羁。这是我的个人博客，主要记录： 个人心情 ：记录自己生活中的所思所想； 学习笔记 ：平时学习中有些东西需要回过头来重新看看，之前曾写过一些记录我的博客； 技术前瞻 ：写下工作中对于技术方面的理解与领域。 [TOC] Markdown简介 Markdown 是一种轻量级标记语言，它允许人们使用易读易写的纯文本格式编写文档，然后转换成格式丰富的HTML页面。 —— 维基百科 正如您在阅读的这份文档，它使用简单的符号标识不同的标题，将某些文字标记为粗体或者斜体，创建一个链接或一个脚注[^demo]。下面列举了几个高级功能，更多语法请按Ctrl + /查看帮助。 代码块12345678910@requires_authorizationdef somefunc(param1='', param2=0): '''A docstring''' if param1 &gt; param2: # interesting print 'Greater' return (param2 - param1 + 1) or Noneclass SomeClass: pass&gt;&gt;&gt; message = '''interpreter... prompt''' LaTeX 公式可以创建行内公式，例如 $\Gamma(n) = (n-1)!\quad\forall n\in\mathbb N$。或者块级公式： $$ x = \dfrac{-b \pm \sqrt{b^2 - 4ac}}{2a} $$ 表格 Item Value Qty Computer 1600 USD 5 Phone 12 USD 12 Pipe 1 USD 234 流程图12345678st=&gt;start: Starte=&gt;endop=&gt;operation: My Operationcond=&gt;condition: Yes or No?st-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op 以及时序图: 123Alice-&gt;Bob: Hello Bob, how are you?Note right of Bob: Bob thinksBob--&gt;Alice: I am good thanks! 提示：想了解更多，请查看流程图语法以及时序图语法。 复选框使用 - [ ] 和 - [x] 语法可以创建复选框，实现 todo-list 等功能。例如： 已完成事项 待办事项1 待办事项2 注意：目前支持尚不完全。 关于超链接超链接格式为：续费。 反馈与建议 微博：@牧云者 邮箱：&#x6d;&#117;&#121;&#x75;&#110;&#122;&#104;&#101;&#46;&#110;&#117;&#x64;&#x74;&#64;&#x67;&#109;&#x61;&#105;&#x6c;&#x2e;&#99;&#x6f;&#x6d; [^demo]: 这是一个示例脚注。请查阅 MultiMarkdown 文档 关于脚注的说明。]]></content>
      <categories>
        <category>common</category>
        <category>others</category>
      </categories>
      <tags>
        <tag>others</tag>
        <tag>help</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive编程-Hive表操作]]></title>
    <url>%2Fcloud-computing%2Fstorage%2FHive%E7%BC%96%E7%A8%8B-Hive%E8%A1%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[表操作包括表、分区、列的增删改查等基本操作以及一些特殊的高级特性。 这部分内容包括数据库、数据表、列、视图、索引、宏、权限等各方面的内容，hive的官方文档说的特别详细清楚。链接如下： hive官方文档]]></content>
      <categories>
        <category>cloud-computing</category>
        <category>storage</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive编程-Hive数据存储]]></title>
    <url>%2Fcloud-computing%2Fstorage%2FHive%E7%BC%96%E7%A8%8B-Hive%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[Hive本身是没有专门的数据存储格式，也没有为数据建立索引，只需要在创建表的时候告诉Hive数据中的列分隔符和行分隔符，Hive就可以解析数据。所以往Hive表里面导入数据只是简单的将数据移动到表所在的目录中。Hive的数据分为表数据和元数据，表数据是Hive中表格(table)具有的数据;而元数据是用来存储表的名字，表的列和分区及其属性，表的属性(是否为外部表等)，表的数据所在目录等。下面分别来介绍。 Hive的数据存储Hive中主要包含以下几种数据模型：Table(表)，External Table(外部表)，Partition(分区)，Bucket(桶)1.Table(表)首先是数据库的存储，每个库在HDFS中都有相应的目录用来存储，这个目录可以通过${HIVE_HOME}/conf/hive-site.xml配置文件中的 hive.metastore.warehouse.dir属性来配置，这个属性默认的值是/user/hive/warehouse(这个目录在 HDFS上)，我们可以根据实际的情况来修改这个配置。并且每个库对应的文件夹都是以db结尾，比如系统下有个数据库叫temp，那么它在HDFS里的存储路径就为：hdfs://user/hive/warehouse/temp.db该数据库下表的数据存放在上述目录下的子文件夹中，如表temp_table存储于：hdfs://user/hive/warehouse/temp.db/temp_table文件中。2.External Table(外部表)Hive中的外部表和表很类似，但是其数据不是放在自己表所属的目录中，而是存放到别处，这样的好处是如果你要删除这个外部表，该外部表所指向的数据是不会被删除的，它只会删除外部表对应的元数据;而如果你要删除表，该表对应的所有数据包括元数据都会被删除。3.Partition(分区)在Hive中，表的每一个分区对应表下的相应目录，所有分区的数据都是存储在对应的目录中。比如wyp 表有dt和city两个分区，则对应dt=20131218,city=BJ对应表的目录为/user/hive/warehouse /dt=20131218/city=BJ，所有属于这个分区的数据都存放在这个目录中。4.Bucket(桶)对指定的列计算其hash，根据hash值切分数据，目的是为了并行，每一个桶对应一个文件(注意和分区的区别)。比如将wyp表id列分散至16个桶中，首先对id列的值计算hash，对应hash值为0和16的数据存储的HDFS目录为：/user /hive/warehouse/wyp/part-00000;而hash值为2的数据存储的HDFS 目录为：/user/hive/warehouse/wyp/part-00002。 Hive的元数据存储Hive中的元数据包括表的名字，表的列和分区及其属性，表的属性(是否为外部表等)，表的数据所在目录等。 目前Hive将元数据存储在RDBMS中，如Mysql、Derby中。元数据中主要存储一下几类数据： Database相关存储Hive Database的元数据信息，DB_ID是数据库ID,NAME是库名,DB_LOCATION_URI是数据库在HDFS中的位置，DESC为数据库的描述信息。 Table相关TBLS 存储Hive Table的元数据信息,每个表有唯一的TBL_ID,SD_ID外键指向所属的Database,SD_IID关联SDS表的主键。 数据存储相关SDSSDS表保存了Hive数据仓库所有的HDFS数据文件信息，每个SD_ID唯一标记一个数据存储记录,CD_ID关联COLUMN_V2.CD_ID，指定该数据的字段信息,SERDE_ID关联SERDES.SERDE_ID，指定该数据的序列化信息 COLUMN相关该表只有一个字段CD_ID，永远存储整个Hive数据仓库中的CD_ID. (序列化) Partition相关(分区)Hive表分区信息 SKEW相关(数据倾斜) BUCKET相关(分桶) PRIVS相关(权限管理)]]></content>
      <categories>
        <category>cloud-computing</category>
        <category>storage</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive编程-Hive库操作]]></title>
    <url>%2Fcloud-computing%2Fstorage%2FHive%E7%BC%96%E7%A8%8B-Hive%E8%A1%A8%E6%93%8D%E4%BD%9C%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Hive作为数据管理工具，这通常意味着必须有组织和结构。否则，我们的数据库会成为一个永久存储无限制数据的死区，没人能够或者想要使用这些数据。当然Hive数据库跟传统数据库有很多不同的地方，下面就来说说Hive数据库到底如何来管理。 数据库创建1234CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] [LOCATION hdfs_path] [WITH DBPROPERTIES (property_name=property_value, ...)]; The uses of SCHEMA and DATABASE are interchangeable – they mean the same thing.IF NOT EXISTS是一个可选子句，通知用户已经存在相同名称的数据库。然后可以通过SHOW DATABASES;查询数据库列表 数据库修改用户可以通过使用alter DATABASE命令为某个数据库的DBPROPERTIES设置键值对属性值，来描述这个数据库的属性信息。数据库的其他属性是不可更改的，包括数据库名和数据库所在的目录位置：ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...); 数据库删除DROP DATABASE是删除所有的表并删除数据库的语句。它的语法如下：12DROP (DATABASE|SCHEMA) [IF EXISTS] database_name[RESTRICT|CASCADE]; 使用CASCADE查询删除数据库。这意味着要全部删除相应的表在删除数据库之前。而如果使用RESTRICT的话，那么就和默认情况一下，也就是，如果想删除某个数据库，那么必须先删除该数据库里的所有表。如果某个数据库被删除了，那么其对应的目录也就同时也被删除。]]></content>
      <categories>
        <category>cloud-computing</category>
        <category>storage</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive编程-Hive实现机制]]></title>
    <url>%2Fcloud-computing%2Fstorage%2FHive%E7%BC%96%E7%A8%8B-Hive%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的SQL查询功能，可以将SQL语句转换为MapReduce任务运行。1.Hive介绍Hive 定义了简单的类 SQL 查询语言，称为 HQL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。Hive是建立在Hadoop基础上的数据仓库软件包,在开始阶段,它被Facebook用于处理大量的用户数据和日志数据.它现在是Hadoop的子项目并有许多贡献者.其目标用户仍然是习惯SQL的数据分析师,他们需要在Hadoop规模的数据上做即席查询/汇总和数据分析.通过称为HiveQL的类SQL语言,你可以发起一个查询来实现与Hive的交互. 2.实现机制利用hadoop Map/Reduce程序和Hive实现hadoop word count的对比 借助于Hadoop和HDFS的大数据存储能力，数据仍然存储于Hadoop的HDFS中，Hive提供了一种类SQL的查询语言：HiveQL（HQL），对数据进行管理和分析，开发人员可以近乎sql的方式来实现逻辑，从而加快应用开发效率。 HQL经过解析和编译，最终会生成基于Hadoop平台的Map Reduce任务，Hadoop通过执行这些任务来完成HQL的执行。Hive的组件总体上可以分为以下几个部分：用户接口（UI）、驱动、编译器、元数据（Hive系统参数数据）和执行引擎。 1.对外的接口UI包括以下几种：命令行CLI，Web界面、JDBC/ODBC接口；2.驱动：接收用户提交的查询HQL；3.编译器：解析查询语句，执行语法分析，生成执行计划；4.元数据Metadata：存放系统的表、分区、列、列类型等所有信息，以及对应的HDFS文件信息等；5.执行引擎：执行执行计划，执行计划是一个有向无环图，执行引擎按照各个任务的依赖关系选择执行任务（Job）。 3.Hive的数据模型Hive没有专门的数据存储格式，也没有为数据建立索引，用于可以非常自由的组织Hive中的表，只需要在创建表的时候定义好表的schema即可。Hive中包含4中数据模型：Tabel、ExternalTable、Partition、Bucket。 Table：类似与传统数据库中的Table，每一个Table在Hive中都有一个相应的目录来存储数据。例如：一个表t，它在HDFS中的路径为：/user/hive/warehouse/t。 Partition：类似于传统数据库中划分列的索引。在Hive中，表中的一个Partition对应于表下的一个目录，所有的Partition数据都存储在对应的目录中。例如：t表中包含ds和city两个Partition，则对应于ds=2014，city=beijing的HDFS子目录为：/user/hive/warehouse/t/ds=2014/city=Beijing； 需要注意的是，分区列是表的伪列，表数据文件中并不存在这个分区列的数据。 Buckets：对指定列计算的hash，根据hash值切分数据，目的是为了便于并行，每一个Buckets对应一个文件。将user列分数至32个Bucket上，首先对user列的值计算hash，比如，对应hash=0的HDFS目录为：/user/hive/warehouse/t/ds=2014/city=Beijing/part-00000；对应hash=20的目录为：/user/hive/warehouse/t/ds=2014/city=Beijing/part-00020。 External Table指向已存在HDFS中的数据，可创建Partition。Managed Table创建和数据加载过程，可以用统一语句实现，实际数据被转移到数据仓库目录中，之后对数据的访问将会直接在数据仓库的目录中完成。删除表时，表中的数据和元数据都会删除。External Table只有一个过程，因为加载数据和创建表是同时完成。数据是存储在Location后面指定的HDFS路径中的，并不会移动到数据仓库中。 4.Hive转化为mapReduceHive编译器将HQL代码转换成一组操作符（operator），操作符是Hive的最小操作单元，每个操作符代表了一种HDFS操作或者MapReduce作业。如下列查询语句：123INSERT OVERWRITE TABLE read_log_tmpSELECT a.userid,a.bookid,b.author,b.categoryidFROM user_read_log a JOIN book_info b ON a.bookid = b.bookid; 最终的编译顺序为：]]></content>
      <categories>
        <category>cloud-computing</category>
        <category>storage</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive编程-HiveQL数据导入与导出]]></title>
    <url>%2Fcloud-computing%2Fstorage%2FHive%E7%BC%96%E7%A8%8B-HiveQL%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E4%B8%8E%E5%AF%BC%E5%87%BA%2F</url>
    <content type="text"><![CDATA[HiveQL,也就是hive查询语言，关注与向表中装载数据和从表中抽取收据到文件系统的数据操作。 数据导入Hive没有行级别的数据插入、数据更新和删除操作，那么往表中装载数据的唯一途径就是使用一种“大量”的数据装载操作。1.从本地文件系统中导入数据在hive环境下运行：123LOAD DATA LOCAL INPATH '$&#123;env:HOME&#125;/california-employee'OVERWRITE INTO TABLE employeesPARTITION (country = 'US',stat = 'CA'); 提示： LOAD DATA LOCAL… 拷贝本地数据到位于分布式文件系统上的目标位置，而LOAD DATA…转移数据到目标位置 如果分区目录不存在的话，这个命令会先创建分区目录，然后再将数据拷贝到该目录下。如果目标是非分区表，那么与剧中应该省略PARTITION子句。通常情况下指定的路径应该是一个目录而不是单个独立的文件。HIve要求源文件和目标文件以及目录应该在同一个文件系统中。如果用户指定了OVERWRITE关键字，那么目标文件夹中之前的数据将会被先删除掉。如果没有该关键字，仅仅会把新增的文件增加到目标文件夹中而不会删除之前的数据，然而如果目标文件夹中已经存在和装载的文件同名的文件，那么旧的文件将被覆盖掉。 2.从HSFS上导数据到hive表新建hdfs文件目录hadoop fs -mkdir /tmp/input;将本地数据放到集群中hadoop fs -put ${env:HOME}/california-employee /tmp/input/california-employee;查看文件是否复制成功dfs -cat /tmp/input/california-employee;导入集群数据load data inpath &#39;/tmp/input/california-employee&#39; overwrite into table employees partition(openingtime=201508); 3.从别的Hive表中导入数据到Hive表中创建数据表123456create table if not exists temp.employees(id int,name string,tel string)row format delimitedfields terminated by ','stored as textfilepartitioned by (age int)CLUSTERED BY (id) INTO 256 BUCKETS STORED AS ORC; 插入数据1234from ext.employees winsert overwrite table temp.employeespartition (age=25)select w.id,w.name,w.tel where w.age=25 也可以采用动态分区1234from ext.employees winsert overwrite table temp.employeespartition (age)select w.id,w.name,w.tel where w.age=25 在这里age分区将会根据w.age的值，被动态的创建。 多表插入，是一种高效的插入方式，同时插入多个分区12345from ext.employees winsert overwrite table temp.employees select w.id,w.name,w.tel where w.age=25insert overwrite table temp.employees select w.id,w.name,w.tel where w.age=27; 4.创建Hive表的同时导入查询数据 12create table temp.employees as select id,name,tel,age from ext.employees where age=25; 5.使用sqoop从关系数据库导入数据到Hive表这部分另说 数据导出1.将查询的结果写入文件系统语法：123456789101112131415Standard syntax:INSERT OVERWRITE [LOCAL] DIRECTORY directory1 [ROW FORMAT row_format] [STORED AS file_format] (Note: Only available starting with Hive 0.11.0) SELECT ... FROM ...Hive extension (multiple inserts):FROM from_statementINSERT OVERWRITE [LOCAL] DIRECTORY directory1 select_statement1[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 select_statement2] ...row_format : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] [NULL DEFINED AS char] (Note: Only available starting with Hive 0.13) 如果使用LOCAL，则数据会写入到本地.如果不使用LOCAL,则数据会写到指定的HDFS中，如果没写全路径，则使用Hadoop的配置项fs.default.name （NameNode的URI）。例子：1.导出到本地insert overwrite local directory &#39;/data/tmp/score&#39; select * from score;这个导出的结果会被分区2.导出到HDFSinsert overwrite directory &#39;/data/tmp/score&#39; select * from score;3.导出到另一个表中insert into table test partition (age=&#39;25&#39;) select id, name, tel from wyp;4.用hive命令带-e或-f参数来导出数据，导出的结果是用\t分割的hive -e &quot;select * from wyp&quot; &gt;&gt; local/wyp.txt]]></content>
      <categories>
        <category>cloud-computing</category>
        <category>storage</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to use SimpleHttpServer]]></title>
    <url>%2Ftechnology%2Fprogramming%2FHow-to-use-SimpleHttpServer%2F</url>
    <content type="text"><![CDATA[In this post we will look at the built-in web server in Python. What is it?The SimpleHTTPServer module that comes with Python is a simple HTTP server thatprovides standard GET and HEAD request handlers. Why should I use it?An advantage with the built-in HTTP server is that you don’t have to installand configure anything. The only thing that you need, is to have Python installed. That makes it perfect to use when you need a quick web server running and youdon’t want to mess with setting up apache. You can use this to turn any directory in your system into your web serverdirectory. How do I use it?To start a HTTP server on port 8000 (which is the default port), simple type: python -m SimpleHTTPServer [port] This will now show the files and directories which are in the current workingdirectory.You can also change the port to something else:$ python -m SimpleHTTPServer 8080 How to share files and directories1.in your terminal, cd into whichever directory you wish to have accessible viabrowsers and HTTP. 12cd /var/www/$ python -m SimpleHTTPServer After you hit enter, you should see the following message:Serving HTTP on 0.0.0.0 port 8000 … 2.Open your favorite browser and put in any of the following addresses:12http://your_ip_address:8000http://127.0.0.1:8000 If you don’t have an index.html file in the directory, then all files anddirectories will be listed. 3.As long as the HTTP server is running, the terminal will update as data areloaded from the Python web server.You should see standard http logging information (GET and PUSH), 404 errors,IP addresses, dates, times, and all that you would expect from a standard httplog as if you were tailing an apache access log file. SummaryIn this post we showed how you with minimal effort can setup a web server toserve content. It’s a great way of serve the contents of the current directory from the commandline While there are many web server software out there (apache, nginx), using Pythonbuilt-in HTTP server require no installation and configuration.]]></content>
      <categories>
        <category>technology</category>
        <category>programming</category>
      </categories>
      <tags>
        <tag>programming</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pycharm安装]]></title>
    <url>%2Ftechnology%2Ftools%2Fpycharm%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[PyCharm是JetBrains系列产品的一员，也是现在最好用的IDE。PyCharm维持了JetBrains一贯高度智能的作风，简要枚举如下： 独特的本地VCS系统 强大的重构功能 基于上下文的智能代码提示和纠错 可以与IDEA、PhpStorm等IDE共享配置文件 PyCharm社区版免费下载地址：http://www.jetbrains.com/pycharm/ 安装完PyCharm后，还需要安装Python解释器：http://www.python.org/getit/ 推荐安装最稳定且比较新的版本，比如3.3。同时为了兼容以前的程序，最好下载一个2.7.6备用，两者并不冲突。 打开PyCharm新建第一个项目，此时解释器还处于未配置的状态，通过如下操作告诉PyCharm我们安装了Python的路径： 通过+号增加一个解释器 增加之后PyCharm会智能地提示你安装setuptool和pip，照着提示一路点击就行了。（Python2.7的setuptool安装会报错UnicodeDecodeError: ‘ascii’ codec can’t decode byte 0xc4 in position 33: ordinal not in range(128)，需要手工修改脚本再安装，详情）。 配置完成后填入项目路径新建一个项目，然后新建一个.py文件，写一句helloworld： 此时还无法运行，因为没有配置项目的入口脚本，通过下图的步骤指定一个： 在scrip框里填入你的入口脚本: 之后就可以点击绿色的播放按钮运行这个项目了。]]></content>
      <categories>
        <category>technology</category>
        <category>tools</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文章创建与发布等常用命令]]></title>
    <url>%2Fcommon%2Fothers%2F%E6%96%87%E7%AB%A0%E5%88%9B%E5%BB%BA%E4%B8%8E%E5%8F%91%E5%B8%83%E7%AD%89%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 部署步骤 每次部署的步骤，可按以下三步来进行。 hexo clean hexo generate hexo deploy 一些常用命令： hexo new”postName” #新建文章 hexo new page”pageName” #新建页面 hexo generate #生成静态页面至public目录 hexo server #开启预览访问端口（默认端口4000，’ctrl + c’关闭server） hexo deploy #将.deploy目录部署到GitHub hexo help # 查看帮助 hexo version #查看Hexo的版本 原文链接：http://www.jianshu.com/p/465830080ea9]]></content>
      <categories>
        <category>common</category>
        <category>others</category>
      </categories>
      <tags>
        <tag>others</tag>
        <tag>help</tag>
      </tags>
  </entry>
</search>
